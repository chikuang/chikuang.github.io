% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications},
  pdfauthor={Chi-Kuang Yeh},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications}
\author{Chi-Kuang Yeh}
\date{2024-05-20}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\newcommand{\btheta}{\mathbf{\theta}}

\chapter{Preface}\label{preface}

This thesis is written by Chi-Kuang Yeh, under the guidance of Professor Julie Zhou. I create this bookdown project to fix some typos within it.

\section{Chi-Kuang Yeh}\label{chi-kuang-yeh}

B.Sc. (Hons.), University of Victoria, 2016

\section{Supervisory Committee}\label{supervisory-committee}

Supervisor: Dr.~\href{https://www.uvic.ca/science/math-statistics/people/home/faculty/zhou_julie.php}{Julie Zhou} (Professor, Department of Mathematics and Statistics)

Departmental Member: Dr.~\href{https://www.uvic.ca/science/math-statistics/people/home/faculty/zhang_xuekui.php}{Xuekui Zhang} (Assistant Professor, Department of Mathematics and Statistics)

External Examiner: Dr.~\href{https://www.uvic.ca/ecs/ece/faculty-and-staff/home/faculty/dongxiaodai.php}{Xiaodai Dong} (Professor, Department of Electrical and Computer Engineering)

\section{Abstract}\label{abstract}

In this thesis, we first review the current development of optimal regression designs under the second-order least squares estimator in the literature. The criteria include A- and D-optimality. We then introduce a new formulation of A-optimality criterion so the result can be extended to c-optimality which has not been studied before. Following Kiefer's equivalence results, we derive the optimality conditions for A-, c- and D-optimal designs under the second-order least squares estimator. In addition, we study the number of support points for various regression models including Peleg models, trigonometric models, regular and fractional polynomial models. A generalized scale invariance property for D-optimal designs is also explored. Furthermore, we discuss one computing algorithm to find optimal designs numerically. Several interesting applications are presented and related MATLAB code are provided in the thesis.

\section{List of Abbreviations}\label{list-of-abbreviations}

\begin{itemize}
\tightlist
\item
  \textbf{BLUE}: Best Linear Unbiased Estimator
\item
  \textbf{CVX}: A MATLAB-based modeling system for convex optimization
\item
  \textbf{FRM}: Fractional Polynomial Model
\item
  \textbf{GSI}: Generalized Scale Invariance
\item
  \textbf{MATLAB}: A script language that uses matrix as default format and a computing environment, which is developed by MathWorks
\item
  \textbf{OLSE}: Ordinary Least Squares Estimator
\item
  \textbf{SDP}: Semi-Definite Programming
\item
  \textbf{SLSE}: Second-Order Least Squares Estimator
\end{itemize}

\chapter{Introduction}\label{intro}

Design of experiment is a sub-field in statistics that has a long history in its developments. After Sir Ronald A. Fisher's pioneering work on the analysis of variance and fractional factorial design concepts, while working in Rothamsted Experimental Station in the 1920s and 1930s \citep[p.~21]{montgomery2012introduction}, many statisticians worked in this research area and made significant contributions. \citet{berger2009introduction} gave many vital examples in the development of optimal design theory over the years, including \citet{chernoff1953locally}, \citet{kiefer1959optimum}, \citet{kiefer1974general}, \citet{kiefer-wolfowitz1959optimum}, \citet{silvey1980optimal} and \citet{atkinson1992optimum}. Their inputs to this field have had huge impacts towards today's design framework. One century later, design techniques have been found to be effective and now are widely used in other disciplines, such as agriculture, engineering, environmental science, biomedical and pharmaceutical studies. See examples in \citet{crary2000optimal}, \citet{crary2002design}, \citet{haines2003bayesian}, \citet{zhou2003a}, \citet{jain2003modeling}, \citet{brosi2008optimal} and \citet{schorning2017optimal}. Its primary objective is to study the cause and effect between variables under some systematic approaches and procedures.

Over the decades, many theoretical results and algorithms have been developed to construct different kinds of optimal designs, which include factorial design, fractional factorial design, response surface design, and regression design. In this thesis, we will focus on optimal regression design under a recently proposed statistical estimator, second-order least squares estimator (SLSE) in \citet{wang2008second}.

\section{Optimal regression design problem}\label{optimal-regression-design-problem}

``Regression analysis is a statistical technique for investigating and modeling the relationship between variables'' \citep{montgomery2012introduction}. It is one of the most widely used statistical methods to explore the relationship between variables based on observed data. Although there are different kinds of regression models, we mainly focus on one response linear and nonlinear regression models.

Suppose we want to study the relationship between \(\boldsymbol{x}\in \mathbb{R}^p\) (a vector of explanatory variables) and \(y\) (a response variable). Consider a general regression model for (\(\boldsymbol{x_i},y_i\)),

\begin{equation}
y_i=g\left(\boldsymbol{x_i};\boldsymbol{\theta}\right)+\epsilon_i,\quad ~i=1,\dots,n,
\label{eq:simple-regression}
\end{equation}
where \(\boldsymbol{\theta}\in \mathbb{R}^q\) is an unknown parameter vector, \(n\) is the sample size, \(g(\cdot)\) is a known linear or nonlinear expectation function depending on \(\boldsymbol{\theta}\), and \(\epsilon_i\) is a random error of the regression model. The random error \(\epsilon_i's\) are assumed to be independent identically distributed with zero mean and variance \(\sigma^2\). The random error terms are further assumed to be homoscedastic in this thesis. Since \(\boldsymbol{x_i}\) and \(y_i\) are observed data and \(g(\cdot)\) is also known, the only unknown component is \(\boldsymbol{\theta}\). A question naturally comes in mind is how to estimate \(\boldsymbol{\theta}\) efficiently.

Suppose \(\hat{\boldsymbol{\theta}}\) is an estimator of \(\boldsymbol{\theta}\). The design problem aims to get the most information about \(\boldsymbol{\theta}\) or \(g\left(\boldsymbol{x}_i;\boldsymbol{\theta}\right)\) by selecting the best probability distribution on \(\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_n\) that maximizes some scalar functions of the Fisher's information matrix of \(\hat{\boldsymbol{\theta}}\). The sample points \(\boldsymbol{x}_i\) and space are called design points and design space, respectively. It is known that Fisher's information matrix is proportional to the inverse of the variance-covariance matrix of \(\hat{\boldsymbol{\theta}}\). Thus, the design problem aims to minimize some scalar functions of the variance-covariance matrix of \(\hat{\boldsymbol{\theta}}\), which are called objective functions or loss functions. The resulted probability measure \(\xi\) contains two components that are the support points and the corresponding probabilities associated with these points. The choice of the loss functions is determined based on the design interest. Various design criteria have been studied in the literature, such as A- and D-optimality criteria. D-optimality is one of the most widely used design criterion that minimizes the determinant of the variance-covariance matrix. The most desired property of D-optimal design is its scale invariant property. A-optimality minimizes the trace that leads to minimize the sum of the variances of the estimated parameters. See \citet{fedorov1972theory}, \citet{silvey1980optimal}, \citet{pukelsheim2006optimal}, \citet{berger2009introduction}, and \citet{dean2015handbook} for other optimality criteria.

Let us consider Gompertz growth model to illustrate a D-optimal design. The model is given by
\begin{equation*}
y_i=\theta_1 e^{-\theta_2e^{-\theta_3 x_i}}+\epsilon_i,~i=1,2,\cdots,n,\quad \boldsymbol{\theta}
=\left(\theta_1,\theta_2,\theta_3\right)^\top,\quad x_i\in S,
\end{equation*}
where \(\theta_1\) describes the maximum growing capacity, \(\theta_2\) explains the initial status of the subject, \(\theta_3\) determines the growth rate, \(y\) is the overall growth at the current time point and \(x\) is the time. Note that \(x\), \(\theta_1\) , \(\theta_2\) and \(\theta_3\) are assumed to be positive in this context. We want to study how one subject's total growth associated with time. The model has broad applications in biological science and cancer studies. See some examples in \citet{laird1964dynamics} and \citet{kozusko2003combining}. Suppose the design space \(S\) is \([0,10]\) (i.e.~\(x\in[0,10]\)) and the true parameters of \(\boldsymbol{\theta}\) is given by \(\boldsymbol{\theta}_o=(1,1,1)^\top\). For this model, D-optimal design aims to select the probability measure on \(S\) that minimizes \(\det(\hat{\boldsymbol{\theta}})\), where \(\hat{\boldsymbol{\theta}}\) is the SLSE. The details of the SLSE will be discussed in Section \ref{section:SLSE} and Chapter \(\ref{chapter:optimal regression under SLSE}\). The resulted probability measure under D-optimality is
\begin{equation}
  \xi_D^*=
\begin{bmatrix}
 0.000 &   1.350 &  10.000\\
 1/3 &   1/3 &  1/3
\end{bmatrix},
\label{eq:gompertz-prob}
\end{equation}

where the top row represents the support points and the second row describes the corresponding probabilities on the points. The resulted design has three support points at \(x=0.000,~1.350\) and \(10.000\) having equal weight (\(1/3\)). The interpretation of \eqref{eq:gompertz-prob} is that we will distribute resources evenly at three points, \(0.000\), \(1.350\) and \(10.000\). For instance, if the maximum number of runs available is fifteen due to the scarcity of resources, the researcher will make five observations at each of the three points. In Figure \ref{fig:gompertz}, the line represents the behavior of expectation function \(g(\boldsymbol{x};\boldsymbol{\theta})\) in the design space S, and the \(*\) represents the support point.

Many studies are conducted by using ordinary least squares estimator (OLSE) as the estimator (\(\hat{\boldsymbol{\theta}}\)) in optimal regression design framework. OLSE is the best linear unbiased estimator (BLUE) in the regression context. However, if the error distribution is asymmetric, the SLSE is more efficient than OLSE from \citet{wang2008second}, which is reviewed in next section.

\section{Second-order least squares estimator}\label{section-SLSE}

We first discuss the relationship between OLSE and SLSE, as well as the advantages of SLSE over OLSE. OLSE is an estimator to estimate the parameter vector \(\boldsymbol{\theta}\) in regression model \eqref{eq:simple-regression}, which is defined to be
\begin{equation*}
\boldsymbol{\hat{\theta}}:=\underset{\boldsymbol{\theta}}{\mathrm{argmin}}\sum_{i=1}^n 
(y_i-g(\boldsymbol{x_i};\boldsymbol{\theta}) )^2. 
\end{equation*}
The assumptions for using OLSE are: the error terms are assumed to be homoscedastic and independently identically distributed with zero mean and finite constant variance. It has many desired properties such as consistency and it is the BLUE, which is widely used. In practice, however, other estimators might outperform OLSE in some scenarios. If the error distribution is asymmetric, \citet{wang2008second} has shown that SLSE is asymptotically more efficient than OLSE. When the random errors are symmetrically distributed, SLSE and OLSE have the same asymptotic efficiency. SLSE has caught attentions in optimal regression design context due to these reasons.

We now review some properties of the SLSE in the regression model (\eqref{eq:simple-regression}. SLSE is defined as
\begin{equation*}
(\boldsymbol{\hat{\theta}}^\top,\hat{\sigma}^2)^\top:=\underset{\boldsymbol{\theta},\sigma^2}{\mathrm{argmin}}\sum_{i=1}^n 
\begin{pmatrix}
y_i-g(\boldsymbol{x}_i;\boldsymbol{\theta})\\
y_i^2-g^2(\boldsymbol{x}_i;\boldsymbol{\theta})-\sigma^2
\end{pmatrix}^\top
W(\boldsymbol{x_i}) 
\begin{pmatrix}
y_i-g(\boldsymbol{x}_i;\boldsymbol{\theta})\\
y_i^2-g^2(\boldsymbol{x}_i;\boldsymbol{\theta})-\sigma^2
\end{pmatrix}.
\end{equation*}
Note that \(W(\boldsymbol{x_i})\) is a \(2\times 2\) non-negative semi-definite matrix which may or may not depend on \(\boldsymbol{x_i}\) \citep{wang2008second}. It is clear that SLSE is a natural extension of the OLSE which is defined based on the first-order difference function (i.e.~\(y_i-\mathbb{E}[y_i]=y_i-g(\boldsymbol{x}_i;\boldsymbol{\theta})\)). On the other hand, SLSE is defined using not only the fist-order difference function, but also second-order difference function (i.e.~\(y_i^2-\mathbb{E}[y_i^2]=y_i^2-(g^2(\boldsymbol{x}_i;\boldsymbol{\theta})+\sigma^2))\). One might think about the downsides of the SLSE after talking about the advantages of SLSE over OLSE. SLSE does have its disadvantages indeed. It is not a linear estimator and there is no closed-form solution. It requires more computational resources compared to the OLSE due to the nonlinearity. However, numerical results can be easily computed for SLSE nowadays. As the result, SLSE is a powerful alternative estimator to be considered in research studies and real-life applications.

\section{Research problem}\label{research-problem}

As introduced in the previous section, \citet{wang2008second} showed that SLSE is asymptotically more efficient than OLSE when the error distribution is asymmetric. Optimal designs under the SLSE was proposed in \citet{gao2014new}. \citet{bose2015optimal} \citet{yin2018optimal} and \citet{gao2017d} did further investigations, including the convexity analysis, numerical methods, transformation and scale invariance properties for both A- and D-optimality criteria. There are other commonly used design criteria under the SLSE that have not been studied in the literature. Our goal is to fill this gap by extending the results to other design criteria, as well as exploring and deriving more theoretical results for the optimal designs under the SLSE.

The rest of the thesis is organized as follows. Chapter 2 describes the detailed formulation of optimal regression designs under SLSE. We derive several analytical results including equivalence theorem and the number of support points in optimal designs. Chapter 3 explains how to use numerical algorithms to solve the proposed optimal regression design problems via convex programming. We also present several interesting applications of the optimal designs studied in this thesis. Chapter 4 provides concluding remarks and discusses possible future research topics. MATLAB code are given in the Appendix.

\section{Main Contributions}\label{main-contributions}

Here is a summary of the main contributions in this thesis.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We have studied the c-optimality design criterion under the SLSE, which has not been studied before.
\item
  We have applied Kiefer's equivalence theorem \citep{kiefer1974general} to obtain the conditions for the A-, c- and D-optimal designs under the SLSE.
\item
  We have obtained the number of support points in optimal designs under the SLSE analytically for various regression models.
\item
  We have studied the generalized scale invariance property of D-optimal designs under the SLSE.
\item
  We have given one efficient and effective computing algorithm based on the program in MATLAB for computing optimal designs under the SLSE on discrete design spaces.
\end{enumerate}

\chapter{Optimal Regression Designs Under SLSE}\label{chapter-SLSE}

In this chapter, we first review the results and properties of optimal regression designs under the SLSE. We then derive several new analytical results for the optimal designs under the SLSE. We begin with recalling the formulation of the optimal regression designs under SLSE based on three different criteria, A-, D-, and c-optimality. The formulation of A-, and D-optimality under the SLSE was first proposed in \citet{gao2014new} while A-optimality was further investigated in \citet{yin2018optimal}. We formulate optimal design problems under A-optimality differently so that the properties can be extended to c-optimality which has not been studied yet. Equivalence results for verifying optimal designs are also obtained. In addition, analytical results are derived for the number of support points for several regression models.

\section{Design criteria under SLSE}\label{design-criteria-under-slse}

Let us introduce the notations first. Assume \(\sigma_o\) and \(\boldsymbol{\theta}_o\) are the true parameter values of \(\sigma\) and \(\boldsymbol{\theta}\), respectively. Let \(S \subset \mathbb{R}^p\) be the design space for \(\boldsymbol{x}\). Let \(\operatorname{tr}(\cdot)\) and \(\det(\cdot)\) be the trace and determinant functions of a matrix, respectively. Moreover, let \(\Xi\) denote the class of all probability measures on \(S\). Define, for any \(\xi\in \Xi\),

\begin{equation*}
\boldsymbol{g}_1=\boldsymbol{g}_1(\xi,\boldsymbol{\theta_o})=\mathbb{E}_{\xi}\bigg[ \frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_o} \bigg],
\end{equation*}

and
\begin{equation*}
\boldsymbol{G}_2=\boldsymbol{G}_2(\xi,\boldsymbol{\theta}_o)=\mathbb{E}_{\xi} \bigg[ \frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}^\top}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_o}\bigg].
\end{equation*}

For a discrete probability measure \(\xi\in \Xi\), we write it as
\[
\xi=\begin{bmatrix}
    \boldsymbol{x}_1&\boldsymbol{x}_2   &\ldots &\boldsymbol{x}_m\\
    p_1     &p_2        &\ldots &p_m
\end{bmatrix},
\]
where \(\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_m\) are the support points in \(S\), and \(p_1,\dots,p_m\) are the probabilities associated with those points. From \citet{gao2014new}, the asymptotic variance-covariance matrix of \(\hat{\boldsymbol{\theta}}\) under the SLSE is given by
\begin{equation}
\boldsymbol{\mathbb{V}}({\boldsymbol{\hat{\theta}}})=\sigma^2_o(1-t)(\boldsymbol{G}_2-t\boldsymbol{g}_1\boldsymbol{g}_1^\top)^{-1},
\label{eq:cov-matrix}
\end{equation}
where \(t=\frac{\mu_3^2}{\sigma_o^2(\mu_4-\sigma_o^4)}\), \(\mu_3=\mathbb{E}[\epsilon_i^3|\boldsymbol{x}]\) and \(\mu_4=\mathbb{E}[\epsilon_i^4|\boldsymbol{x}]\). Note that \citet{gao2014new} discussed that \(t \in [0,1)\) for any error distributions, and \(t=0\) for symmetric error distributions. Define a matrix \(\boldsymbol{J}=\boldsymbol{J}(\xi,\boldsymbol{\theta}_o,t) = \boldsymbol{G}_2-t\boldsymbol{g}_1\boldsymbol{g}_1^T\). It is clear that matrix \(\boldsymbol{J}\) is proportional to the inverse of the variance-covariance matrix \eqref{eq:cov-matrix}. For the rest of the thesis, we will be working on the design problems using matrix \(\boldsymbol{J}\).

As discussed in Chapter \ref{intro}, we aim to minimize the loss functions in optimal design problems, and the loss functions for D-, A- and c-optimality criteria under SLSE can be expressed as
\begin{equation}
\begin{aligned}
  \phi_D(\xi,\boldsymbol{\theta_o},t)&=&\det(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
  \phi_A(\xi,\boldsymbol{\theta_o},t)&=&\operatorname{tr}(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
  \phi_c(\xi,\boldsymbol{\theta_o},t)&=&\boldsymbol{c}_1^\top\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{c}_1,
\end{aligned}
\label{eq:loss-J}
\end{equation}
when \(\boldsymbol{J}\) is non-singular, and \(\boldsymbol{c}_1\) is a given vector in \(\mathbb{R}^q\). If \(\boldsymbol{J}\) is singular, all the three loss functions are defined to be \(+\infty\). We use \(\xi_D^*,~\xi_A^*\) and \(\xi_c^*\) to denote for \text{D-,} A- and c-optimal designs, respectively. For two measures \(\xi_1\) and \(\xi_2\in \Xi\), define \(\xi_{\alpha}=(1-\alpha)\xi_1+\alpha \xi_2\) for \(\alpha \in[0,1]\).

\begin{lemma}
\(\log(\phi_D(\xi_{\alpha},\boldsymbol{\theta_o},t))\), \(\phi_A(\xi_{\alpha},\boldsymbol{\theta_o},t)\) and \(\phi_c(\xi_{\alpha},\boldsymbol{\theta_o},t)\) are convex functions of \(\alpha\).
\end{lemma}

The convexity results are discussed in \citet{boyd2004convex} and \citet{wong2019cvx}. Similar convexity results are given in \citet{bose2015optimal}. We will use \(\log(\det(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)))\) for D-optimal design for the rest of the thesis as \(\log(\cdot)\) is a monotonic increasing function which does not change the optimality.

Although we have formulated the loss functions, there are some issues associated with the formulation in @ref\{eq-loss-J\}. The reason is that \(\boldsymbol{J}\) is lacking of linearity. From construction of \(\boldsymbol{J}\), \(\boldsymbol{J}(\xi_{\alpha},\boldsymbol{\theta_o},t)\) is not a linear combination of \(\boldsymbol{J}(\xi_{1},\boldsymbol{\theta_o},t)\) and \(\boldsymbol{J}(\xi_{2},\boldsymbol{\theta_o},t)\). Thus, it is difficult to obtain the theoretical results using \(\boldsymbol{J}\). To solve this issue, \citet{gao2017d} proposed an alternative expression for characterizing the loss functions. The key is to define a matrix
\begin{equation}
\boldsymbol{B}(\xi)=\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1               &   \sqrt{t}\boldsymbol{g_1}^\top\\
\sqrt{t}\boldsymbol{g}_1    &   \boldsymbol{G}_2
\end{pmatrix},
 \label{eq:B-matrix}
\end{equation}
which plays an important role in the following formulation. Note \(\boldsymbol{B}(\xi_{\alpha})\) is now an affine function of \(\alpha\), i.e.,
\[ \boldsymbol{B}(\xi_{\alpha})=(1-\alpha)\boldsymbol{B}(\xi_1)+\alpha\boldsymbol{B}(\xi_2).\]
This fact ultimately makes \(\boldsymbol{B}\) much more useful than \(\boldsymbol{J}\) to study optimal designs under SLSE. The inverse of \(\boldsymbol{B}\) is given as
\begin{equation}
\boldsymbol{B}^{-1}(\xi)=\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)   =\begin{pmatrix}
\frac{1}{r}             &   \frac{-\sqrt{t}}{r}\boldsymbol{g}_1^\top\boldsymbol{G}_2^{-1}   \\
\frac{-\sqrt{t}}{r}\boldsymbol{G}_2^{-1}\boldsymbol{g}_1    &\boldsymbol{J}^{-1}
\end{pmatrix},
 \label{eq:B-inverse}
\end{equation}
where \(r=1-t\boldsymbol{g}_1^\top\boldsymbol{G}_2^{-1}\boldsymbol{g}_1\). Note that if \(\boldsymbol{J}\) is invertible, \(\boldsymbol{G}_2\) must also be invertible since \(\boldsymbol{G}_2=\boldsymbol{J}+t\boldsymbol{g}_1\boldsymbol{g}_1^\top\) and \(t\boldsymbol{g}_1\boldsymbol{g}_1^\top\) is positive semi-definite. Consequently, \(\boldsymbol{B}^{-1}\) exists from \eqref{eq:B-inverse}. Now, we are going to present the following lemmas to characterize the loss functions for A-, c- and D-optimal design problems. Lemma \ref{lem:loss-A} is slightly different from a result in \citet{yin2018optimal}, Lemma \ref{lem:loss-D} is a result from \citet{gao2017d}, and Lemma \ref{lem:loss-c} is a new result.

\begin{lemma}
\protect\hypertarget{lem:loss-A}{}\label{lem:loss-A}If \(\boldsymbol{J}\) is invertible, then
\begin{equation*}
  \phi_A(\xi,\boldsymbol{\theta}_o,t)=\operatorname{tr}(\boldsymbol{J}^{-1})=\operatorname{tr}(\boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C}),
  \end{equation*}
where \(\boldsymbol{C}=0\oplus \boldsymbol{I}_q\), \(\boldsymbol{I}_q\) denotes for the \(q\times q\) identity matrix, and \(\oplus\) denotes for matrix direct sum operator.
\end{lemma}

\begin{proof}
From \eqref{B inverse} and \(\boldsymbol{C}=0\oplus \boldsymbol{I_q}\), we get
\begin{align*}
    \boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C}&=  \begin{pmatrix}
                                0   &   0\\
                                0   &   \boldsymbol{I}_q
                                \end{pmatrix}^T\begin{pmatrix}
            \frac{1}{r}             &   \frac{-\sqrt{t}}{r}\boldsymbol{g_1}^\top\boldsymbol{G}_2^{-1}   \\
            \frac{-\sqrt{t}}{r}\boldsymbol{g}_1\boldsymbol{G}_2^{-1}    &\boldsymbol{J}^{-1}
            \end{pmatrix}\begin{pmatrix}
                                0   &   0\\
                                0   &   \boldsymbol{I}_q
                                \end{pmatrix}\\
                            &= \begin{pmatrix}
                                0   &   0\\
                                0   &   \boldsymbol{J}^{-1}
                                \end{pmatrix},
    \end{align*}
which implies \(\operatorname{tr}(\boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C})=\operatorname{tr}(\boldsymbol{J}^{-1})\).
\end{proof}

\begin{lemma}
\protect\hypertarget{lem:loss-D}{}\label{lem:loss-D}If \(\boldsymbol{J}\) is invertible, then
\begin{equation*}
    \phi_D(\xi,\boldsymbol{\theta_o},t)=\det(\boldsymbol{J}^{-1})=\det(\boldsymbol{B}^{-1}).
\end{equation*}
\end{lemma}

From @ref\{eq:B-matrix\}, we have
\begin{align*}
    \det(\boldsymbol{B})    &= \det(1-t\boldsymbol{g_1}^T\boldsymbol{G_2}^{-1}\boldsymbol{g_1})\det(\boldsymbol{G_2})\\
            &= \det(\boldsymbol{I}-t\boldsymbol{g_1}\boldsymbol{g_1}^\top\boldsymbol{G_2}^{-1}) \det(\boldsymbol{G_2}) \\
            &=  \det(\boldsymbol{G_2}-t\boldsymbol{g_1}\boldsymbol{g_1}^T)\\
            &= \det(\boldsymbol{J}),
    \end{align*}
which gives \(\det(\boldsymbol{J}^{-1})=\det(\boldsymbol{B}^{-1})\).

\begin{lemma}
\protect\hypertarget{lem:loss-c}{}\label{lem:loss-c}If \(\boldsymbol{J}\) is invertible, then
\begin{equation*}
    \phi_c(\xi,\boldsymbol{\theta_o},t)=\boldsymbol{c}_1\top\boldsymbol{J}^{-1}\boldsymbol{c}_1=\boldsymbol{c}^\top\boldsymbol{B}^{-1}\boldsymbol{c},
\end{equation*}
where \(\boldsymbol{c_1}\) is a vector in \(\mathbb{R}^q\) and \(\boldsymbol{c}^\top=(0,\boldsymbol{c}_1^\top)\).
\end{lemma}

Thus, by Lemmas \ref{lemma:loss A}, \ref{lemma:loss D} and \ref{lemma:loss c}, the alternative expressions for the loss functions in \eqref{loss J} are
\begin{equation} \label{loss B}
\begin{aligned}
    \phi_D(\xi,\boldsymbol{\theta_o},t))&=&\det(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
    \phi_A(\xi,\boldsymbol{\theta}_o,t)&=&\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{C}),\\
    \phi_c(\xi,\boldsymbol{\theta_o},t)&=&\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{c},
\end{aligned}
\end{equation}
where \(\boldsymbol{C}= 0 \oplus \boldsymbol{I_q}\), \(\boldsymbol{c_1}\in \mathbb{R}^q\) and \(\boldsymbol{c}^T=(0,\boldsymbol{c_1}^T)\). If \(\boldsymbol{B}\) is singular, all the three loss functions are defined to be \(+\infty\).

\section{Equivalence theorem for optimal designs under SLSE}\label{equivalence-theorem-for-optimal-designs-under-slse}

In this section we derive the optimality conditions for the optimal designs under the SLSE which follows from the equivalence theorem in \citet{kiefer-wolfowitz1959optimum} and \citet{kiefer1974general}. We also analyze the minimum number of support points in optimal designs for various regression models, and theoretical results are obtained. Note we study approximate designs in this thesis. The advantages of working with approximate designs instead of exact design are well documented in \citet{kiefer1985jack}.

Define a vector \(f(\boldsymbol{x,\theta_o})=\frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{ \partial \boldsymbol{\theta}}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta_o}} \in \mathbb{R}^q\) and a matrix
\begin{equation}
\boldsymbol{M}(\boldsymbol{x})=\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)=\begin{pmatrix}
1       &   \sqrt{t}f^T(\boldsymbol{x},\boldsymbol{\theta_o})\\
\sqrt{t}f(\boldsymbol{x},\boldsymbol{\theta_o}) &f(\boldsymbol{x},\boldsymbol{\theta_o})f^T(\boldsymbol{x},\boldsymbol{\theta_o})
\end{pmatrix}_{(q+1)\times (q+1)}.
 \label{eq:M-matrix}
\end{equation}

Then \(\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)=\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)]\). Define dispersion functions
\begin{equation} \label{fuction:dispersion}
\begin{aligned}
  &d_D(\boldsymbol{x},\xi,t) = \operatorname{tr}(\boldsymbol{B}^{-1}\boldsymbol{M}(\boldsymbol{x}))-(q+1),\\
  &d_A(\boldsymbol{x},\xi,t) = \operatorname{tr}(\boldsymbol{M}(\boldsymbol{x})\boldsymbol{B}^{-1}\boldsymbol{C}^T\boldsymbol{C}\boldsymbol{B}^{-1})-\operatorname{tr}(\boldsymbol{C}\boldsymbol{B}^{-1}\boldsymbol{C}^T),\\
  &d_c(\boldsymbol{x},\xi,t) = \boldsymbol{c}^T\boldsymbol{B}^{-1}\boldsymbol{M}(\boldsymbol{x})\boldsymbol{B}^{-1}\boldsymbol{c}-\boldsymbol{c}^T\boldsymbol{B}^{-1}\boldsymbol{c},\\
\end{aligned}
\end{equation}
where \(\boldsymbol{B}=\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)\) is invertible.

\begin{theorem}
\protect\hypertarget{thm:dispersion}{}\label{thm:dispersion}We suppose all the dispersion functions are evaluated at \(\boldsymbol{\theta_o}\). If \(\xi_D^*\), \(\xi_A^*\) and \(\xi_c^*\) are the optimal probability measures for D-, A- and c- optimality, respectively, then \(\boldsymbol{B}\) is invertible and for any \(\boldsymbol{x}\in S\),
\begin{align}
d_D(\boldsymbol{x},\xi_D^*,t) &\leq 0, \label{eq:dire-D} \\
d_A(\boldsymbol{x},\xi_A^*,t) &\leq 0, \label{eq:dire-A} \\
d_c(\boldsymbol{x},\xi_c^*,t) &\leq 0. \label{eq:dire-c}
\end{align}
\end{theorem}

\begin{proof}
In this proof, we use \(\boldsymbol{B}(\xi)\) for \(\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)\) and \(\boldsymbol{M}(\boldsymbol{x})\) for \(\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)\). Suppose \(\xi^*\) is an optimal design to a criterion. Define \(\xi_\alpha=(1-\alpha)\xi^*+\alpha \xi\) where \(\xi\) is an arbitrary probability measure. This proof is based on Kiefer's general equivalence theorem \citep{kiefer1974general}, and the optimal condition can be derived from \(\frac{\partial \phi(\xi_\alpha)}{\partial \alpha}\big|_{\alpha=0}\geq 0\) for any measure \(\xi \in \Xi\), where \(\phi\) is a loss function.

We first prove \eqref{eq:dire-D}. Let \(\xi_D^*\) be the optimal measure under D-optimality. We have
\begin{align*}
    \frac{\partial \log(\phi_D(\xi_\alpha))}{\partial \alpha}\Big|_{\alpha=0}   
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)(-\boldsymbol{B}(\xi_D^*)+\boldsymbol{B}(\xi)))\\
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\boldsymbol{B}(\xi))+\operatorname{tr}(\boldsymbol{I_{q+1}})\\
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\boldsymbol{B}(\xi))+(q+1))\\
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]-(q+1))\\
    &=-\mathbb{E}_{\xi}[d_D(\boldsymbol{x},\xi_D^*,t)]\\
    &\geq 0,~\text{for any } \xi \text{ on } S,
    \end{align*}
which implies \(d_D(\boldsymbol{x},\xi_D^*,t)\leq 0\), for all \(\boldsymbol{x} \in S\).

To prove \eqref{eq:dire-A}, let \(\xi_A^*\) be the optimal measure under A-optimality. We have
\begin{align*}
    \frac{\partial \phi_A(\xi_\alpha)}{\partial \alpha}\Big|_{\alpha=0} 
    &=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)[\boldsymbol{B}(\xi)-\boldsymbol{B}(\xi_A^*)]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})\\
    &=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{B}(\xi)\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})+\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})\\
    &=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}) +\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi^*)\boldsymbol{C})\\
    &= - (\operatorname{tr}(\mathbb{E}_{\xi}[\boldsymbol{M}_{\xi}(\boldsymbol{x},\boldsymbol{\theta_o})]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi^*))-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}) )\\
    &= -\mathbb{E}_{\xi}[d_A(\boldsymbol{x},\xi_A^*,t)]\\
    &\geq 0,~ \text{for any } \xi \text{ on } S,
    \end{align*}
which implies \(d_A(\boldsymbol{x},\xi_A^*,t)\leq 0\), for all \(\boldsymbol{x} \in S\).

Lastly, to prove \eqref{eq:dire-D}, let \(\xi_c^*\) be the optimal measure under c-optimality. We have
\begin{align*}
    \frac{\partial \phi_c(\xi_\alpha)}{\partial \alpha}\Big|_{\alpha=0} 
    &=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)[-\boldsymbol{B}(\xi)+\boldsymbol{B}(\xi_c^*)]\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}\\
    &=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{B}(\xi)\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{B}(\xi_c^*)\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}\\
    &=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}(\xi_c^*)^{-1}\boldsymbol{c}\\
    &=-\mathbb{E}_{\xi}[\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{M}(\boldsymbol{x}) \boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}(\xi_c^*)^{-1}\boldsymbol{c}]\\
    &=-\mathbb{E}_{\xi}[d_c(\boldsymbol{x},\xi_c^*,t)]\\
    &\geq 0,~ \text{for any } \xi \text{ on } S,
    \end{align*}
which implies \(d_c(\boldsymbol{x},\xi_c^*,t)\leq 0\), for all \(\boldsymbol{x} \in S\).
\end{proof}

\section{Results on the number of support points}\label{section-support-pt}

Using the results in Theorem \ref{thm:dispersion} , we can explore the properties of the optimal designs. In \citet{yin2018optimal} and \citet{gao2017d}, there are some discussions about the number of support points based on computational results. However, there is little discussion on the number of support points theoretically in \citet{gao2017d}, and there is still a large gap to be filled in. Hence we derive several results about the number of support points for various models, including polynomial models, fractional polynomial models, Michaelis-Menton model, Peleg model and trigonometric models.

A polynomial regression model of degree \(q\) (\(q \ge 1\)) without intercept is given by
\begin{equation}
y_i=\theta_1 x_i+\theta_2 x_i^2+\cdots+\theta_q x_i^q+\epsilon_i,~x_i\in S=[-1,+1],~i=1,2,\cdots,n.
\label{eq:model-poly}
\end{equation}
Polynomial regression models are widely used when the response and regressors have curvilinear relationship. Complex nonlinear relationships can be well approximated by polynomials over a small range of the explanatory variables \citep[p.~223]{montgomery2012introduction}. There are different kinds of polynomial models such as orthogonal polynomial models, multi-variable polynomial models, and one variable polynomial models. Polynomial models are often used in design of experiment for the response surface methodology, and there are many applications in industry. For example, see \citet{box1987empirical}, \citet{box1978statistics} and \citet{khuri1996response}.

A- and D-optimal designs for \eqref{eq:model-poly} under SLSE are symmetric on \(S\) {[}\citet{yin2018optimal},\citet{gao2017d}\}. In \eqref{eq:model-poly}, we have
\(\boldsymbol{f}(x,\boldsymbol{\theta})= \left(x,x^2,\cdots, x^q \right)^\top\) and
\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=\begin{pmatrix}
1           &\sqrt{t}x  &\sqrt{t}x^2    &...&\sqrt{t}x^q\\
\sqrt{t}x   &   x^2     &x^3    &\cdots &x^{q+1}\\
\vdots      &\vdots     &\vdots &\vdots&    \vdots\\
\sqrt{t}x^q &   x^{q+1} &\dots  &\dots  &x^{2q}
\end{pmatrix}_{(q+1)\times(q+1)}.
\label{eq:poly}
\end{equation}

\begin{theorem}
\protect\hypertarget{thm:support}{}\label{thm:support}Let \(n_A\) and \(n_D\) denote the minimum number of support points in A- and D-optimal designs under SLSE, respectively. For \eqref{eq:model-poly}, we have
\begin{equation} \label{support}
n_A ~ = ~ \text{q or q+1},
\end{equation}
and
\begin{equation} \label{support2}
n_D ~ = ~ \text{q or q+1}.
\end{equation}
\end{theorem}

\begin{proof}
The proof includes the following three parts.

(i). From @ref(thm:dispersion\} and \eqref{eq:poly}, we can see that \(d_A(x,\xi_A^*,t)\) and \(d_D(x,\xi_D^*,t)\) are polynomial functions of \(x\) with highest degree \(2q\). By fundamental theorem of algebra, there are exactly \(2q\) roots for \(x\) in equations \(d_A(x,\xi_A^*,t)=0\) and \(d_D(x,\xi_D^*,t)=0\). However, we have at most \(2q\) real roots.

(ii). By the construction, the determinant of \(\boldsymbol{B}\) matrix is not zero if and only if the determinant of \(\boldsymbol{G_2}\) is not zero. Therefore, there are at least q support points in \(\xi\).

(iii). Both boundary points are the support points, so the number of support points are at most \(2q-2\) in the interval \((-1, +1)\). From the equivalence theorem, we know that the dispersion functions are all less or equal to zero (i.e.~\(d_A(x,\xi_A^*,t)\leq 0\) and \(d_D(x,\xi_D^*,t)\leq 0\)), so all those support points in \((-1,+1)\) have a multiplicity of two. In total, we have at most \(2+\frac{(2q-2)}{2}=q+1\) distinct support points.

Thus, the number of support points in \(\xi_A^*\) and \(\xi_D^*\) is either \(q\) or \(q+1\).
\textbackslash end\{proof\}
\end{proof}

\begin{example}
\protect\hypertarget{exm:poly2}{}\label{exm:poly2}Consider \eqref{eq:model-poly} with \(q=2\) and \(S=[-1,+1]\). Let \(\eta_m=\mathbb{E}[x^m]\) be the \(m\)-th moment of distribution \(\xi(x)\). \citet{gao2014new} showed that the D-optimal design is symmetric in this case (i.e.~\(\eta_1=\eta_3=0\)), and is given by

\begin{equation*} 
\xi_D^* = \begin{cases} 
    \begin{bmatrix}
    -1          &   +1\\
    \frac{1}{2} &   \frac{1}{2}
    \end{bmatrix},      & \mbox{for $t\in [0,\frac{2}{3})$}, \\
    \begin{bmatrix}
    -1           &  0       &+1\\
    \frac{1}{3t} &\frac{3t-2}{3t}           &\frac{1}{3t}
    \end{bmatrix},                                  & \mbox{for $t\in [\frac{2}{3},1)$}.\\
\end{cases}
\end{equation*}

We now study the A-optimal design. By taking the advantage of the symmetric result in \citet{yin2018optimal}, we have
\[
    \boldsymbol{B}(\xi)=
    \begin{pmatrix}
    1           &0      &\sqrt{t}\eta_2\\
    0           &\eta_2 &0  \\
    \sqrt{t}\eta_2 &    0   &   \eta_4
    \end{pmatrix},~\text{and }
    \boldsymbol{B}^{-1}(\xi)=\begin{pmatrix} 
        \frac{\eta_4}{\eta_4-t\eta_2}&0         &\frac{\sqrt{t}\eta_2}{t\eta_2^2-\eta_4}\\
        0       &\frac{1}{\eta_2}       &0\\
       \frac{\sqrt{t}\eta_2}{t\eta_2^2-\eta_4}&0&\frac{1}{\eta_4-t\eta_2^2} 
    \end{pmatrix}.
\]
From \citet{dette1997theory}, on \(S=[-1,+1]\), the even moments of any distributions must satisfy \(0\leq \eta_2^2 \leq \eta_4 \leq \eta_2 \leq 1\). Then our loss function can be expressed as
\[
  \phi_A(\xi)=\operatorname{tr}(\boldsymbol{C}^\top \boldsymbol{B}^{-1}\boldsymbol{C})= \frac{1}{\eta_2}+\frac{1}{\eta_4-t\eta_2^2}.
\]
The optimal design problem can be written as
\begin{equation*}
        \begin{aligned}
            & \underset{\eta_2,\eta_4}{\text{min}}
            & & \frac{1}{\eta_2}+\frac{1}{\eta_4-t\eta_2^2} \\
            & \text{s.t.}
            & & 0\leq \eta_2^2 \leq \eta_4 \leq \eta_2 \leq 1.
        \end{aligned}
    \end{equation*}
In order to minimize the loss function, we first fix \(\eta_2\). Then it is trivial to see we should make \(\eta_4\) as big as possible, which leads to \(\eta_4=\eta_2\), its ceiling. Now the question becomes
\begin{equation*}
        \begin{aligned}
            & \underset{\eta_2}{\text{min}}
            & & \frac{1}{\eta_2}+\frac{1}{\eta_2-t\eta_2^2} \\
            & \text{s.t.}
            & & 0\leq  \eta_2 \leq 1.
        \end{aligned}
    \end{equation*}
It yields to \(\eta_2=\frac{2-\sqrt{2}}{t}\) or \(\eta_2=\frac{2+\sqrt{2}}{t}\) where \(t\ne 0\). Note \(t\in[0,+1)\) and \(\eta_2\in[0,+1]\), so the latter solution should be excluded as it is not within the feasible region. Consequently,
\begin{equation*} 
\eta_2 = \begin{cases} 
    \min{(\frac{2-\sqrt{2}}{t},1)},     & \mbox{$t \in(0,1)$}, \\
    1,                                  & \mbox{$t = 0$}.\\
\end{cases}
\end{equation*}
By the symmetric property, the A-optimal design is
\[  \xi_A^*=
    \begin{bmatrix}
    -1                  &   0           &   +1\\
    \frac{\eta_2}{2}    &   1-\eta_2    &   \frac{\eta_2}{2}
    \end{bmatrix}.
\]
Here we have three cases:\textbackslash{}
\textbf{Case 1}
When \(t=0\),
\[  \xi_A^*=
    \begin{bmatrix}
    -1              &   +1\\
    \frac{1}{2}     &   \frac{1}{2}
    \end{bmatrix}.
\]
\textbf{Case 2}
When \(t\in(0,2-\sqrt{2})\),
\[  \xi_A^*=
    \begin{bmatrix}
    -1              &   +1\\
    \frac{1}{2}     &   \frac{1}{2}
    \end{bmatrix}.
\]
\textbf{Case 3}
When \(t\in[2-\sqrt{2},1)\),
\[  \xi_A^*=
    \begin{bmatrix}
    -1                  &   0           &   +1\\
    \frac{2-\sqrt{2}}{2t}   &   1-\frac{2-\sqrt{2}}{t}  &   \frac{2-\sqrt{2}}{2t}
    \end{bmatrix}.
\]
In summary, the A-optimal design for \eqref{eq:model-poly} when \(q=2\) is
\begin{equation*} 
\xi_A^* = \begin{cases} 
    \begin{bmatrix}
    -1          &   +1\\
    \frac{1}{2} &   \frac{1}{2}
        \end{bmatrix},      & \mbox{for $t\in[0, \leq 2-\sqrt{2})$}, \\
    \begin{bmatrix}
    -1          &   0       &+1\\
    \frac{2-\sqrt{2}}{2t}   &\frac{2t+\sqrt{2}-2}{t}            &\frac{2-\sqrt{2}}{2t}
\end{bmatrix},                                  & \mbox{for $t\in[2-\sqrt{2},1)$}.\\
\end{cases}
\end{equation*}

From the results above, it is clear to observe that A- and D-optimal designs have either \(2\) or \(3\) support points which is consistent with Theorem \ref{thm:support}.
\hfill\(\Box\)
\end{example}

To show the result in Theorem \ref{thm:dispersion}, we plot \(d_A(x,\xi^*,t)\) and \(d_D(x,\xi^*,t)\) in Figure {[}TOFILL{]} using the optimal designs in Example \ref{exm:poly2}. It is clear that \(d_A(x,\xi^*,t) \leq 0\) and \(d_D(x,\xi^*,t)\leq 0\) for all \(x\in S\), which is consistent with Theorem \ref{theorem:dispersion}. Also \(d_A(x,\xi_A^*,t)=0\) at the support points of \(\xi_A^*\) and \(d_D(x,\xi_D^*,t)=0\) at the support points of \(\xi_D^*\). As \(t\) increases, the origin becomes a support point which changes the number of support points from 2 to 3. This is again consistent with Theorem \ref{thm:support}.

When the design space \(S\) is asymmetric, say \(S=[-1,b]\), \(b\in(-1,1)\), the number of support points under A- and D-optimality are either \(q\) or \(q+1\) for all \(q\in \mathbb{Z^+}\). When \(t=0\), \(n_D=q\) for all \(q\in\mathbb{Z^+}\). The derivation is similar to that of Theorem \ref{thm:support} and is omitted. We will show some computational results in Chapter \ref{chapter:applications}.

Fractional polynomial model (FPM) is given by
\begin{equation}
 y=\theta_1x^{q_1}+\theta_2x^{q_2}+\cdots+\theta_px^{q_p}+\epsilon,~q_i\in\mathbb{Q},~\forall i,
 \label{eq:model-frac-poly}
\end{equation}
which provides more flexible parameterization with wide range of applications in many disciplines. For instance, \citet{cui2009fractional} used FPM for longitudinal epidemiological studies, and \citet{royston1995regression} applied this model to analyze medical data. This model has also been studied for optimal designs. For example, \citet{torsney995minimally} used FPM to study the effects of concentration \(x\) to viscosity \(y\) and this model is given by
\begin{equation}
    y=\theta_1x+\theta_2x^{\frac{1}{2}}+\theta_3x^2+\epsilon,~x\in S=(0.0,0.2].
    \label{eq:ex-frac-poly}
\end{equation}
Let \(z=x^{1/2}\), then @ref(exm:frac\_poly) becomes a polynomial model of order \(4\) with the new design space \(S'=(0.0,\sqrt{0.2}]\). Now, we can apply the result for the polynomial model \eqref{eq:model-poly} and conclude that \(n_A\) and \(n_D\) are at most \(4\) or \(5\). Moreover, notice that there are only three parameters in model \eqref{ex:frac_poly}. In order to have an invertible matrix \(\boldsymbol{G_2}(\xi,\boldsymbol{\theta}_o)\) which is a \(3\times 3\) matrix, we need at least \(3\) support points. Thus, the number of support point for both A- and D-optimal design is at least 3. In summary, the number of support points in A- and D-optimal designs are either \(3\), \(4\), or \(5\). The purpose of this example is for illustrating that FPM can be transformed into a polynomial model so we can use the result for the number of support points in polynomial models.

Michaelis-Menton model is one of the best-known models proposed by Leonor Michaelis and Maud Menten \citep{mm1913kinetik} that studies the enzyme reactions between the enzyme and the substrate concentration. This model is given by
\begin{equation}
y_i=\frac{\theta_1x_i}{\theta_2+x_i}+\epsilon_i.~ i=1,2,\dots,n,~ x_i \in(0,                                                                k_o],~\theta_1,\theta_2\geq 0.
\label{eq:model-mm}
\end{equation}
Here, \(\boldsymbol{\theta}=(\theta_1,\theta_2)^\top\). Define \(z=\frac{1}{\theta_2+x}\). Then \(\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{x}{\theta_2+x},\frac{-\theta_1x}{(\theta_2+x)^2})^T= (1-z\theta_2, -z\theta_1 +\theta_1 \theta_2z^2)^\top\), and
\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1                       & \sqrt{t} (1-z\theta_2)        &   \sqrt{t}(-\theta_1 z(1-z\theta_2))\\
\sqrt{t} (1-z\theta_2)  &(1-z\theta_2)^2            &   -z\theta_1(1-z\theta)^2\\
\sqrt{t}(-\theta_1 z(1-z\theta_2))&-z\theta_1(1-z\theta)^2&(z\theta_1(1-z\theta_2))^2
\end{pmatrix}.
\end{equation}

We can clearly see that, after the transformation, \(z\in S'=[\frac{1}{\theta_1+k_o},\frac{1}{\theta_1}]\), and since \(\boldsymbol{f}(x,\boldsymbol{\theta_o})\) and \(\boldsymbol{M}(x,\boldsymbol{\theta},t)\) are polynomial functions of \(z\), \(\phi_D\) and \(\phi_A\) can be described as polynomial functions with highest degrees \(4\). Now, similar to the discussion for model \eqref{eq:model-poly}, we can find the results for \(n_A\) and \(n_D\) for model @ref(eq:model-mm\} and they are presented in the following Lemma.

\begin{lemma}
\protect\hypertarget{lem:mm-support}{}\label{lem:mm-support}For Michaelis-Menton model in \eqref{eq:model-mm}, the number of support points for D-optimal design and A-optimal design are either \(2\) or \(3\) for \(t\in[0,1)\). Moreover, there are 2 support points under D-optimality when \(t=0\).
\end{lemma}

The proof of Lemma \ref{lem:mm-support} is similar to that of Theorem \ref{theorem:support} and is omitted. It is easy to observe that \(d_D(0,\xi_D^*,0)<0\), so the boundary point, 0, is not a support point under D-optimality which leads to the conclusion \(n_D=2\). Moreover, \(n_c\le 3\) for \(t\in[0,1)\).

Peleg model is a statistical model used to investigate the relationship of water absorption for various kinds of food. This model is given by
\begin{equation} 
y_i=y_o+\frac{x_i}{\theta_1+\theta_2 x_i}+\epsilon_i,\quad i=1,2,\cdots,n,
\label{eq:model-peleg}
\end{equation}
where \(y_o\) represents the initial moisture of the food, \(y_i\) is the current level of moisture at current time \(x_i\), \(\theta_1\) is the Peleg's moisture rate constant, and \(\theta_2\) is the asymptotic moisture as time increases. Note that parameters \(\theta_1\) and \(\theta_2\) must be positive. Optimal experimental designs have been studied for this model using OLSE, for example, \citet{paquet2015optimal}. For \eqref{eq:model-peleg}, \(\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{-x}{(\theta_1+\theta_2x_i)^2},\frac{-x^2}{(\theta_1+\theta_2x)^2})^T\). We let \(z=\frac{1}{\theta_1+\theta_2x}\). Then for \(S=[0,d]\), the new design space after the transformation is \(S'=[\frac{1}{\theta_1+\theta_2d},\frac{1}{\theta_1}]\). Now, \(\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{-z(1-\theta_1z)}{\theta_2},\frac{-(1-\theta_1z)^2}{\theta_2^2})^T\). Since \(\theta_1\) and \(\theta_2\) are parameters, \(\boldsymbol{f}\) depends on \(z\) only. Hence, \(\boldsymbol{M}(x,\boldsymbol{\theta},t)\) becomes a polynomial function of \(z\) of degree \(4\). Therefore, the number of support points, \(n_A\) and \(n_D\) are also either 2 or 3 for the Peleg model. In addition, \(n_c\) is either 1, 2 or 3 for \(t\in[0,1)\).

Next we consider trigonometric models without intercept. If the intercept term is included, it has been proven that optimal designs under SLSE and OLSE are the same. The \(k\)th order trigonometric model is given by
\begin{equation} 
    y_i=\sum_{j=1}^{k}[cos(jx_i)\theta_{1j}+sin(jx_i)\theta_{2j}]+\epsilon_i,~i=1,2,\cdots,n,
    \label{eq:model-trig}
\end{equation}
where \(x_i\in S_b=[-b\pi,b\pi],~0<b\leq 1\). Let \(\boldsymbol{\theta}=(\theta_{11},\theta_{12},\cdots,\theta_{1k},\theta_{21},\cdots,\theta_{2k})^T\). From \eqref{eq:M-matrix}, we get
\begin{equation} 
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1       &\sqrt{t}cos(x) &   \dots       &\sqrt{t}sin(kx)\\
\sqrt{t}cos(x)  &cos^2(x)   &\dots  &cos(x)sin(kx)\\
\vdots  &\vdots &   \ddots  &\vdots\\
\sqrt{t}sin(kx) &cos(x)sin(kx)&\dots    &sin^2(kx)
\end{pmatrix}_{(2k+1)\times(2k+1)}.
\label{eq:trig-M-matrix}
\end{equation}
We consider two cases of design space, (i) \(b=1\), full circle and (ii) \(0<b<1\), the partial circle.

For case (i), the following Lemma is helpful for finding the number of support points for \eqref{eq:model-trig}.

\begin{lemma}
\protect\hypertarget{lem:tring-property}{}\label{lem:tring-property}

For trigonometric functions, \(j, u =1,2,\cdots,k,~u\ne j\), we have

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\int_{-\pi}^{\pi}cos(jx)dx=\int_{-\pi}^{\pi}sin(jx)dx=0\),
\item
  \(\int_{-\pi}^{\pi}cos^2(jx)dx=\int_{-\pi}^{\pi}sin^2(jx)dx=\pi\),
\item
  \(\int_{-\pi}^{\pi}cos(jx)cos(ux)dx=\int_{-\pi}^{\pi}sin(jx)sin(ux)dx=0\),
\item
  \(\int_{-\pi}^{\pi}sin(jx)cos(ux)dx=\int_{-\pi}^{\pi}cos(jx)sin(ux)dx=0\). This also holds for \(u=j\).
\end{enumerate}

\end{lemma}

\begin{theorem}
For model \eqref{eq:model-trig} with design space \(S=[-\pi,\pi]\), the uniform distribution on \(S\) is both A- and D-optimal designs.
\end{theorem}

\begin{proof}
For the uniform distribution \(\xi^*\), we have \(\boldsymbol{B}(\xi^*,\boldsymbol{\theta_o},t)=1\oplus \frac{1}{2} \boldsymbol{I_{2k}}\) by Lemma \ref{lem:trig-property}. From \eqref{eq:trig-M-matrix}, we obtain
\begin{equation*}
\begin{aligned}
&tr(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))-(q+1)\\    &=1+2cos^2(x)+2cos^2(2x)+\dots+2sin^2(kx)-(2k+1)\\
&=1+2[cos^2(x)+sin^2(x)]+\dots+2[cos^2(kx)+sin^2(kx)]-(2k+1)\\
        &=1+2k-(2k+1)\\
        &=0,\quad \text{for all } x\in S.
\end{aligned}
\end{equation*}
This implies that \(d_D(x,\xi^*,t)=0\) for all \(x\in S\). By Theorem \ref{thm:dispersion}, \(\xi^*\) is a D-optimal design.

For A-optimality, it is easy to get
\begin{equation*}
\begin{aligned}
&tr(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t)\boldsymbol{C}^T\boldsymbol{C}\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))-\operatorname{tr}(\boldsymbol{C}\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))\\&=4cos^2(x)+4cos^2(2x)+\dots+4sin^2(kx)-4k\\
&=4[cos^2(x)+sin^2(x)]+\dots+4[cos^2(kx)+sin^2(kx)]-4k\\
&=4k-4k\\
&=0,\quad \text{for all } x\in S,
\end{aligned}
\end{equation*}
which gives \(d_A(x,\xi^*,t)=0\) for all \(x \in S\). Thus, by Theorem \ref{thm:dispersion}, \(\xi^*\) is an A-optimal design.
\end{proof}

For case (ii), \(0<b<1\), the partial circle \(S=[-b\pi,+b\pi]\), let \(z=cos(x)\). Now, instead of using \(x\) directly, we study the number of support point of \(x\) through \(z\) in \(S'=[cos(b\pi),1]\). Note that cosine is an even function, so each point of \(z\in S'\) corresponds to two symmetric points around \(0\), \(\pm x\in S\). \citet{gao2017d} discussed that all the elements in \(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\) in \eqref{eq:trig-M-matrix} can be written as polynomial functions of \(z\). Hence, functions \(d_A(x,\xi,t)\), \(d_D(x,\xi,t)\) and \(d_c(x,\xi,t)\) are all polynomial functions of \(z\) with degree \(2k\). Using the similar arguments for the polynomial models, A-, D- and c-optimal designs, in terms of \(z\in S'\), cannot exceed \(k+1\) support points.

\section{Scale invariance property of D-optimal design}\label{scale-invariance-property-of-d-optimal-design}

D-optimality is one of the most used design criteria due to its many advantages. One good property of this criterion is that it is invariant under some scaling of the independent variables using OLSE \citep{berger2009introduction}. Recently there are some discussions about the invariance properties including scale invariance and shift invariance for the optimal designs under the SLSE. Examples can be found in \citet{gao2014new} and \citet{yin2018optimal}. In this section, we focus on finding a new property of scale invariance of D-optimal designs under the SLSE for nonlinear regression models.

For linear regression models, D-optimal designs are often scale invariant. On the other hand, if the model is nonlinear, the scale invariance property is no longer available. The optimal designs for nonlinear models are called locally optimal, since they depend on the true parameter vector \(\boldsymbol{\theta_o}\). Thus, D-optimal designs have to be constructed for each \(\boldsymbol{\theta_o}\) and for each \(S\). \citet{wong2019cvx} proposed a generalized scale invariance (GSI) concept for studying scale invariance property of D-optimal designs for nonlinear models and generalized linear models. The GSI property is also useful for studying D-optimal designs under the SLSE such that the designs may be constructed on a scaled design space \(S^V\) instead of the original design space \(S\).

Denote the D-optimal design for a given model, with true parameter vector \(\boldsymbol{\theta}_o\) on a design space \(S\) by \(\xi_D^*(S,\boldsymbol{\theta}_o)\). Also, denote the scaled design space from the original design space \(S\) by \(S^{V}=\{\boldsymbol{V}\boldsymbol{x}|\boldsymbol{x}\in S\}\).

\begin{definition}
\protect\hypertarget{def:scale-matrix}{}\label{def:scale-matrix}The matrix \(\boldsymbol{V}\) is called scale matrix which is a diagonal matrix defined as \(\boldsymbol{V}=diag(v_1,v_2,\cdots,v_p)\), where all \(v_i\) are positive.
\end{definition}

\begin{definition}
\protect\hypertarget{def:scale-invariant}{}\label{def:scale-invariant}Transforming \(\boldsymbol{x}\) to \(\boldsymbol{V} \boldsymbol{x}\) is called scale transformation.
\end{definition}

\begin{definition}
\protect\hypertarget{def:GSI}{}\label{def:GSI}\(\xi_D^*(S,\boldsymbol{\theta}_o)\) is said to be scale invariant for a model if there exists a parameter vector \(\tilde{\boldsymbol{\theta_o}}\) such that the D-optimal design \(\xi_D^*(S^V,\tilde{\boldsymbol{\theta_o}})\) can be obtained from \(\xi_D^*(S,\boldsymbol{\theta_o})\) using some scale transformations.
\end{definition}

This property of the design, \(\xi_D^*(S,\boldsymbol{\theta_o})\), is defined as a generalized scale invariance. Note that \(\tilde{\boldsymbol{\theta_o}}\) and \(\boldsymbol{\theta_o}\) are closed related through the elements \(v_1,\cdots,v_p\) in the scale matrix \(\boldsymbol{V}\). When the model is linear, GSI of \(\xi_D^*(S,\boldsymbol{\theta_o})\) becomes the traditional scale invariance since it does not depend on \(\boldsymbol{\theta_o}\). For many nonlinear regression models, the GSI property holds for D-optimal designs under both OLSE and SLSE. The following lemma provides a condition to check for this property.

\begin{lemma}
\protect\hypertarget{lem:GSI}{}\label{lem:GSI}For a given model and a scale matrix \(\boldsymbol{V}\), if there exists a parameter vector \(\tilde{\boldsymbol{\theta}_o}\) and an invertible diagonal matrix \(\boldsymbol{K}\) which does not depend on \(\boldsymbol{x}\), such that \(\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta}_o)\) for all \(\boldsymbol{x}\in S\), then the design \(\xi_D^*(S,\boldsymbol{\theta}_o)\) has the generalized scale invariance property.
\textbackslash end\{lemma\}
\end{lemma}

\begin{proof}
For design space \(S\) and parameter vector \(\boldsymbol{\theta_o}\), \(\xi_D^*(S,\boldsymbol{\theta_o})\) minimizes \(\phi_D^*(\xi,\boldsymbol{\theta_o})=\det(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o}))\) where \(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o)=\mathbb{E}[\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)]\) and \(\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)\) is given by \eqref{M matrix}, and the expectation is taken with respect to \(\xi(\boldsymbol{x})\) on \(S\).

For design space \(S^V=\{\boldsymbol{V} \boldsymbol{x}|\boldsymbol{x}\in S\}\) and parameter vector \(\tilde{\boldsymbol{\theta_o}}\), we minimize the following function to find \(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\), \(\phi_D^*(\xi,\tilde{\boldsymbol{\theta}_o})=\det(\boldsymbol{B}^{-1}(\xi,\tilde{\boldsymbol{\theta}_o}))\), where \(\boldsymbol{B}(\xi,\tilde{\boldsymbol{\theta}_o})=\mathbb{E}[\boldsymbol{M}(\boldsymbol{z},\tilde{\boldsymbol{\theta}_o})]\), and the expectation is taken with respect to \(\xi(\boldsymbol{z})\) on \(S^V\). Each \(\boldsymbol{z}\in S^V\) can be written as \(\boldsymbol{V}\boldsymbol{x}\) followed by definition, where \(\boldsymbol{x}\in S\). Thus, we have \(\boldsymbol{M}(\boldsymbol{z},\tilde{\boldsymbol{\theta}_o})=\boldsymbol{M}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})\).

By the assumption of Lemma \ref{lem:GSI} and \eqref{eq:M-matrix}, we get \[
\boldsymbol{M}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})=(1 \oplus \boldsymbol{K}) \boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)(1 \oplus \boldsymbol{K}),~for~all~\boldsymbol{x}\in S.
\]
Therefore, \(\boldsymbol{B}(\xi,\tilde{\boldsymbol{\theta}_o})= (1 \oplus \boldsymbol{K}) \boldsymbol{B}(\xi,\boldsymbol{\theta}_o)(1 \oplus \boldsymbol{K})\) and \(\phi_D(\xi,\tilde{\boldsymbol{\theta}_o})=\frac{\phi_D(\xi,\boldsymbol{\theta}_o)}{\det^2(\boldsymbol{K})}\). This implies that we can minimize \(\phi_D(\xi,\boldsymbol{\theta}_o)\) to obtain \(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\), where \(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\) is the scale transformation from \(\xi_D^*(S,\boldsymbol{\theta}_o)\).
\end{proof}

\begin{example}
\protect\hypertarget{exm:peleg}{}\label{exm:peleg}Consider Peleg regression model in \eqref{eq:model-peleg}. In this model, since \(p=1\), the scale matrix \(V=v_1>0\) is a scalar and \(\boldsymbol{\theta}_o=(a,b)^\top\). Now, let \(\tilde{\boldsymbol{\theta}_o}=(a,\frac{b}{v_1})^\top\) and we can obtain

\begin{equation*}
\begin{aligned}
\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})  
        &= \boldsymbol{f}(v_1x,\tilde{\boldsymbol{\theta}_o})\\
        &= \begin{pmatrix}\frac{-v_1x}{(a+\frac{b}{v_1}v_1x)^2}, & \frac{-(v_1x)^2}{(a+\frac{b}{v_1}v_1x)^2}\end{pmatrix}^\top\\
        &=\begin{pmatrix} v_1\frac{-x}{(a+bx)^2}, & v_1^2\frac{-x^2}{(a+bx)^2}\end{pmatrix}^\top\\
        &= \begin{pmatrix} v_1  &0\\0&v_1^2\end{pmatrix}\begin{pmatrix} \frac{-x}{(a+bx)^2}, & \frac{-(v_1x)^2}{(a+bx)^2}\end{pmatrix}^\top\\
        &=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta_o}).
\end{aligned}
\end{equation*}
Hence, Peleg model has GSI property based on Lemma \ref{lem:GSI} by choosing \(\boldsymbol{K}=diag(v_1,v_1^2)\) and \(\tilde{\boldsymbol{\theta_o}}=(a,\frac{b}{v_1})^T\). \hfill\(\Box\)
\end{example}

It is worth noting that when matrix \(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)\) is ill-conditioned (i.e.~the condition number of the matrix is very large), numerical algorithm computing \(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta}_o,t)\) may fail as the numerical inverse is inaccurate and imprecise. In these situations, the GSI property may be helpful. Here we provide one example to demonstrate the usefulness of the GSI property.

\begin{example}
Piecewise polynomial regression using knots is frequently used and has various applications. See \citet{dette2008optimal} and the references therein. They investigated optimal designs under OLSE for piecewise polynomial regression model with unknown knots, and obtained results for the number of support points under D-optimality and various other properties about designs. Here we consider one model they used, a cubic spline regression model with unknown knots, which is given by
\begin{equation}
y=\theta_1+\theta_2x+\theta_3x^2+\theta_4x^3+\theta_5(x-\lambda)_{+}^3+\epsilon,~x\in [0,b]
\label{eq:spline-regression}
\end{equation}
where \((x-\lambda)_{+} = \max(0,(x-\lambda))\). Model \eqref{model:spline_regression} is nonlinear with parameter vector \(\boldsymbol{\theta}_o=(\theta_1,\cdots,\theta_5,\lambda)^T\), \(\boldsymbol{f}(x,\boldsymbol{\theta}_o)=(1,x,x^2,x^3,(x-\lambda)_+^3,-3\theta_5(x-\lambda)_+^2)^T\). We now start to illustrate GSI property for this model. Consider a scale matrix \(\boldsymbol{V}=v_1>0\) and let \(\tilde{\boldsymbol{\theta}_o}=(\theta_1,\cdots,\theta_5,v_1\lambda)^T\). Then we have, for all \(x\in[0,b]\),

\begin{equation*}
\begin{aligned}
\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta_o}})  
        &= \boldsymbol{f}(v_1x,\tilde{\boldsymbol{\theta_o}})\\
        &= (1, v_1x,  (v_1x)^2,  (v_1x)^3,  (v_1x-v_1\lambda)_+^3,  -3\theta_5(v_1x-v_1\lambda)_+^2)^T\\
        &= (1,  v_1,x,  (v_1x)^2,  (v_1x)^3,  v_1^3(x-\lambda)_+^3,  -3\theta_5v_1^2(x-\lambda)_+^2 )^T\\
        &= diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2) (1, x,  x^2, x^3, (x-\lambda)_+^3, -3\theta_5(x-\lambda)_+^2)^T\\
        &=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta_o}),\quad \text{with } \boldsymbol{K}=diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2).
\end{aligned}
\end{equation*}
Hence, by Lemma \ref{lem:GSI}, the D-optimal design under SLSE has the GSI property. Moreover, the model is linear in \(\boldsymbol{\theta}_o'=(\theta_1,\theta_2,\cdots,\theta_5)^\top\), so the D-optimal designs under both OLSE and SLSE do not depend on \(\boldsymbol{\theta}_o'\) \citet{yin2018optimal}. Thus, for \(S=[0,b]\) and \(S^{V}=[0,v_1b]\), the D-optimal designs \(\xi_D^*(S,\boldsymbol{\theta}_o)\) and \(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\) can be obtained from each other by using the corresponding \(\boldsymbol{\theta}_o\) and \(\tilde{\boldsymbol{\theta}}\). This property can help us dramatically for finding the D-optimal designs when matrix \(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)\) is ill-conditioned. Numerical results of D-optimal designs are presented in Example \ref{example:spline} after we discuss about numerical algorithms in Chapter \ref{chapter:applications}. \hfill\(\Box\)
\end{example}

We also want to note that in model \eqref{eq:spline-regression}, the intercept is included so the D-optimal designs under both OLSE and SLSE are the same \citep{gao2014new}. Moreover, the result may easily be extended for piecewise regression models with multiple unknown knots.

\chapter{Numerical Algorithm and Applications}\label{chapter-applications}

We have studied the optimality conditions and the number of support points for optimal designs under SLSE in Chapter \ref{chapter-SLSE}. However, it is still challenging to find optimal designs analytically, except for simple regression models. Often numerical algorithms are used to compute optimal designs, and various algorithms have been studied including \citet{titterington1978estimation}, \citet{silvey1978an}, \citet{dette2008improving}, and \citet{torsney2009multiplicative}. For optimal designs under the SLSE, \citet{bose2015optimal} used multiplicative algorithms for finding the optimal designs whereas \citet{gao2017d} used convex programming and semi-definite programming (SDP) in MATLAB. Here we want to further extend the computational strategies for finding optimal designs under the SLSE. In this chapter, we discuss an effective algorithm to compute optimal designs based on CVX programming in MATLAB.

\section{Optimization problem}\label{optimization-problem}

We first discretize the design space \(S \subset \mathbb{R}^p\), and the discretized space is denoted as \(S_N\subset S\), where \(N\) is the number of points in \(S_N\). The magnitude of \(N\) can be very large, but often the designs are highly efficient already with moderate \(N\). More discussions about the choice of \(N\) is given in the examples in this chapter The discretization can be done in many different ways, but we discretize the design space using equally spaced gird points for the sake of simplicity. For an one dimensional design space, say \(S=[a,b]\), the design points are \(a+(i-1)\frac{(b-a)}{N},i=1,2,...,N\). For higher dimensional design spaces, we use equally spaced grid points for each variable, and \(S_N\) is formed by Cartesian product.

Suppose \(\xi^*\) is the optimal probability measure in \(\Xi_N\), where \(\Xi_N\) includes all the distributions on \(S_N\). We denote \(S_N=\{\boldsymbol{u}_1,\boldsymbol{u}_2,...,\boldsymbol{u}_N\}\subset S\). Any probability measure \(\xi \in \Xi_N\) can be express as
\[
    \xi=\begin{bmatrix}
    \boldsymbol{u}_1    &\boldsymbol{u}_2   &...&\boldsymbol{u}_N\\
    w_1     &w_2    &...&w_N
    \end{bmatrix},
\]

where \(w_i\) is the weight at \(\boldsymbol{u_i}\), which is non-negative and \(\sum_{i=1}^Nw_i=1\). The optimal regression design problem can be expressed as
\begin{equation} 
\begin{aligned}
& \underset{w_1, ..., w_N, \delta}{\text{min}}
& &  \delta \\
& \text{s.t.}
& & \sum_{i=1}^N w_i=1,\\
& & & -w_i\leq 0,~ \text{for }i = 1,2, ..., N,\\
& & & \phi \leq \delta, 
\end{aligned}
\label{eq:optimization}
\end{equation}
where \(\phi\) is a loss function, which can be \(\phi_A\), \(\log(\phi_D)\) or \(\phi_c\) defined in \eqref{eq:loss-B}. Notice that for any \(\xi \in \Xi_N\), \(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)=\mathbb{E}_{\xi}[\boldsymbol{M(\boldsymbol{x},\boldsymbol{\theta}_o},t)]=\sum_{i=1}^N \boldsymbol{M}(\boldsymbol{u}_i,\boldsymbol{\theta}_o,t)w_i\), where \(\boldsymbol{M}(\boldsymbol{u_i},\boldsymbol{\theta}_o,t)\) is defined in \eqref{eq:M-matrix}. We also note that a design point is selected if its weight is greater than a very small number \(\delta_1\) (a small positive number, say \(10^{-5}\)) in practice. Lastly, we report the \(d_D^{\max}=\max\{d_D |\boldsymbol{x}\in S\}\) (\(d_A^{\max},~d_c^{\max}\)) for D-optimality (A-, c-optimality), where \(d_D,d_A\) and \(d_c\) are defined in \eqref{eq:dire}. If \(d_D^{\max}<\delta_2\) (a small positive number, say \(10^{-4}\)), then the numerical solution to the above optimization problem is a D-optimal design by Theorem \ref{thm:dispersion}. Similarly we can check for A- and c-optimal designs. Problem \eqref{eq:optimization} is a convex optimization problem. We will discuss an algorithm to solve \eqref{eq:optimization} in the next section.

\section{Computational Algorithm}\label{section-algorithm}

There are some numerical algorithms which have been studied under the SLSE. \citet{bose2015optimal} applied multiplicative algorithms for finding the optimal designs to binary design points. \citet{gao2017d} computed D-optimal designs using the moments of distribution \(\xi \in \Xi\). \citet{yin2018optimal} formulated the design problems as convex optimization problem and applied convex optimization programs CVX and SeDuMi in MATLAB to find D- and A-optimal designs, respectively. In their work, the A-optimal design problems are solved by SeDuMi which is hard to write the code in MATLAB. In our newly proposed formulation in \eqref{eq:optimization}, we are able to use CVX to find A-optimal designs, which is much easier to code. Moreover, the CVX programming can be also applied to find c-optimal designs, which the previous algorithms were not applied before.

Figure \ref{Flow chart} illustrates a simple algorithm with six steps to apply CVX for finding optimal designs. Users input the variables, partition the design space into \(N\) grid points, and compute the \(\boldsymbol{M}\) matrix in the first three steps. Steps (i), (ii) and (iii) depend on the regression model and the design space. Step (iv) solves the optimal design problem by CVX program, which depends on the optimality criterion. The details of using CVX and MATLAB code can be found in Appendix \ref{appendix:algorithm} and Appendix \ref{appendix:matlab}. We present several examples using this algorithm to show optimal designs in Section \ref{section:applications}.

\section{Applications}\label{section:applications}

We compute A-, D- and c-optimal designs for various regression models. Notice that when the intercept term presents, the resulting optimal designs under the OLSE and the SLSE are the same \citet{gao2014new}. Since there are many studies in optimal regression designs under the OLSE in the literature already, we only consider the models without the intercept. All the examples are computed via CVX package in MATLAB. We provide the results with different values of \(t\), where \(t\) is related to a measure of skewness of the error distribution. The related MATLAB code can be found in Appendix \citet{appendix:matlab}.

All the examples in this thesis are computed on a PC with Intel Core I5 6500 4 cores CPU, 3.20 GHz. The MATLAB version is R2018a academic student version with CVX version 2.1 and build number 1123. The RAM and the platform for this particular machine are 16 Gigabyte and Windows 10 Professional version, respectively.

\begin{example}
\protect\hypertarget{exm:gompertz}{}\label{exm:gompertz}This example is for Gompertz growth model which was briefly described in Chapter \ref{chapter:introduction} to showcase the output. Here, we use this model to show the effects of N on the D-optimal designs under SLSE with \(\boldsymbol{\theta_o}=(1,1,1)^\top\), \(S=[0.0,10.0]\) and \(t=0\).

Table \ref{table:gompertz1} gives D-optimal designs for various values of \(N\). Note that the D-optimal design in Figure \ref{fig:gompertz} in Chapter \ref{chapter:introduction} is for \(N=2001\). We can see that the optimal design changes when \(N\) increases. However, when \(N\ge 1001\), the optimal designs do not change much and converge to a distribution having three support points with equal weights. We also show the change of the loss function as \(N\) changes in Figure \ref{fig:gom_loss}. As \(N\rightarrow +\infty\), the loss function converges too. In this example, moderate \(N\) already gives a highly efficient design as shown in Figure \ref{fig:gom_loss}. Similar results are obtained for A- and c-optimal designs, and representative results are given in Table \ref{table:gompertz2}, where \(\boldsymbol{c_1}=(2,0.5,1)^T\) is used for c-optimal designs, and \(t= 0.0, 0.3, 0.5, 0.7\) and \(0.9\).

It is clear that \(d_A^{\max}=\max\limits_{x}(d_A(x,\xi_A^*,t)\) (or \(d_c(x,\xi_c^*,t)\)) is small, which satisfies the optimality conditions in Theorem \ref{theorem:dispersion}. We should also point out that for some situations when two numerical support points are very close to each other, they are probably representing only one theoretical support point which is between the two numerical support points. For instance, when \(t=0.0\), the two support points \(1.315\) and \(1.320\) are very close to each other in A-optimal design. If we discretize the design space to more points (i.e.~choose larger \(N\)), we can find this particular point. For instance, the A-optimal design with \(N=9001\) is

\[ 
    \xi_A^*=\begin{bmatrix}
                0.000   &   1.318   &   10.000\\
                0.354   &   0.385   &   0.261
    \end{bmatrix},
\]
which only has three support points and \(1.318\) is indeed in between \(1.315\) and \(1.320\).
The computational time changes from 1.65 seconds to 86.16 seconds for \(N=51\) to \(N=20001\), which increases quite a bit but the computational time is still less than two minutes.
\hfill\(\Box\)
\end{example}

\begin{example}
\protect\hypertarget{exm:polynomial}{}\label{exm:polynomial}We consider the polynomial regression model in \eqref{model:poly} with \(q=5\). Here \(\mathbf{f}(x;\mathbf{\theta}) = (x,x^2,... ,x^q)^\top\) which does not depend on \(\mathbf{\theta}\). The A- and D-optimal designs are shown in Figure \ref{fig:poly_5}. With \(N=2001\), we can see that the number of support points is always six for this model. When \(q\) is even, the number of support points for D- and A-optimal designs equals to \(q\) for small \(t\), and equals to \(q+1\) for larger \(t\). This is consistent with Theorem \ref{theorem:support}.
\end{example}

\chapter{Discussions}\label{chapter:discussions}

We have reviewed the current development of optimal regression designs under the SLSE and derived more theoretical properties for the optimal designs. We can characterize the loss function for the A-optimal design in convex optimization framework differently, so that the result can be extended to c-optimality criterion under the SLSE. We have also derived the optimality conditions based on the equivalence results for A-, D- and c-optimality criteria. Furthermore We have studied and obtained the number of support points for several regression models under the SLSE analytically.

For nonlinear models, the optimal designs often depend on the value of parameter vector \(\boldsymbol{\theta}_o\), and it is difficult to solve the optimal regression design problems analytically. We have investigated a CVX based numerical algorithm to compute for the optimal designs using the properties we obtained. Using the approximate design approach by discretizing the design space into a user-specified number of gird points, we can find optimal designs efficiently and effectively with the CVX based algorithm. Several applications are given in the thesis for showcasing the effectiveness of the algorithm.

Generalized scale invariance property for D-optimality is a very useful property that is also studied in this thesis. It can be applied to both linear and nonlinear regression models. This property can be helpful in many situations. For instance, if matrix \(\boldsymbol{B}\) is ill-conditioned, our numerical algorithm may fail. We may be able to use this property to rescale the design space and find the D-optimal design on the scaled design space. This scaling is helpful to avoid computing issues related to ill-conditioned matrix.

Although our algorithm is effective and efficient for the models presented in this thesis, there are still challenges to construct optimal designs for complicated models. We discuss a few of them here.

When the dimension of the design space \(S\) is high (i.e.~having many design variables), for instance, \(10\) \(x's\) in the mixture experiment, the algorithm may be unstable. Sometimes, since we keep the point when its weight is more than \(10^{-4}\) by using the numerical algorithm, there can be many points having very small positive weights. Such points may not be significant in the optimal designs. Also, it is difficult to partition the high dimensional space into tiny gird points, as the computation time increases exponentially as \(N\) increases.

When matrix \(\boldsymbol{B}\) is ill-conditioned, the numerical algorithm will fail as it cannot find optimal designs precisely. For D-optimal designs, we have discussed that for some models, we may use the generalized scale invariant properties to rescale the ill-conditioned matrix and obtain the optimal designs. However, not all the models under D-optimality have such property. Furthermore, other design criteria, such as A- and c-, generally do not have such property. Therefore, further investigations are required to address this issue in the future.

Although we have extended the theoretical properties under the SLSE to c-optimality criterion which has not been studied before, there are still many other design criteria under the SLSE, such as E-, G- and minimax criteria that have not been studied.

\chapter{Final Words}\label{final-words}

We have finished a nice book.

  \bibliography{book.bib,packages.bib}

\end{document}
