<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Optimal Regression Designs Under SLSE | Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications</title>
<meta name="author" content="Chi-Kuang Yeh">
<meta name="description" content="In this chapter, we first review the results and properties of optimal regression designs under the SLSE. We then derive several new analytical results for the optimal designs under the SLSE. We...">
<meta name="generator" content="bookdown 0.39 with bs4_book()">
<meta property="og:title" content="Chapter 3 Optimal Regression Designs Under SLSE | Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications">
<meta property="og:type" content="book">
<meta property="og:description" content="In this chapter, we first review the results and properties of optimal regression designs under the SLSE. We then derive several new analytical results for the optimal designs under the SLSE. We...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Optimal Regression Designs Under SLSE | Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications">
<meta name="twitter:description" content="In this chapter, we first review the results and properties of optimal regression designs under the SLSE. We then derive several new analytical results for the optimal designs under the SLSE. We...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.7.0/transition.js"></script><script src="libs/bs3compat-0.7.0/tabs.js"></script><script src="libs/bs3compat-0.7.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="active" href="chapter-SLSE.html"><span class="header-section-number">3</span> Optimal Regression Designs Under SLSE</a></li>
<li><a class="" href="chapter-applications.html"><span class="header-section-number">4</span> Numerical Algorithm and Applications</a></li>
<li><a class="" href="chapter:discussions.html"><span class="header-section-number">5</span> Discussions</a></li>
<li><a class="" href="final-words.html"><span class="header-section-number">6</span> Final Words</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="chapter-SLSE" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Optimal Regression Designs Under SLSE<a class="anchor" aria-label="anchor" href="#chapter-SLSE"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter, we first review the results and properties of optimal regression designs under the SLSE. We then derive several new analytical results for the optimal designs under the SLSE. We begin with recalling the formulation of the optimal regression designs under SLSE based on three different criteria, A-, D-, and c-optimality. The formulation of A-, and D-optimality under the SLSE was first proposed in <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2014new">2014</a>)</span> while A-optimality was further investigated in <span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span>. We formulate optimal design problems under A-optimality differently so that the properties can be extended to c-optimality which has not been studied yet. Equivalence results for verifying optimal designs are also obtained. In addition, analytical results are derived for the number of support points for several regression models.</p>
<div id="design-criteria-under-slse" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Design criteria under SLSE<a class="anchor" aria-label="anchor" href="#design-criteria-under-slse"><i class="fas fa-link"></i></a>
</h2>
<p>Let us introduce the notations first. Assume <span class="math inline">\(\sigma_o\)</span> and <span class="math inline">\(\boldsymbol{\theta}_o\)</span> are the true parameter values of <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span>, respectively. Let <span class="math inline">\(S \subset \mathbb{R}^p\)</span> be the design space for <span class="math inline">\(\boldsymbol{x}\)</span>. Let <span class="math inline">\(\operatorname{tr}(\cdot)\)</span> and <span class="math inline">\(\det(\cdot)\)</span> be the trace and determinant functions of a matrix, respectively. Moreover, let <span class="math inline">\(\Xi\)</span> denote the class of all probability measures on <span class="math inline">\(S\)</span>. Define, for any <span class="math inline">\(\xi\in \Xi\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
\boldsymbol{g}_1=\boldsymbol{g}_1(\xi,\boldsymbol{\theta_o})=\mathbb{E}_{\xi}\bigg[ \frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_o} \bigg],
\end{equation*}\]</span></p>
<p>and
<span class="math display">\[\begin{equation*}
\boldsymbol{G}_2=\boldsymbol{G}_2(\xi,\boldsymbol{\theta}_o)=\mathbb{E}_{\xi} \bigg[ \frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}^\top}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_o}\bigg].
\end{equation*}\]</span></p>
<p>For a discrete probability measure <span class="math inline">\(\xi\in \Xi\)</span>, we write it as
<span class="math display">\[
\xi=\begin{bmatrix}
    \boldsymbol{x}_1&amp;\boldsymbol{x}_2   &amp;\ldots &amp;\boldsymbol{x}_m\\
    p_1     &amp;p_2        &amp;\ldots &amp;p_m
\end{bmatrix},
\]</span>
where <span class="math inline">\(\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_m\)</span> are the support points in <span class="math inline">\(S\)</span>, and <span class="math inline">\(p_1,\dots,p_m\)</span> are the probabilities associated with those points. From <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2014new">2014</a>)</span>, the asymptotic variance-covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> under the SLSE is given by
<span class="math display" id="eq:cov-matrix">\[\begin{equation}
\boldsymbol{\mathbb{V}}({\boldsymbol{\hat{\theta}}})=\sigma^2_o(1-t)(\boldsymbol{G}_2-t\boldsymbol{g}_1\boldsymbol{g}_1^\top)^{-1},
\tag{3.1}
\end{equation}\]</span>
where <span class="math inline">\(t=\frac{\mu_3^2}{\sigma_o^2(\mu_4-\sigma_o^4)}\)</span>, <span class="math inline">\(\mu_3=\mathbb{E}[\epsilon_i^3|\boldsymbol{x}]\)</span> and <span class="math inline">\(\mu_4=\mathbb{E}[\epsilon_i^4|\boldsymbol{x}]\)</span>. Note that <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2014new">2014</a>)</span> discussed that <span class="math inline">\(t \in [0,1)\)</span> for any error distributions, and <span class="math inline">\(t=0\)</span> for symmetric error distributions. Define a matrix <span class="math inline">\(\boldsymbol{J}=\boldsymbol{J}(\xi,\boldsymbol{\theta}_o,t) = \boldsymbol{G}_2-t\boldsymbol{g}_1\boldsymbol{g}_1^T\)</span>. It is clear that matrix <span class="math inline">\(\boldsymbol{J}\)</span> is proportional to the inverse of the variance-covariance matrix <a href="chapter-SLSE.html#eq:cov-matrix">(3.1)</a>. For the rest of the thesis, we will be working on the design problems using matrix <span class="math inline">\(\boldsymbol{J}\)</span>.</p>
<p>As discussed in Chapter <a href="intro.html#intro">2</a>, we aim to minimize the loss functions in optimal design problems, and the loss functions for D-, A- and c-optimality criteria under SLSE can be expressed as
<span class="math display" id="eq:loss-J">\[\begin{equation}
\begin{aligned}
  \phi_D(\xi,\boldsymbol{\theta_o},t)&amp;=&amp;\det(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
  \phi_A(\xi,\boldsymbol{\theta_o},t)&amp;=&amp;\operatorname{tr}(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
  \phi_c(\xi,\boldsymbol{\theta_o},t)&amp;=&amp;\boldsymbol{c}_1^\top\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{c}_1,
\end{aligned}
\tag{3.2}
\end{equation}\]</span>
when <span class="math inline">\(\boldsymbol{J}\)</span> is non-singular, and <span class="math inline">\(\boldsymbol{c}_1\)</span> is a given vector in <span class="math inline">\(\mathbb{R}^q\)</span>. If <span class="math inline">\(\boldsymbol{J}\)</span> is singular, all the three loss functions are defined to be <span class="math inline">\(+\infty\)</span>. We use <span class="math inline">\(\xi_D^*,~\xi_A^*\)</span> and <span class="math inline">\(\xi_c^*\)</span> to denote for A- and c-optimal designs, respectively. For two measures <span class="math inline">\(\xi_1\)</span> and <span class="math inline">\(\xi_2\in \Xi\)</span>, define <span class="math inline">\(\xi_{\alpha}=(1-\alpha)\xi_1+\alpha \xi_2\)</span> for <span class="math inline">\(\alpha \in[0,1]\)</span>.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-1" class="lemma"><strong>Lemma 3.1  </strong></span><span class="math inline">\(\log(\phi_D(\xi_{\alpha},\boldsymbol{\theta_o},t))\)</span>, <span class="math inline">\(\phi_A(\xi_{\alpha},\boldsymbol{\theta_o},t)\)</span> and <span class="math inline">\(\phi_c(\xi_{\alpha},\boldsymbol{\theta_o},t)\)</span> are convex functions of <span class="math inline">\(\alpha\)</span>.</p>
</div>
<p>The convexity results are discussed in <span class="citation">Boyd, Boyd, and Vandenberghe (<a href="references.html#ref-boyd2004convex">2004</a>)</span> and <span class="citation">Wong and Zhou (<a href="references.html#ref-wong2019cvx">2019</a>)</span>. Similar convexity results are given in <span class="citation">Bose and Mukerjee (<a href="references.html#ref-bose2015optimal">2015</a>)</span>. We will use <span class="math inline">\(\log(\det(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)))\)</span> for D-optimal design for the rest of the thesis as <span class="math inline">\(\log(\cdot)\)</span> is a monotonic increasing function which does not change the optimality.</p>
<p>Although we have formulated the loss functions, there are some issues associated with the formulation in @ref{eq-loss-J}. The reason is that <span class="math inline">\(\boldsymbol{J}\)</span> is lacking of linearity. From construction of <span class="math inline">\(\boldsymbol{J}\)</span>, <span class="math inline">\(\boldsymbol{J}(\xi_{\alpha},\boldsymbol{\theta_o},t)\)</span> is not a linear combination of <span class="math inline">\(\boldsymbol{J}(\xi_{1},\boldsymbol{\theta_o},t)\)</span> and <span class="math inline">\(\boldsymbol{J}(\xi_{2},\boldsymbol{\theta_o},t)\)</span>. Thus, it is difficult to obtain the theoretical results using <span class="math inline">\(\boldsymbol{J}\)</span>. To solve this issue, <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2017d">2017</a>)</span> proposed an alternative expression for characterizing the loss functions. The key is to define a matrix
<span class="math display" id="eq:B-matrix">\[\begin{equation}
\boldsymbol{B}(\xi)=\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1               &amp;   \sqrt{t}\boldsymbol{g_1}^\top\\
\sqrt{t}\boldsymbol{g}_1    &amp;   \boldsymbol{G}_2
\end{pmatrix},
\tag{3.3}
\end{equation}\]</span>
which plays an important role in the following formulation. Note <span class="math inline">\(\boldsymbol{B}(\xi_{\alpha})\)</span> is now an affine function of <span class="math inline">\(\alpha\)</span>, i.e.,
<span class="math display">\[ \boldsymbol{B}(\xi_{\alpha})=(1-\alpha)\boldsymbol{B}(\xi_1)+\alpha\boldsymbol{B}(\xi_2).\]</span>
This fact ultimately makes <span class="math inline">\(\boldsymbol{B}\)</span> much more useful than <span class="math inline">\(\boldsymbol{J}\)</span> to study optimal designs under SLSE. The inverse of <span class="math inline">\(\boldsymbol{B}\)</span> is given as
<span class="math display" id="eq:B-inverse">\[\begin{equation}
\boldsymbol{B}^{-1}(\xi)=\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)   =\begin{pmatrix}
\frac{1}{r}             &amp;   \frac{-\sqrt{t}}{r}\boldsymbol{g}_1^\top\boldsymbol{G}_2^{-1}   \\
\frac{-\sqrt{t}}{r}\boldsymbol{G}_2^{-1}\boldsymbol{g}_1    &amp;\boldsymbol{J}^{-1}
\end{pmatrix},
\tag{3.4}
\end{equation}\]</span>
where <span class="math inline">\(r=1-t\boldsymbol{g}_1^\top\boldsymbol{G}_2^{-1}\boldsymbol{g}_1\)</span>. Note that if <span class="math inline">\(\boldsymbol{J}\)</span> is invertible, <span class="math inline">\(\boldsymbol{G}_2\)</span> must also be invertible since <span class="math inline">\(\boldsymbol{G}_2=\boldsymbol{J}+t\boldsymbol{g}_1\boldsymbol{g}_1^\top\)</span> and <span class="math inline">\(t\boldsymbol{g}_1\boldsymbol{g}_1^\top\)</span> is positive semi-definite. Consequently, <span class="math inline">\(\boldsymbol{B}^{-1}\)</span> exists from <a href="chapter-SLSE.html#eq:B-inverse">(3.4)</a>. Now, we are going to present the following lemmas to characterize the loss functions for A-, c- and D-optimal design problems. Lemma <a href="chapter-SLSE.html#lem:loss-A">3.2</a> is slightly different from a result in <span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span>, Lemma <a href="chapter-SLSE.html#lem:loss-D">3.3</a> is a result from <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2017d">2017</a>)</span>, and Lemma <a href="chapter-SLSE.html#lem:loss-c">3.4</a> is a new result.</p>
<div class="lemma">
<p><span id="lem:loss-A" class="lemma"><strong>Lemma 3.2  </strong></span>If <span class="math inline">\(\boldsymbol{J}\)</span> is invertible, then
<span class="math display">\[\begin{equation*}
  \phi_A(\xi,\boldsymbol{\theta}_o,t)=\operatorname{tr}(\boldsymbol{J}^{-1})=\operatorname{tr}(\boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C}),
  \end{equation*}\]</span>
where <span class="math inline">\(\boldsymbol{C}=0\oplus \boldsymbol{I}_q\)</span>, <span class="math inline">\(\boldsymbol{I}_q\)</span> denotes for the <span class="math inline">\(q\times q\)</span> identity matrix, and <span class="math inline">\(\oplus\)</span> denotes for matrix direct sum operator.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-2" class="proof"><em>Proof</em>. </span>From <span class="math inline">\(\eqref{B inverse}\)</span> and <span class="math inline">\(\boldsymbol{C}=0\oplus \boldsymbol{I_q}\)</span>, we get
<span class="math display">\[\begin{align*}
    \boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C}&amp;=  \begin{pmatrix}
                                0   &amp;   0\\
                                0   &amp;   \boldsymbol{I}_q
                                \end{pmatrix}^T\begin{pmatrix}
            \frac{1}{r}             &amp;   \frac{-\sqrt{t}}{r}\boldsymbol{g_1}^\top\boldsymbol{G}_2^{-1}   \\
            \frac{-\sqrt{t}}{r}\boldsymbol{g}_1\boldsymbol{G}_2^{-1}    &amp;\boldsymbol{J}^{-1}
            \end{pmatrix}\begin{pmatrix}
                                0   &amp;   0\\
                                0   &amp;   \boldsymbol{I}_q
                                \end{pmatrix}\\
                            &amp;= \begin{pmatrix}
                                0   &amp;   0\\
                                0   &amp;   \boldsymbol{J}^{-1}
                                \end{pmatrix},
    \end{align*}\]</span>
which implies <span class="math inline">\(\operatorname{tr}(\boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C})=\operatorname{tr}(\boldsymbol{J}^{-1})\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:loss-D" class="lemma"><strong>Lemma 3.3  </strong></span>If <span class="math inline">\(\boldsymbol{J}\)</span> is invertible, then
<span class="math display">\[\begin{equation*}
    \phi_D(\xi,\boldsymbol{\theta_o},t)=\det(\boldsymbol{J}^{-1})=\det(\boldsymbol{B}^{-1}).
\end{equation*}\]</span></p>
</div>
<div class="{..proof}">
<p>From @ref{eq:B-matrix}, we have
<span class="math display">\[\begin{align*}
    \det(\boldsymbol{B})    &amp;= \det(1-t\boldsymbol{g_1}^T\boldsymbol{G_2}^{-1}\boldsymbol{g_1})\det(\boldsymbol{G_2})\\
            &amp;= \det(\boldsymbol{I}-t\boldsymbol{g_1}\boldsymbol{g_1}^\top\boldsymbol{G_2}^{-1}) \det(\boldsymbol{G_2}) \\
            &amp;=  \det(\boldsymbol{G_2}-t\boldsymbol{g_1}\boldsymbol{g_1}^T)\\
            &amp;= \det(\boldsymbol{J}),
    \end{align*}\]</span>
which gives <span class="math inline">\(\det(\boldsymbol{J}^{-1})=\det(\boldsymbol{B}^{-1})\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:loss-c" class="lemma"><strong>Lemma 3.4  </strong></span>If <span class="math inline">\(\boldsymbol{J}\)</span> is invertible, then
<span class="math display">\[\begin{equation*}
    \phi_c(\xi,\boldsymbol{\theta_o},t)=\boldsymbol{c}_1\top\boldsymbol{J}^{-1}\boldsymbol{c}_1=\boldsymbol{c}^\top\boldsymbol{B}^{-1}\boldsymbol{c},
\end{equation*}\]</span>
where <span class="math inline">\(\boldsymbol{c_1}\)</span> is a vector in <span class="math inline">\(\mathbb{R}^q\)</span> and <span class="math inline">\(\boldsymbol{c}^\top=(0,\boldsymbol{c}_1^\top)\)</span>.</p>
</div>
<p>Thus, by Lemmas <span class="math inline">\(\ref{lemma:loss A}\)</span>, <span class="math inline">\(\ref{lemma:loss D}\)</span> and <span class="math inline">\(\ref{lemma:loss c}\)</span>, the alternative expressions for the loss functions in <span class="math inline">\(\eqref{loss J}\)</span> are
<span class="math display">\[\begin{equation} \label{loss B}
\begin{aligned}
    \phi_D(\xi,\boldsymbol{\theta_o},t))&amp;=&amp;\det(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
    \phi_A(\xi,\boldsymbol{\theta}_o,t)&amp;=&amp;\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{C}),\\
    \phi_c(\xi,\boldsymbol{\theta_o},t)&amp;=&amp;\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{c},
\end{aligned}
\end{equation}\]</span>
where <span class="math inline">\(\boldsymbol{C}= 0 \oplus \boldsymbol{I_q}\)</span>, <span class="math inline">\(\boldsymbol{c_1}\in \mathbb{R}^q\)</span> and <span class="math inline">\(\boldsymbol{c}^T=(0,\boldsymbol{c_1}^T)\)</span>. If <span class="math inline">\(\boldsymbol{B}\)</span> is singular, all the three loss functions are defined to be <span class="math inline">\(+\infty\)</span>.</p>
</div>
<div id="equivalence-theorem-for-optimal-designs-under-slse" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Equivalence theorem for optimal designs under SLSE<a class="anchor" aria-label="anchor" href="#equivalence-theorem-for-optimal-designs-under-slse"><i class="fas fa-link"></i></a>
</h2>
<p>In this section we derive the optimality conditions for the optimal designs under the SLSE which follows from the equivalence theorem in <span class="citation">Kiefer and Wolfowitz (<a href="references.html#ref-kiefer-wolfowitz1959optimum">1959</a>)</span> and <span class="citation">Kiefer (<a href="references.html#ref-kiefer1974general">1974</a>)</span>. We also analyze the minimum number of support points in optimal designs for various regression models, and theoretical results are obtained. Note we study approximate designs in this thesis. The advantages of working with approximate designs instead of exact design are well documented in <span class="citation">Kiefer and Brown (<a href="references.html#ref-kiefer1985jack">1985</a>)</span>.</p>
<p>Define a vector <span class="math inline">\(f(\boldsymbol{x,\theta_o})=\frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{ \partial \boldsymbol{\theta}}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta_o}} \in \mathbb{R}^q\)</span> and a matrix
<span class="math display" id="eq:M-matrix">\[\begin{equation}
\boldsymbol{M}(\boldsymbol{x})=\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)=\begin{pmatrix}
1       &amp;   \sqrt{t}f^T(\boldsymbol{x},\boldsymbol{\theta_o})\\
\sqrt{t}f(\boldsymbol{x},\boldsymbol{\theta_o}) &amp;f(\boldsymbol{x},\boldsymbol{\theta_o})f^T(\boldsymbol{x},\boldsymbol{\theta_o})
\end{pmatrix}_{(q+1)\times (q+1)}.
\tag{3.5}
\end{equation}\]</span></p>
<p>Then <span class="math inline">\(\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)=\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)]\)</span>. Define dispersion functions
<span class="math display">\[\begin{equation} \label{fuction:dispersion}
\begin{aligned}
  &amp;d_D(\boldsymbol{x},\xi,t) = \operatorname{tr}(\boldsymbol{B}^{-1}\boldsymbol{M}(\boldsymbol{x}))-(q+1),\\
  &amp;d_A(\boldsymbol{x},\xi,t) = \operatorname{tr}(\boldsymbol{M}(\boldsymbol{x})\boldsymbol{B}^{-1}\boldsymbol{C}^T\boldsymbol{C}\boldsymbol{B}^{-1})-\operatorname{tr}(\boldsymbol{C}\boldsymbol{B}^{-1}\boldsymbol{C}^T),\\
  &amp;d_c(\boldsymbol{x},\xi,t) = \boldsymbol{c}^T\boldsymbol{B}^{-1}\boldsymbol{M}(\boldsymbol{x})\boldsymbol{B}^{-1}\boldsymbol{c}-\boldsymbol{c}^T\boldsymbol{B}^{-1}\boldsymbol{c},\\
\end{aligned}
\end{equation}\]</span>
where <span class="math inline">\(\boldsymbol{B}=\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)\)</span> is invertible.</p>
<div class="theorem">
<p><span id="thm:dispersion" class="theorem"><strong>Theorem 3.1  </strong></span>We suppose all the dispersion functions are evaluated at <span class="math inline">\(\boldsymbol{\theta_o}\)</span>. If <span class="math inline">\(\xi_D^*\)</span>, <span class="math inline">\(\xi_A^*\)</span> and <span class="math inline">\(\xi_c^*\)</span> are the optimal probability measures for D-, A- and c- optimality, respectively, then <span class="math inline">\(\boldsymbol{B}\)</span> is invertible and for any <span class="math inline">\(\boldsymbol{x}\in S\)</span>,
<span class="math display" id="eq:dire-c">\[\begin{align}
d_D(\boldsymbol{x},\xi_D^*,t) &amp;\leq 0, \tag{3.6} \\
d_A(\boldsymbol{x},\xi_A^*,t) &amp;\leq 0, \tag{3.7} \\
d_c(\boldsymbol{x},\xi_c^*,t) &amp;\leq 0. \tag{3.8}
\end{align}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-3" class="proof"><em>Proof</em>. </span>In this proof, we use <span class="math inline">\(\boldsymbol{B}(\xi)\)</span> for <span class="math inline">\(\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)\)</span> and <span class="math inline">\(\boldsymbol{M}(\boldsymbol{x})\)</span> for <span class="math inline">\(\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)\)</span>. Suppose <span class="math inline">\(\xi^*\)</span> is an optimal design to a criterion. Define <span class="math inline">\(\xi_\alpha=(1-\alpha)\xi^*+\alpha \xi\)</span> where <span class="math inline">\(\xi\)</span> is an arbitrary probability measure. This proof is based on Kiefer’s general equivalence theorem , and the optimal condition can be derived from <span class="math inline">\(\frac{\partial \phi(\xi_\alpha)}{\partial \alpha}\big|_{\alpha=0}\geq 0\)</span> for any measure <span class="math inline">\(\xi \in \Xi\)</span>, where <span class="math inline">\(\phi\)</span> is a loss function.</p>
<p>We first prove <a href="chapter-SLSE.html#eq:dire-D">(3.6)</a>. Let <span class="math inline">\(\xi_D^*\)</span> be the optimal measure under D-optimality. We have
<span class="math display">\[\begin{align*}
    \frac{\partial \log(\phi_D(\xi_\alpha))}{\partial \alpha}\Big|_{\alpha=0}   
    &amp;=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)(-\boldsymbol{B}(\xi_D^*)+\boldsymbol{B}(\xi)))\\
    &amp;=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\boldsymbol{B}(\xi))+\operatorname{tr}(\boldsymbol{I_{q+1}})\\
    &amp;=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\boldsymbol{B}(\xi))+(q+1))\\
    &amp;=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]-(q+1))\\
    &amp;=-\mathbb{E}_{\xi}[d_D(\boldsymbol{x},\xi_D^*,t)]\\
    &amp;\geq 0,~\text{for any } \xi \text{ on } S,
    \end{align*}\]</span>
which implies <span class="math inline">\(d_D(\boldsymbol{x},\xi_D^*,t)\leq 0\)</span>, for all <span class="math inline">\(\boldsymbol{x} \in S\)</span>.</p>
<p>To prove <a href="chapter-SLSE.html#eq:dire-A">(3.7)</a>, let <span class="math inline">\(\xi_A^*\)</span> be the optimal measure under A-optimality. We have
<span class="math display">\[\begin{align*}
    \frac{\partial \phi_A(\xi_\alpha)}{\partial \alpha}\Big|_{\alpha=0}
    &amp;=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)[\boldsymbol{B}(\xi)-\boldsymbol{B}(\xi_A^*)]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})\\
    &amp;=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{B}(\xi)\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})+\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})\\
    &amp;=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}) +\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi^*)\boldsymbol{C})\\
    &amp;= - (\operatorname{tr}(\mathbb{E}_{\xi}[\boldsymbol{M}_{\xi}(\boldsymbol{x},\boldsymbol{\theta_o})]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi^*))-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}) )\\
    &amp;= -\mathbb{E}_{\xi}[d_A(\boldsymbol{x},\xi_A^*,t)]\\
    &amp;\geq 0,~ \text{for any } \xi \text{ on } S,
    \end{align*}\]</span>
which implies <span class="math inline">\(d_A(\boldsymbol{x},\xi_A^*,t)\leq 0\)</span>, for all <span class="math inline">\(\boldsymbol{x} \in S\)</span>.</p>
<p>Lastly, to prove <a href="chapter-SLSE.html#eq:dire-D">(3.6)</a>, let <span class="math inline">\(\xi_c^*\)</span> be the optimal measure under c-optimality. We have
<span class="math display">\[\begin{align*}
    \frac{\partial \phi_c(\xi_\alpha)}{\partial \alpha}\Big|_{\alpha=0}
    &amp;=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)[-\boldsymbol{B}(\xi)+\boldsymbol{B}(\xi_c^*)]\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}\\
    &amp;=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{B}(\xi)\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{B}(\xi_c^*)\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}\\
    &amp;=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}(\xi_c^*)^{-1}\boldsymbol{c}\\
    &amp;=-\mathbb{E}_{\xi}[\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{M}(\boldsymbol{x}) \boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}(\xi_c^*)^{-1}\boldsymbol{c}]\\
    &amp;=-\mathbb{E}_{\xi}[d_c(\boldsymbol{x},\xi_c^*,t)]\\
    &amp;\geq 0,~ \text{for any } \xi \text{ on } S,
    \end{align*}\]</span>
which implies <span class="math inline">\(d_c(\boldsymbol{x},\xi_c^*,t)\leq 0\)</span>, for all <span class="math inline">\(\boldsymbol{x} \in S\)</span>.</p>
</div>
</div>
<div id="section-support-pt" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> Results on the number of support points<a class="anchor" aria-label="anchor" href="#section-support-pt"><i class="fas fa-link"></i></a>
</h2>
<p>Using the results in Theorem <a href="chapter-SLSE.html#thm:dispersion">3.1</a> , we can explore the properties of the optimal designs. In <span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span> and <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2017d">2017</a>)</span>, there are some discussions about the number of support points based on computational results. However, there is little discussion on the number of support points theoretically in <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2017d">2017</a>)</span>, and there is still a large gap to be filled in. Hence we derive several results about the number of support points for various models, including polynomial models, fractional polynomial models, Michaelis-Menton model, Peleg model and trigonometric models.</p>
<p>A polynomial regression model of degree <span class="math inline">\(q\)</span> (<span class="math inline">\(q \ge 1\)</span>) without intercept is given by
<span class="math display" id="eq:model-poly">\[\begin{equation}
y_i=\theta_1 x_i+\theta_2 x_i^2+\cdots+\theta_q x_i^q+\epsilon_i,~x_i\in S=[-1,+1],~i=1,2,\cdots,n.
\tag{3.9}
\end{equation}\]</span>
Polynomial regression models are widely used when the response and regressors have curvilinear relationship. Complex nonlinear relationships can be well approximated by polynomials over a small range of the explanatory variables . There are different kinds of polynomial models such as orthogonal polynomial models, multi-variable polynomial models, and one variable polynomial models. Polynomial models are often used in design of experiment for the response surface methodology, and there are many applications in industry. For example, see <span class="citation">G. E. P. Box and Draper (<a href="references.html#ref-box1987empirical">1987</a>)</span>, <span class="citation">G. E. Box et al. (<a href="references.html#ref-box1978statistics">1978</a>)</span> and <span class="citation">Khuri and Cornell (<a href="references.html#ref-khuri1996response">1996</a>)</span>.</p>
<p>A- and D-optimal designs for <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a> under SLSE are symmetric on <span class="math inline">\(S\)</span> [<span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span>,<span class="citation">Gao and Zhou (<a href="references.html#ref-gao2017d">2017</a>)</span>}. In <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a>, we have
<span class="math inline">\(\boldsymbol{f}(x,\boldsymbol{\theta})= \left(x,x^2,\cdots, x^q \right)^\top\)</span> and
<span class="math display" id="eq:poly">\[\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=\begin{pmatrix}
1           &amp;\sqrt{t}x  &amp;\sqrt{t}x^2    &amp;...&amp;\sqrt{t}x^q\\
\sqrt{t}x   &amp;   x^2     &amp;x^3    &amp;\cdots &amp;x^{q+1}\\
\vdots      &amp;\vdots     &amp;\vdots &amp;\vdots&amp;    \vdots\\
\sqrt{t}x^q &amp;   x^{q+1} &amp;\dots  &amp;\dots  &amp;x^{2q}
\end{pmatrix}_{(q+1)\times(q+1)}.
\tag{3.10}
\end{equation}\]</span></p>
<div class="theorem">
<p><span id="thm:support" class="theorem"><strong>Theorem 3.2  </strong></span>Let <span class="math inline">\(n_A\)</span> and <span class="math inline">\(n_D\)</span> denote the minimum number of support points in A- and D-optimal designs under SLSE, respectively. For <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a>, we have
<span class="math display">\[\begin{equation} \label{support}
n_A ~ = ~ \text{q or q+1},
\end{equation}\]</span>
and
<span class="math display">\[\begin{equation} \label{support2}
n_D ~ = ~ \text{q or q+1}.
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>The proof includes the following three parts.</p>
<p>(i). From @ref(thm:dispersion} and <a href="chapter-SLSE.html#eq:poly">(3.10)</a>, we can see that <span class="math inline">\(d_A(x,\xi_A^*,t)\)</span> and <span class="math inline">\(d_D(x,\xi_D^*,t)\)</span> are polynomial functions of <span class="math inline">\(x\)</span> with highest degree <span class="math inline">\(2q\)</span>. By fundamental theorem of algebra, there are exactly <span class="math inline">\(2q\)</span> roots for <span class="math inline">\(x\)</span> in equations <span class="math inline">\(d_A(x,\xi_A^*,t)=0\)</span> and <span class="math inline">\(d_D(x,\xi_D^*,t)=0\)</span>. However, we have at most <span class="math inline">\(2q\)</span> real roots.</p>
<p>(ii). By the construction, the determinant of <span class="math inline">\(\boldsymbol{B}\)</span> matrix is not zero if and only if the determinant of <span class="math inline">\(\boldsymbol{G_2}\)</span> is not zero. Therefore, there are at least q support points in <span class="math inline">\(\xi\)</span>.</p>
<p>(iii). Both boundary points are the support points, so the number of support points are at most <span class="math inline">\(2q-2\)</span> in the interval <span class="math inline">\((-1, +1)\)</span>. From the equivalence theorem, we know that the dispersion functions are all less or equal to zero (i.e. <span class="math inline">\(d_A(x,\xi_A^*,t)\leq 0\)</span> and <span class="math inline">\(d_D(x,\xi_D^*,t)\leq 0\)</span>), so all those support points in <span class="math inline">\((-1,+1)\)</span> have a multiplicity of two. In total, we have at most <span class="math inline">\(2+\frac{(2q-2)}{2}=q+1\)</span> distinct support points.</p>
<p>Thus, the number of support points in <span class="math inline">\(\xi_A^*\)</span> and <span class="math inline">\(\xi_D^*\)</span> is either <span class="math inline">\(q\)</span> or <span class="math inline">\(q+1\)</span>.
\end{proof}</p>
</div>
<div class="example">
<p><span id="exm:poly2" class="example"><strong>Example 3.1  </strong></span>Consider <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a> with <span class="math inline">\(q=2\)</span> and <span class="math inline">\(S=[-1,+1]\)</span>. Let <span class="math inline">\(\eta_m=\mathbb{E}[x^m]\)</span> be the <span class="math inline">\(m\)</span>-th moment of distribution <span class="math inline">\(\xi(x)\)</span>. <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2014new">2014</a>)</span> showed that the D-optimal design is symmetric in this case (i.e. <span class="math inline">\(\eta_1=\eta_3=0\)</span>), and is given by</p>
<p><span class="math display">\[\begin{equation*}
\xi_D^* = \begin{cases}
    \begin{bmatrix}
    -1          &amp;   +1\\
    \frac{1}{2} &amp;   \frac{1}{2}
    \end{bmatrix},      &amp; \mbox{for $t\in [0,\frac{2}{3})$}, \\
    \begin{bmatrix}
    -1           &amp;  0       &amp;+1\\
    \frac{1}{3t} &amp;\frac{3t-2}{3t}           &amp;\frac{1}{3t}
    \end{bmatrix},                                  &amp; \mbox{for $t\in [\frac{2}{3},1)$}.\\
\end{cases}
\end{equation*}\]</span></p>
<p>We now study the A-optimal design. By taking the advantage of the symmetric result in <span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span>, we have
<span class="math display">\[
    \boldsymbol{B}(\xi)=
    \begin{pmatrix}
    1           &amp;0      &amp;\sqrt{t}\eta_2\\
    0           &amp;\eta_2 &amp;0  \\
    \sqrt{t}\eta_2 &amp;    0   &amp;   \eta_4
    \end{pmatrix},~\text{and }
    \boldsymbol{B}^{-1}(\xi)=\begin{pmatrix}
        \frac{\eta_4}{\eta_4-t\eta_2}&amp;0         &amp;\frac{\sqrt{t}\eta_2}{t\eta_2^2-\eta_4}\\
        0       &amp;\frac{1}{\eta_2}       &amp;0\\
       \frac{\sqrt{t}\eta_2}{t\eta_2^2-\eta_4}&amp;0&amp;\frac{1}{\eta_4-t\eta_2^2}
    \end{pmatrix}.
\]</span>
From <span class="citation">Dette and Studden (<a href="references.html#ref-dette1997theory">1997</a>)</span>, on <span class="math inline">\(S=[-1,+1]\)</span>, the even moments of any distributions must satisfy <span class="math inline">\(0\leq \eta_2^2 \leq \eta_4 \leq \eta_2 \leq 1\)</span>. Then our loss function can be expressed as
<span class="math display">\[
  \phi_A(\xi)=\operatorname{tr}(\boldsymbol{C}^\top \boldsymbol{B}^{-1}\boldsymbol{C})= \frac{1}{\eta_2}+\frac{1}{\eta_4-t\eta_2^2}.
\]</span>
The optimal design problem can be written as
<span class="math display">\[\begin{equation*}
        \begin{aligned}
            &amp; \underset{\eta_2,\eta_4}{\text{min}}
            &amp; &amp; \frac{1}{\eta_2}+\frac{1}{\eta_4-t\eta_2^2} \\
            &amp; \text{s.t.}
            &amp; &amp; 0\leq \eta_2^2 \leq \eta_4 \leq \eta_2 \leq 1.
        \end{aligned}
    \end{equation*}\]</span>
In order to minimize the loss function, we first fix <span class="math inline">\(\eta_2\)</span>. Then it is trivial to see we should make <span class="math inline">\(\eta_4\)</span> as big as possible, which leads to <span class="math inline">\(\eta_4=\eta_2\)</span>, its ceiling. Now the question becomes
<span class="math display">\[\begin{equation*}
        \begin{aligned}
            &amp; \underset{\eta_2}{\text{min}}
            &amp; &amp; \frac{1}{\eta_2}+\frac{1}{\eta_2-t\eta_2^2} \\
            &amp; \text{s.t.}
            &amp; &amp; 0\leq  \eta_2 \leq 1.
        \end{aligned}
    \end{equation*}\]</span>
It yields to <span class="math inline">\(\eta_2=\frac{2-\sqrt{2}}{t}\)</span> or <span class="math inline">\(\eta_2=\frac{2+\sqrt{2}}{t}\)</span> where <span class="math inline">\(t\ne 0\)</span>. Note <span class="math inline">\(t\in[0,+1)\)</span> and <span class="math inline">\(\eta_2\in[0,+1]\)</span>, so the latter solution should be excluded as it is not within the feasible region. Consequently,
<span class="math display">\[\begin{equation*}
\eta_2 = \begin{cases}
    \min{(\frac{2-\sqrt{2}}{t},1)},     &amp; \mbox{$t \in(0,1)$}, \\
    1,                                  &amp; \mbox{$t = 0$}.\\
\end{cases}
\end{equation*}\]</span>
By the symmetric property, the A-optimal design is
<span class="math display">\[  \xi_A^*=
    \begin{bmatrix}
    -1                  &amp;   0           &amp;   +1\\
    \frac{\eta_2}{2}    &amp;   1-\eta_2    &amp;   \frac{\eta_2}{2}
    \end{bmatrix}.
\]</span>
Here we have three cases:\
When <span class="math inline">\(t=0\)</span>,
<span class="math display">\[  \xi_A^*=
    \begin{bmatrix}
    -1              &amp;   +1\\
    \frac{1}{2}     &amp;   \frac{1}{2}
    \end{bmatrix}.
\]</span>
When <span class="math inline">\(t\in(0,2-\sqrt{2})\)</span>,
<span class="math display">\[  \xi_A^*=
    \begin{bmatrix}
    -1              &amp;   +1\\
    \frac{1}{2}     &amp;   \frac{1}{2}
    \end{bmatrix}.
\]</span>
When <span class="math inline">\(t\in[2-\sqrt{2},1)\)</span>,
<span class="math display">\[  \xi_A^*=
    \begin{bmatrix}
    -1                  &amp;   0           &amp;   +1\\
    \frac{2-\sqrt{2}}{2t}   &amp;   1-\frac{2-\sqrt{2}}{t}  &amp;   \frac{2-\sqrt{2}}{2t}
    \end{bmatrix}.
\]</span>
In summary, the A-optimal design for <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a> when <span class="math inline">\(q=2\)</span> is
<span class="math display">\[\begin{equation*}
\xi_A^* = \begin{cases}
    \begin{bmatrix}
    -1          &amp;   +1\\
    \frac{1}{2} &amp;   \frac{1}{2}
        \end{bmatrix},      &amp; \mbox{for $t\in[0, \leq 2-\sqrt{2})$}, \\
    \begin{bmatrix}
    -1          &amp;   0       &amp;+1\\
    \frac{2-\sqrt{2}}{2t}   &amp;\frac{2t+\sqrt{2}-2}{t}            &amp;\frac{2-\sqrt{2}}{2t}
\end{bmatrix},                                  &amp; \mbox{for $t\in[2-\sqrt{2},1)$}.\\
\end{cases}
\end{equation*}\]</span></p>
<p>From the results above, it is clear to observe that A- and D-optimal designs have either <span class="math inline">\(2\)</span> or <span class="math inline">\(3\)</span> support points which is consistent with Theorem <a href="chapter-SLSE.html#thm:support">3.2</a>.
<span class="math inline">\(\Box\)</span></p>
</div>
<p>To show the result in Theorem <a href="chapter-SLSE.html#thm:dispersion">3.1</a>, we plot <span class="math inline">\(d_A(x,\xi^*,t)\)</span> and <span class="math inline">\(d_D(x,\xi^*,t)\)</span> in Figure [TOFILL] using the optimal designs in Example <a href="chapter-SLSE.html#exm:poly2">3.1</a>. It is clear that <span class="math inline">\(d_A(x,\xi^*,t) \leq 0\)</span> and <span class="math inline">\(d_D(x,\xi^*,t)\leq 0\)</span> for all <span class="math inline">\(x\in S\)</span>, which is consistent with Theorem <span class="math inline">\(\ref{theorem:dispersion}\)</span>. Also <span class="math inline">\(d_A(x,\xi_A^*,t)=0\)</span> at the support points of <span class="math inline">\(\xi_A^*\)</span> and <span class="math inline">\(d_D(x,\xi_D^*,t)=0\)</span> at the support points of <span class="math inline">\(\xi_D^*\)</span>. As <span class="math inline">\(t\)</span> increases, the origin becomes a support point which changes the number of support points from 2 to 3. This is again consistent with Theorem <a href="chapter-SLSE.html#thm:support">3.2</a>.</p>
<p>When the design space <span class="math inline">\(S\)</span> is asymmetric, say <span class="math inline">\(S=[-1,b]\)</span>, <span class="math inline">\(b\in(-1,1)\)</span>, the number of support points under A- and D-optimality are either <span class="math inline">\(q\)</span> or <span class="math inline">\(q+1\)</span> for all <span class="math inline">\(q\in \mathbb{Z^+}\)</span>. When <span class="math inline">\(t=0\)</span>, <span class="math inline">\(n_D=q\)</span> for all <span class="math inline">\(q\in\mathbb{Z^+}\)</span>. The derivation is similar to that of Theorem <a href="chapter-SLSE.html#thm:support">3.2</a> and is omitted. We will show some computational results in Chapter <a href="#chapter:applications"><strong>??</strong></a>.</p>
<p>Fractional polynomial model (FPM) is given by
<span class="math display" id="eq:model-frac-poly">\[\begin{equation}
y=\theta_1x^{q_1}+\theta_2x^{q_2}+\cdots+\theta_px^{q_p}+\epsilon,~q_i\in\mathbb{Q},~\forall i,
\tag{3.11}
\end{equation}\]</span>
which provides more flexible parameterization with wide range of applications in many disciplines. For instance, <span class="citation">Cui et al. (<a href="references.html#ref-cui2009fractional">2009</a>)</span> used FPM for longitudinal epidemiological studies, and <span class="citation">Royston and Altman (<a href="references.html#ref-royston1995regression">1995</a>)</span> applied this model to analyze medical data. This model has also been studied for optimal designs. For example, <span class="citation">Torsney and Alahmadi (<a href="references.html#ref-torsney995minimally">1995</a>)</span> used FPM to study the effects of concentration <span class="math inline">\(x\)</span> to viscosity <span class="math inline">\(y\)</span> and this model is given by
<span class="math display" id="eq:ex-frac-poly">\[\begin{equation}
    y=\theta_1x+\theta_2x^{\frac{1}{2}}+\theta_3x^2+\epsilon,~x\in S=(0.0,0.2].
    \tag{3.12}
\end{equation}\]</span>
Let <span class="math inline">\(z=x^{1/2}\)</span>, then @ref(exm:frac_poly) becomes a polynomial model of order <span class="math inline">\(4\)</span> with the new design space <span class="math inline">\(S'=(0.0,\sqrt{0.2}]\)</span>. Now, we can apply the result for the polynomial model <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a> and conclude that <span class="math inline">\(n_A\)</span> and <span class="math inline">\(n_D\)</span> are at most <span class="math inline">\(4\)</span> or <span class="math inline">\(5\)</span>. Moreover, notice that there are only three parameters in model <span class="math inline">\(\eqref{ex:frac_poly}\)</span>. In order to have an invertible matrix <span class="math inline">\(\boldsymbol{G_2}(\xi,\boldsymbol{\theta}_o)\)</span> which is a <span class="math inline">\(3\times 3\)</span> matrix, we need at least <span class="math inline">\(3\)</span> support points. Thus, the number of support point for both A- and D-optimal design is at least 3. In summary, the number of support points in A- and D-optimal designs are either <span class="math inline">\(3\)</span>, <span class="math inline">\(4\)</span>, or <span class="math inline">\(5\)</span>. The purpose of this example is for illustrating that FPM can be transformed into a polynomial model so we can use the result for the number of support points in polynomial models.</p>
<p>Michaelis-Menton model is one of the best-known models proposed by Leonor Michaelis and Maud Menten <span class="citation">(<a href="references.html#ref-mm1913kinetik">Michaelis and Menten 1913</a>)</span> that studies the enzyme reactions between the enzyme and the substrate concentration. This model is given by
<span class="math display" id="eq:model-mm">\[\begin{equation}
y_i=\frac{\theta_1x_i}{\theta_2+x_i}+\epsilon_i.~ i=1,2,\dots,n,~ x_i \in(0,                                                                k_o],~\theta_1,\theta_2\geq 0.
\tag{3.13}
\end{equation}\]</span>
Here, <span class="math inline">\(\boldsymbol{\theta}=(\theta_1,\theta_2)^\top\)</span>. Define <span class="math inline">\(z=\frac{1}{\theta_2+x}\)</span>. Then <span class="math inline">\(\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{x}{\theta_2+x},\frac{-\theta_1x}{(\theta_2+x)^2})^T= (1-z\theta_2, -z\theta_1 +\theta_1 \theta_2z^2)^\top\)</span>, and
<span class="math display">\[\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1                       &amp; \sqrt{t} (1-z\theta_2)        &amp;   \sqrt{t}(-\theta_1 z(1-z\theta_2))\\
\sqrt{t} (1-z\theta_2)  &amp;(1-z\theta_2)^2            &amp;   -z\theta_1(1-z\theta)^2\\
\sqrt{t}(-\theta_1 z(1-z\theta_2))&amp;-z\theta_1(1-z\theta)^2&amp;(z\theta_1(1-z\theta_2))^2
\end{pmatrix}.
\end{equation}\]</span></p>
<p>We can clearly see that, after the transformation, <span class="math inline">\(z\in S'=[\frac{1}{\theta_1+k_o},\frac{1}{\theta_1}]\)</span>, and since <span class="math inline">\(\boldsymbol{f}(x,\boldsymbol{\theta_o})\)</span> and <span class="math inline">\(\boldsymbol{M}(x,\boldsymbol{\theta},t)\)</span> are polynomial functions of <span class="math inline">\(z\)</span>, <span class="math inline">\(\phi_D\)</span> and <span class="math inline">\(\phi_A\)</span> can be described as polynomial functions with highest degrees <span class="math inline">\(4\)</span>. Now, similar to the discussion for model <a href="chapter-SLSE.html#eq:model-poly">(3.9)</a>, we can find the results for <span class="math inline">\(n_A\)</span> and <span class="math inline">\(n_D\)</span> for model @ref(eq:model-mm} and they are presented in the following Lemma.</p>
<div class="lemma">
<p><span id="lem:mm-support" class="lemma"><strong>Lemma 3.5  </strong></span>For Michaelis-Menton model in <a href="chapter-SLSE.html#eq:model-mm">(3.13)</a>, the number of support points for D-optimal design and A-optimal design are either <span class="math inline">\(2\)</span> or <span class="math inline">\(3\)</span> for <span class="math inline">\(t\in[0,1)\)</span>. Moreover, there are 2 support points under D-optimality when <span class="math inline">\(t=0\)</span>.</p>
</div>
<p>The proof of Lemma <a href="chapter-SLSE.html#lem:mm-support">3.5</a> is similar to that of Theorem <span class="math inline">\(\ref{theorem:support}\)</span> and is omitted. It is easy to observe that <span class="math inline">\(d_D(0,\xi_D^*,0)&lt;0\)</span>, so the boundary point, 0, is not a support point under D-optimality which leads to the conclusion <span class="math inline">\(n_D=2\)</span>. Moreover, <span class="math inline">\(n_c\le 3\)</span> for <span class="math inline">\(t\in[0,1)\)</span>.</p>
<p>Peleg model is a statistical model used to investigate the relationship of water absorption for various kinds of food. This model is given by
<span class="math display" id="eq:model-peleg">\[\begin{equation}
y_i=y_o+\frac{x_i}{\theta_1+\theta_2 x_i}+\epsilon_i,\quad i=1,2,\cdots,n,
\tag{3.14}
\end{equation}\]</span>
where <span class="math inline">\(y_o\)</span> represents the initial moisture of the food, <span class="math inline">\(y_i\)</span> is the current level of moisture at current time <span class="math inline">\(x_i\)</span>, <span class="math inline">\(\theta_1\)</span> is the Peleg’s moisture rate constant, and <span class="math inline">\(\theta_2\)</span> is the asymptotic moisture as time increases. Note that parameters <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> must be positive. Optimal experimental designs have been studied for this model using OLSE, for example, <span class="citation">Paquet-Durand, Zettel, and Hitzmann (<a href="references.html#ref-paquet2015optimal">2015</a>)</span>. For <a href="chapter-SLSE.html#eq:model-peleg">(3.14)</a>, <span class="math inline">\(\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{-x}{(\theta_1+\theta_2x_i)^2},\frac{-x^2}{(\theta_1+\theta_2x)^2})^T\)</span>. We let <span class="math inline">\(z=\frac{1}{\theta_1+\theta_2x}\)</span>. Then for <span class="math inline">\(S=[0,d]\)</span>, the new design space after the transformation is <span class="math inline">\(S'=[\frac{1}{\theta_1+\theta_2d},\frac{1}{\theta_1}]\)</span>. Now, <span class="math inline">\(\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{-z(1-\theta_1z)}{\theta_2},\frac{-(1-\theta_1z)^2}{\theta_2^2})^T\)</span>. Since <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> are parameters, <span class="math inline">\(\boldsymbol{f}\)</span> depends on <span class="math inline">\(z\)</span> only. Hence, <span class="math inline">\(\boldsymbol{M}(x,\boldsymbol{\theta},t)\)</span> becomes a polynomial function of <span class="math inline">\(z\)</span> of degree <span class="math inline">\(4\)</span>. Therefore, the number of support points, <span class="math inline">\(n_A\)</span> and <span class="math inline">\(n_D\)</span> are also either 2 or 3 for the Peleg model. In addition, <span class="math inline">\(n_c\)</span> is either 1, 2 or 3 for <span class="math inline">\(t\in[0,1)\)</span>.</p>
<p>Next we consider trigonometric models without intercept. If the intercept term is included, it has been proven that optimal designs under SLSE and OLSE are the same. The <span class="math inline">\(k\)</span>th order trigonometric model is given by
<span class="math display" id="eq:model-trig">\[\begin{equation}
    y_i=\sum_{j=1}^{k}[cos(jx_i)\theta_{1j}+sin(jx_i)\theta_{2j}]+\epsilon_i,~i=1,2,\cdots,n,
    \tag{3.15}
\end{equation}\]</span>
where <span class="math inline">\(x_i\in S_b=[-b\pi,b\pi],~0&lt;b\leq 1\)</span>. Let <span class="math inline">\(\boldsymbol{\theta}=(\theta_{11},\theta_{12},\cdots,\theta_{1k},\theta_{21},\cdots,\theta_{2k})^T\)</span>. From <a href="chapter-SLSE.html#eq:M-matrix">(3.5)</a>, we get
<span class="math display" id="eq:trig-M-matrix">\[\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1       &amp;\sqrt{t}cos(x) &amp;   \dots       &amp;\sqrt{t}sin(kx)\\
\sqrt{t}cos(x)  &amp;cos^2(x)   &amp;\dots  &amp;cos(x)sin(kx)\\
\vdots  &amp;\vdots &amp;   \ddots  &amp;\vdots\\
\sqrt{t}sin(kx) &amp;cos(x)sin(kx)&amp;\dots    &amp;sin^2(kx)
\end{pmatrix}_{(2k+1)\times(2k+1)}.
\tag{3.16}
\end{equation}\]</span>
We consider two cases of design space, (i) <span class="math inline">\(b=1\)</span>, full circle and (ii) <span class="math inline">\(0&lt;b&lt;1\)</span>, the partial circle.</p>
<p>For case (i), the following Lemma is helpful for finding the number of support points for <a href="chapter-SLSE.html#eq:model-trig">(3.15)</a>.</p>
<div class="lemma">
<p><span id="lem:tring-property" class="lemma"><strong>Lemma 3.6  </strong></span>For trigonometric functions, <span class="math inline">\(j, u =1,2,\cdots,k,~u\ne j\)</span>, we have</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\int_{-\pi}^{\pi}cos(jx)dx=\int_{-\pi}^{\pi}sin(jx)dx=0\)</span>,</li>
<li>
<span class="math inline">\(\int_{-\pi}^{\pi}cos^2(jx)dx=\int_{-\pi}^{\pi}sin^2(jx)dx=\pi\)</span>,</li>
<li>
<span class="math inline">\(\int_{-\pi}^{\pi}cos(jx)cos(ux)dx=\int_{-\pi}^{\pi}sin(jx)sin(ux)dx=0\)</span>,</li>
<li>
<span class="math inline">\(\int_{-\pi}^{\pi}sin(jx)cos(ux)dx=\int_{-\pi}^{\pi}cos(jx)sin(ux)dx=0\)</span>. This also holds for <span class="math inline">\(u=j\)</span>.</li>
</ol>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-5" class="theorem"><strong>Theorem 3.3  </strong></span>For model <a href="chapter-SLSE.html#eq:model-trig">(3.15)</a> with design space <span class="math inline">\(S=[-\pi,\pi]\)</span>, the uniform distribution on <span class="math inline">\(S\)</span> is both A- and D-optimal designs.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-6" class="proof"><em>Proof</em>. </span>For the uniform distribution <span class="math inline">\(\xi^*\)</span>, we have <span class="math inline">\(\boldsymbol{B}(\xi^*,\boldsymbol{\theta_o},t)=1\oplus \frac{1}{2} \boldsymbol{I_{2k}}\)</span> by Lemma <a href="#lem:trig-property"><strong>??</strong></a>. From <a href="chapter-SLSE.html#eq:trig-M-matrix">(3.16)</a>, we obtain
<span class="math display">\[\begin{equation*}
\begin{aligned}
&amp;tr(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))-(q+1)\\    &amp;=1+2cos^2(x)+2cos^2(2x)+\dots+2sin^2(kx)-(2k+1)\\
&amp;=1+2[cos^2(x)+sin^2(x)]+\dots+2[cos^2(kx)+sin^2(kx)]-(2k+1)\\
        &amp;=1+2k-(2k+1)\\
        &amp;=0,\quad \text{for all } x\in S.
\end{aligned}
\end{equation*}\]</span>
This implies that <span class="math inline">\(d_D(x,\xi^*,t)=0\)</span> for all <span class="math inline">\(x\in S\)</span>. By Theorem <a href="chapter-SLSE.html#thm:dispersion">3.1</a>, <span class="math inline">\(\xi^*\)</span> is a D-optimal design.</p>
<p>For A-optimality, it is easy to get
<span class="math display">\[\begin{equation*}
\begin{aligned}
&amp;tr(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t)\boldsymbol{C}^T\boldsymbol{C}\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))-\operatorname{tr}(\boldsymbol{C}\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))\\&amp;=4cos^2(x)+4cos^2(2x)+\dots+4sin^2(kx)-4k\\
&amp;=4[cos^2(x)+sin^2(x)]+\dots+4[cos^2(kx)+sin^2(kx)]-4k\\
&amp;=4k-4k\\
&amp;=0,\quad \text{for all } x\in S,
\end{aligned}
\end{equation*}\]</span>
which gives <span class="math inline">\(d_A(x,\xi^*,t)=0\)</span> for all <span class="math inline">\(x \in S\)</span>. Thus, by Theorem <a href="chapter-SLSE.html#thm:dispersion">3.1</a>, <span class="math inline">\(\xi^*\)</span> is an A-optimal design.</p>
</div>
<p>For case (ii), <span class="math inline">\(0&lt;b&lt;1\)</span>, the partial circle <span class="math inline">\(S=[-b\pi,+b\pi]\)</span>, let <span class="math inline">\(z=cos(x)\)</span>. Now, instead of using <span class="math inline">\(x\)</span> directly, we study the number of support point of <span class="math inline">\(x\)</span> through <span class="math inline">\(z\)</span> in <span class="math inline">\(S'=[cos(b\pi),1]\)</span>. Note that cosine is an even function, so each point of <span class="math inline">\(z\in S'\)</span> corresponds to two symmetric points around <span class="math inline">\(0\)</span>, <span class="math inline">\(\pm x\in S\)</span>. <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2017d">2017</a>)</span> discussed that all the elements in <span class="math inline">\(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\)</span> in <a href="chapter-SLSE.html#eq:trig-M-matrix">(3.16)</a> can be written as polynomial functions of <span class="math inline">\(z\)</span>. Hence, functions <span class="math inline">\(d_A(x,\xi,t)\)</span>, <span class="math inline">\(d_D(x,\xi,t)\)</span> and <span class="math inline">\(d_c(x,\xi,t)\)</span> are all polynomial functions of <span class="math inline">\(z\)</span> with degree <span class="math inline">\(2k\)</span>. Using the similar arguments for the polynomial models, A-, D- and c-optimal designs, in terms of <span class="math inline">\(z\in S'\)</span>, cannot exceed <span class="math inline">\(k+1\)</span> support points.</p>
</div>
<div id="scale-invariance-property-of-d-optimal-design" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> Scale invariance property of D-optimal design<a class="anchor" aria-label="anchor" href="#scale-invariance-property-of-d-optimal-design"><i class="fas fa-link"></i></a>
</h2>
<p>D-optimality is one of the most used design criteria due to its many advantages. One good property of this criterion is that it is invariant under some scaling of the independent variables using OLSE <span class="citation">(<a href="references.html#ref-berger2009introduction">Berger and Wong 2009</a>)</span>. Recently there are some discussions about the invariance properties including scale invariance and shift invariance for the optimal designs under the SLSE. Examples can be found in <span class="citation">Gao and Zhou (<a href="references.html#ref-gao2014new">2014</a>)</span> and <span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span>. In this section, we focus on finding a new property of scale invariance of D-optimal designs under the SLSE for nonlinear regression models.</p>
<p>For linear regression models, D-optimal designs are often scale invariant. On the other hand, if the model is nonlinear, the scale invariance property is no longer available. The optimal designs for nonlinear models are called locally optimal, since they depend on the true parameter vector <span class="math inline">\(\boldsymbol{\theta_o}\)</span>. Thus, D-optimal designs have to be constructed for each <span class="math inline">\(\boldsymbol{\theta_o}\)</span> and for each <span class="math inline">\(S\)</span>. <span class="citation">Wong and Zhou (<a href="references.html#ref-wong2019cvx">2019</a>)</span> proposed a generalized scale invariance (GSI) concept for studying scale invariance property of D-optimal designs for nonlinear models and generalized linear models. The GSI property is also useful for studying D-optimal designs under the SLSE such that the designs may be constructed on a scaled design space <span class="math inline">\(S^V\)</span> instead of the original design space <span class="math inline">\(S\)</span>.</p>
<p>Denote the D-optimal design for a given model, with true parameter vector <span class="math inline">\(\boldsymbol{\theta}_o\)</span> on a design space <span class="math inline">\(S\)</span> by <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta}_o)\)</span>. Also, denote the scaled design space from the original design space <span class="math inline">\(S\)</span> by <span class="math inline">\(S^{V}=\{\boldsymbol{V}\boldsymbol{x}|\boldsymbol{x}\in S\}\)</span>.</p>
<div class="definition">
<p><span id="def:scale-matrix" class="definition"><strong>Definition 3.1  </strong></span>The matrix <span class="math inline">\(\boldsymbol{V}\)</span> is called scale matrix which is a diagonal matrix defined as <span class="math inline">\(\boldsymbol{V}=diag(v_1,v_2,\cdots,v_p)\)</span>, where all <span class="math inline">\(v_i\)</span> are positive.</p>
</div>
<div class="definition">
<p><span id="def:scale-invariant" class="definition"><strong>Definition 3.2  </strong></span>Transforming <span class="math inline">\(\boldsymbol{x}\)</span> to <span class="math inline">\(\boldsymbol{V} \boldsymbol{x}\)</span> is called scale transformation.</p>
</div>
<div class="definition">
<p><span id="def:GSI" class="definition"><strong>Definition 3.3  </strong></span><span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta}_o)\)</span> is said to be scale invariant for a model if there exists a parameter vector <span class="math inline">\(\tilde{\boldsymbol{\theta_o}}\)</span> such that the D-optimal design <span class="math inline">\(\xi_D^*(S^V,\tilde{\boldsymbol{\theta_o}})\)</span> can be obtained from <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta_o})\)</span> using some scale transformations.</p>
</div>
<p>This property of the design, <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta_o})\)</span>, is defined as a generalized scale invariance. Note that <span class="math inline">\(\tilde{\boldsymbol{\theta_o}}\)</span> and <span class="math inline">\(\boldsymbol{\theta_o}\)</span> are closed related through the elements <span class="math inline">\(v_1,\cdots,v_p\)</span> in the scale matrix <span class="math inline">\(\boldsymbol{V}\)</span>. When the model is linear, GSI of <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta_o})\)</span> becomes the traditional scale invariance since it does not depend on <span class="math inline">\(\boldsymbol{\theta_o}\)</span>. For many nonlinear regression models, the GSI property holds for D-optimal designs under both OLSE and SLSE. The following lemma provides a condition to check for this property.</p>
<div class="lemma">
<p><span id="lem:GSI" class="lemma"><strong>Lemma 3.7  </strong></span>For a given model and a scale matrix <span class="math inline">\(\boldsymbol{V}\)</span>, if there exists a parameter vector <span class="math inline">\(\tilde{\boldsymbol{\theta}_o}\)</span> and an invertible diagonal matrix <span class="math inline">\(\boldsymbol{K}\)</span> which does not depend on <span class="math inline">\(\boldsymbol{x}\)</span>, such that <span class="math inline">\(\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta}_o)\)</span> for all <span class="math inline">\(\boldsymbol{x}\in S\)</span>, then the design <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta}_o)\)</span> has the generalized scale invariance property.
\end{lemma}</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-7" class="proof"><em>Proof</em>. </span>For design space <span class="math inline">\(S\)</span> and parameter vector <span class="math inline">\(\boldsymbol{\theta_o}\)</span>, <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta_o})\)</span> minimizes <span class="math inline">\(\phi_D^*(\xi,\boldsymbol{\theta_o})=\det(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o}))\)</span> where <span class="math inline">\(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o)=\mathbb{E}[\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)]\)</span> and <span class="math inline">\(\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)\)</span> is given by <span class="math inline">\(\eqref{M matrix}\)</span>, and the expectation is taken with respect to <span class="math inline">\(\xi(\boldsymbol{x})\)</span> on <span class="math inline">\(S\)</span>.</p>
<p>For design space <span class="math inline">\(S^V=\{\boldsymbol{V} \boldsymbol{x}|\boldsymbol{x}\in S\}\)</span> and parameter vector <span class="math inline">\(\tilde{\boldsymbol{\theta_o}}\)</span>, we minimize the following function to find <span class="math inline">\(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\)</span>, <span class="math inline">\(\phi_D^*(\xi,\tilde{\boldsymbol{\theta}_o})=\det(\boldsymbol{B}^{-1}(\xi,\tilde{\boldsymbol{\theta}_o}))\)</span>, where <span class="math inline">\(\boldsymbol{B}(\xi,\tilde{\boldsymbol{\theta}_o})=\mathbb{E}[\boldsymbol{M}(\boldsymbol{z},\tilde{\boldsymbol{\theta}_o})]\)</span>, and the expectation is taken with respect to <span class="math inline">\(\xi(\boldsymbol{z})\)</span> on <span class="math inline">\(S^V\)</span>. Each <span class="math inline">\(\boldsymbol{z}\in S^V\)</span> can be written as <span class="math inline">\(\boldsymbol{V}\boldsymbol{x}\)</span> followed by definition, where <span class="math inline">\(\boldsymbol{x}\in S\)</span>. Thus, we have <span class="math inline">\(\boldsymbol{M}(\boldsymbol{z},\tilde{\boldsymbol{\theta}_o})=\boldsymbol{M}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})\)</span>.</p>
<p>By the assumption of Lemma <a href="chapter-SLSE.html#lem:GSI">3.7</a> and <a href="chapter-SLSE.html#eq:M-matrix">(3.5)</a>, we get <span class="math display">\[
\boldsymbol{M}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})=(1 \oplus \boldsymbol{K}) \boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)(1 \oplus \boldsymbol{K}),~for~all~\boldsymbol{x}\in S.
\]</span>
Therefore, <span class="math inline">\(\boldsymbol{B}(\xi,\tilde{\boldsymbol{\theta}_o})= (1 \oplus \boldsymbol{K}) \boldsymbol{B}(\xi,\boldsymbol{\theta}_o)(1 \oplus \boldsymbol{K})\)</span> and <span class="math inline">\(\phi_D(\xi,\tilde{\boldsymbol{\theta}_o})=\frac{\phi_D(\xi,\boldsymbol{\theta}_o)}{\det^2(\boldsymbol{K})}\)</span>. This implies that we can minimize <span class="math inline">\(\phi_D(\xi,\boldsymbol{\theta}_o)\)</span> to obtain <span class="math inline">\(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\)</span>, where <span class="math inline">\(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\)</span> is the scale transformation from <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta}_o)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:peleg" class="example"><strong>Example 3.2  </strong></span>Consider Peleg regression model in <a href="chapter-SLSE.html#eq:model-peleg">(3.14)</a>. In this model, since <span class="math inline">\(p=1\)</span>, the scale matrix <span class="math inline">\(V=v_1&gt;0\)</span> is a scalar and <span class="math inline">\(\boldsymbol{\theta}_o=(a,b)^\top\)</span>. Now, let <span class="math inline">\(\tilde{\boldsymbol{\theta}_o}=(a,\frac{b}{v_1})^\top\)</span> and we can obtain</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})  
        &amp;= \boldsymbol{f}(v_1x,\tilde{\boldsymbol{\theta}_o})\\
        &amp;= \begin{pmatrix}\frac{-v_1x}{(a+\frac{b}{v_1}v_1x)^2}, &amp; \frac{-(v_1x)^2}{(a+\frac{b}{v_1}v_1x)^2}\end{pmatrix}^\top\\
        &amp;=\begin{pmatrix} v_1\frac{-x}{(a+bx)^2}, &amp; v_1^2\frac{-x^2}{(a+bx)^2}\end{pmatrix}^\top\\
        &amp;= \begin{pmatrix} v_1  &amp;0\\0&amp;v_1^2\end{pmatrix}\begin{pmatrix} \frac{-x}{(a+bx)^2}, &amp; \frac{-(v_1x)^2}{(a+bx)^2}\end{pmatrix}^\top\\
        &amp;=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta_o}).
\end{aligned}
\end{equation*}\]</span>
Hence, Peleg model has GSI property based on Lemma <a href="chapter-SLSE.html#lem:GSI">3.7</a> by choosing <span class="math inline">\(\boldsymbol{K}=diag(v_1,v_1^2)\)</span> and <span class="math inline">\(\tilde{\boldsymbol{\theta_o}}=(a,\frac{b}{v_1})^T\)</span>. <span class="math inline">\(\Box\)</span></p>
</div>
<p>It is worth noting that when matrix <span class="math inline">\(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)\)</span> is ill-conditioned (i.e. the condition number of the matrix is very large), numerical algorithm computing <span class="math inline">\(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta}_o,t)\)</span> may fail as the numerical inverse is inaccurate and imprecise. In these situations, the GSI property may be helpful. Here we provide one example to demonstrate the usefulness of the GSI property.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 3.3  </strong></span>Piecewise polynomial regression using knots is frequently used and has various applications. See <span class="citation">Dette, Melas, and Pepelyshev (<a href="references.html#ref-dette2008optimal">2008</a>)</span> and the references therein. They investigated optimal designs under OLSE for piecewise polynomial regression model with unknown knots, and obtained results for the number of support points under D-optimality and various other properties about designs. Here we consider one model they used, a cubic spline regression model with unknown knots, which is given by
<span class="math display" id="eq:spline-regression">\[\begin{equation}
y=\theta_1+\theta_2x+\theta_3x^2+\theta_4x^3+\theta_5(x-\lambda)_{+}^3+\epsilon,~x\in [0,b]
\tag{3.17}
\end{equation}\]</span>
where <span class="math inline">\((x-\lambda)_{+} = \max(0,(x-\lambda))\)</span>. Model <span class="math inline">\(\eqref{model:spline_regression}\)</span> is nonlinear with parameter vector <span class="math inline">\(\boldsymbol{\theta}_o=(\theta_1,\cdots,\theta_5,\lambda)^T\)</span>, <span class="math inline">\(\boldsymbol{f}(x,\boldsymbol{\theta}_o)=(1,x,x^2,x^3,(x-\lambda)_+^3,-3\theta_5(x-\lambda)_+^2)^T\)</span>. We now start to illustrate GSI property for this model. Consider a scale matrix <span class="math inline">\(\boldsymbol{V}=v_1&gt;0\)</span> and let <span class="math inline">\(\tilde{\boldsymbol{\theta}_o}=(\theta_1,\cdots,\theta_5,v_1\lambda)^T\)</span>. Then we have, for all <span class="math inline">\(x\in[0,b]\)</span>,</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta_o}})  
        &amp;= \boldsymbol{f}(v_1x,\tilde{\boldsymbol{\theta_o}})\\
        &amp;= (1, v_1x,  (v_1x)^2,  (v_1x)^3,  (v_1x-v_1\lambda)_+^3,  -3\theta_5(v_1x-v_1\lambda)_+^2)^T\\
        &amp;= (1,  v_1,x,  (v_1x)^2,  (v_1x)^3,  v_1^3(x-\lambda)_+^3,  -3\theta_5v_1^2(x-\lambda)_+^2 )^T\\
        &amp;= diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2) (1, x,  x^2, x^3, (x-\lambda)_+^3, -3\theta_5(x-\lambda)_+^2)^T\\
        &amp;=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta_o}),\quad \text{with } \boldsymbol{K}=diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2).
\end{aligned}
\end{equation*}\]</span>
Hence, by Lemma <a href="chapter-SLSE.html#lem:GSI">3.7</a>, the D-optimal design under SLSE has the GSI property. Moreover, the model is linear in <span class="math inline">\(\boldsymbol{\theta}_o'=(\theta_1,\theta_2,\cdots,\theta_5)^\top\)</span>, so the D-optimal designs under both OLSE and SLSE do not depend on <span class="math inline">\(\boldsymbol{\theta}_o'\)</span> <span class="citation">Yin and Zhou (<a href="references.html#ref-yin2018optimal">2017</a>)</span>. Thus, for <span class="math inline">\(S=[0,b]\)</span> and <span class="math inline">\(S^{V}=[0,v_1b]\)</span>, the D-optimal designs <span class="math inline">\(\xi_D^*(S,\boldsymbol{\theta}_o)\)</span> and <span class="math inline">\(\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})\)</span> can be obtained from each other by using the corresponding <span class="math inline">\(\boldsymbol{\theta}_o\)</span> and <span class="math inline">\(\tilde{\boldsymbol{\theta}}\)</span>. This property can help us dramatically for finding the D-optimal designs when matrix <span class="math inline">\(\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)\)</span> is ill-conditioned. Numerical results of D-optimal designs are presented in Example <span class="math inline">\(\ref{example:spline}\)</span> after we discuss about numerical algorithms in Chapter <a href="#chapter:applications"><strong>??</strong></a>. <span class="math inline">\(\Box\)</span></p>
</div>
<p>We also want to note that in model <a href="chapter-SLSE.html#eq:spline-regression">(3.17)</a>, the intercept is included so the D-optimal designs under both OLSE and SLSE are the same <span class="citation">(<a href="references.html#ref-gao2014new">Gao and Zhou 2014</a>)</span>. Moreover, the result may easily be extended for piecewise regression models with multiple unknown knots.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">2</span> Introduction</a></div>
<div class="next"><a href="chapter-applications.html"><span class="header-section-number">4</span> Numerical Algorithm and Applications</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#chapter-SLSE"><span class="header-section-number">3</span> Optimal Regression Designs Under SLSE</a></li>
<li><a class="nav-link" href="#design-criteria-under-slse"><span class="header-section-number">3.1</span> Design criteria under SLSE</a></li>
<li><a class="nav-link" href="#equivalence-theorem-for-optimal-designs-under-slse"><span class="header-section-number">3.2</span> Equivalence theorem for optimal designs under SLSE</a></li>
<li><a class="nav-link" href="#section-support-pt"><span class="header-section-number">3.3</span> Results on the number of support points</a></li>
<li><a class="nav-link" href="#scale-invariance-property-of-d-optimal-design"><span class="header-section-number">3.4</span> Scale invariance property of D-optimal design</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Optimal Regression Design under Second-Order Least Squares Estimator: Theory, Algorithm and Applications</strong>" was written by Chi-Kuang Yeh. It was last built on 2024-05-20.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
