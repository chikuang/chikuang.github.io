[{"path":"index.html","id":"preface","chapter":"1 Preface","heading":"1 Preface","text":"thesis written Chi-Kuang Yeh, guidance Professor Julie Zhou. create bookdown project fix typos within .","code":""},{"path":"index.html","id":"chi-kuang-yeh","chapter":"1 Preface","heading":"1.1 Chi-Kuang Yeh","text":"B.Sc. (Hons.), University Victoria, 2016","code":""},{"path":"index.html","id":"supervisory-committee","chapter":"1 Preface","heading":"1.2 Supervisory Committee","text":"Supervisor: Dr. Julie Zhou (Professor, Department Mathematics Statistics)Departmental Member: Dr. Xuekui Zhang (Assistant Professor, Department Mathematics Statistics)External Examiner: Dr. Xiaodai Dong (Professor, Department Electrical Computer Engineering)","code":""},{"path":"index.html","id":"abstract","chapter":"1 Preface","heading":"1.3 Abstract","text":"thesis, first review current development optimal regression designs second-order least squares estimator literature. criteria include - D-optimality. introduce new formulation -optimality criterion result can extended c-optimality studied . Following Kiefer’s equivalence results, derive optimality conditions -, c- D-optimal designs second-order least squares estimator. addition, study number support points various regression models including Peleg models, trigonometric models, regular fractional polynomial models. generalized scale invariance property D-optimal designs also explored. Furthermore, discuss one computing algorithm find optimal designs numerically. Several interesting applications presented related MATLAB code provided thesis.","code":""},{"path":"index.html","id":"list-of-abbreviations","chapter":"1 Preface","heading":"1.4 List of Abbreviations","text":"BLUE: Best Linear Unbiased EstimatorCVX: MATLAB-based modeling system convex optimizationFRM: Fractional Polynomial ModelGSI: Generalized Scale InvarianceMATLAB: script language uses matrix default format computing environment, developed MathWorksOLSE: Ordinary Least Squares EstimatorSDP: Semi-Definite ProgrammingSLSE: Second-Order Least Squares Estimator","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"Design experiment sub-field statistics long history developments. Sir Ronald . Fisher’s pioneering work analysis variance fractional factorial design concepts, working Rothamsted Experimental Station 1920s 1930s , many statisticians worked research area made significant contributions. Berger Wong (2009) gave many vital examples development optimal design theory years, including Chernoff (1953), Kiefer (1959), Kiefer (1974), Kiefer Wolfowitz (1959), Samuel David Silvey (1980) Atkinson Donev (1992). inputs field huge impacts towards today’s design framework. One century later, design techniques found effective now widely used disciplines, agriculture, engineering, environmental science, biomedical pharmaceutical studies. See examples Crary et al. (2000), Crary (2002), Haines, Perevozskaya, Rosenberger (2003), Zhou et al. (2003), Jain Tiwari (2003), Brosi, Armsworth, Daily (2008) Schorning et al. (2017). primary objective study cause effect variables systematic approaches procedures.decades, many theoretical results algorithms developed construct different kinds optimal designs, include factorial design, fractional factorial design, response surface design, regression design. thesis, focus optimal regression design recently proposed statistical estimator, second-order least squares estimator (SLSE) Wang Leblanc (2008).","code":""},{"path":"intro.html","id":"optimal-regression-design-problem","chapter":"2 Introduction","heading":"2.1 Optimal regression design problem","text":"``Regression analysis statistical technique investigating modeling relationship variables” (Montgomery, Peck, Vining 2012). one widely used statistical methods explore relationship variables based observed data. Although different kinds regression models, mainly focus one response linear nonlinear regression models.Suppose want study relationship \\(\\boldsymbol{x}\\\\mathbb{R}^p\\) (vector explanatory variables) \\(y\\) (response variable). Consider general regression model (\\(\\boldsymbol{x_i},y_i\\)),\\[\\begin{equation}\ny_i=g\\left(\\boldsymbol{x_i};\\boldsymbol{\\theta}\\right)+\\epsilon_i,\\quad ~=1,\\dots,n,\n\\tag{2.1}\n\\end{equation}\\]\n\\(\\boldsymbol{\\theta}\\\\mathbb{R}^q\\) unknown parameter vector, \\(n\\) sample size, \\(g(\\cdot)\\) known linear nonlinear expectation function depending \\(\\boldsymbol{\\theta}\\), \\(\\epsilon_i\\) random error regression model. random error \\(\\epsilon_i's\\) assumed independent identically distributed zero mean variance \\(\\sigma^2\\). random error terms assumed homoscedastic thesis. Since \\(\\boldsymbol{x_i}\\) \\(y_i\\) observed data \\(g(\\cdot)\\) also known, unknown component \\(\\boldsymbol{\\theta}\\). question naturally comes mind estimate \\(\\boldsymbol{\\theta}\\) efficiently.Suppose \\(\\hat{\\boldsymbol{\\theta}}\\) estimator \\(\\boldsymbol{\\theta}\\). design problem aims get information \\(\\boldsymbol{\\theta}\\) \\(g\\left(\\boldsymbol{x}_i;\\boldsymbol{\\theta}\\right)\\) selecting best probability distribution \\(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\dots,\\boldsymbol{x}_n\\) maximizes scalar functions Fisher’s information matrix \\(\\hat{\\boldsymbol{\\theta}}\\). sample points \\(\\boldsymbol{x}_i\\) space called design points design space, respectively. known Fisher’s information matrix proportional inverse variance-covariance matrix \\(\\hat{\\boldsymbol{\\theta}}\\). Thus, design problem aims minimize scalar functions variance-covariance matrix \\(\\hat{\\boldsymbol{\\theta}}\\), called objective functions loss functions. resulted probability measure \\(\\xi\\) contains two components support points corresponding probabilities associated points. choice loss functions determined based design interest. Various design criteria studied literature, - D-optimality criteria. D-optimality one widely used design criterion minimizes determinant variance-covariance matrix. desired property D-optimal design scale invariant property. -optimality minimizes trace leads minimize sum variances estimated parameters. See Fedorov (1972), Samuel David Silvey (1980), Pukelsheim (2006), Berger Wong (2009), Dean et al. (2015) optimality criteria.Let us consider Gompertz growth model illustrate D-optimal design. model given \n\\[\\begin{equation*}\ny_i=\\theta_1 e^{-\\theta_2e^{-\\theta_3 x_i}}+\\epsilon_i,~=1,2,\\cdots,n,\\quad \\boldsymbol{\\theta}\n=\\left(\\theta_1,\\theta_2,\\theta_3\\right)^\\top,\\quad x_i\\S,\n\\end{equation*}\\]\n\\(\\theta_1\\) describes maximum growing capacity, \\(\\theta_2\\) explains initial status subject, \\(\\theta_3\\) determines growth rate, \\(y\\) overall growth current time point \\(x\\) time. Note \\(x\\), \\(\\theta_1\\) , \\(\\theta_2\\) \\(\\theta_3\\) assumed positive context. want study one subject’s total growth associated time. model broad applications biological science cancer studies. See examples . Suppose design space \\(S\\) \\([0,10]\\) (.e. \\(x\\[0,10]\\)) true parameters \\(\\boldsymbol{\\theta}\\) given \\(\\boldsymbol{\\theta}_o=(1,1,1)^\\top\\). model, D-optimal design aims select probability measure \\(S\\) minimizes \\(\\det(\\hat{\\boldsymbol{\\theta}})\\), \\(\\hat{\\boldsymbol{\\theta}}\\) SLSE. details SLSE discussed Section \\(\\ref{section:SLSE}\\) Chapter \\(\\ref{chapter:optimal regression SLSE}\\). resulted probability measure D-optimality \n\\[\\begin{equation}\n  \\xi_D^*=\n\\begin{bmatrix}\n0.000 &   1.350 &  10.000\\\\\n1/3 &   1/3 &  1/3\n\\end{bmatrix},\n\\tag{2.2}\n\\end{equation}\\]top row represents support points second row describes corresponding probabilities points. resulted design three support points \\(x=0.000,~1.350\\) \\(10.000\\) equal weight (\\(1/3\\)). interpretation (2.2) distribute resources evenly three points, \\(0.000\\), \\(1.350\\) \\(10.000\\). instance, maximum number runs available fifteen due scarcity resources, researcher make five observations three points. Figure \\(\\ref{fig:gompertz}\\), line represents behavior expectation function \\(g(\\boldsymbol{x};\\boldsymbol{\\theta})\\) design space S, \\(*\\) represents support point.Many studies conducted using ordinary least squares estimator (OLSE) estimator (\\(\\hat{\\boldsymbol{\\theta}}\\)) optimal regression design framework. OLSE best linear unbiased estimator (BLUE) regression context. However, error distribution asymmetric, SLSE efficient OLSE Wang Leblanc (2008), reviewed next section.","code":""},{"path":"intro.html","id":"section-SLSE","chapter":"2 Introduction","heading":"2.2 Second-order least squares estimator","text":"first discuss relationship OLSE SLSE, well advantages SLSE OLSE. OLSE estimator estimate parameter vector \\(\\boldsymbol{\\theta}\\) regression model (2.1), defined \n\\[\\begin{equation*}\n\\boldsymbol{\\hat{\\theta}}:=\\underset{\\boldsymbol{\\theta}}{\\mathrm{argmin}}\\sum_{=1}^n\n(y_i-g(\\boldsymbol{x_i};\\boldsymbol{\\theta}) )^2.\n\\end{equation*}\\]\nassumptions using OLSE : error terms assumed homoscedastic independently identically distributed zero mean finite constant variance. many desired properties consistency BLUE, widely used. practice, however, estimators might outperform OLSE scenarios. error distribution asymmetric, Wang Leblanc (2008) shown SLSE asymptotically efficient OLSE. random errors symmetrically distributed, SLSE OLSE asymptotic efficiency. SLSE caught attentions optimal regression design context due reasons.now review properties SLSE regression model ((2.1). SLSE defined \n\\[\\begin{equation*}\n(\\boldsymbol{\\hat{\\theta}}^\\top,\\hat{\\sigma}^2)^\\top:=\\underset{\\boldsymbol{\\theta},\\sigma^2}{\\mathrm{argmin}}\\sum_{=1}^n\n\\begin{pmatrix}\ny_i-g(\\boldsymbol{x}_i;\\boldsymbol{\\theta})\\\\\ny_i^2-g^2(\\boldsymbol{x}_i;\\boldsymbol{\\theta})-\\sigma^2\n\\end{pmatrix}^\\top\nW(\\boldsymbol{x_i})\n\\begin{pmatrix}\ny_i-g(\\boldsymbol{x}_i;\\boldsymbol{\\theta})\\\\\ny_i^2-g^2(\\boldsymbol{x}_i;\\boldsymbol{\\theta})-\\sigma^2\n\\end{pmatrix}.\n\\end{equation*}\\]\nNote \\(W(\\boldsymbol{x_i})\\) \\(2\\times 2\\) non-negative semi-definite matrix may may depend \\(\\boldsymbol{x_i}\\) (Wang Leblanc 2008). clear SLSE natural extension OLSE defined based first-order difference function (.e. \\(y_i-\\mathbb{E}[y_i]=y_i-g(\\boldsymbol{x}_i;\\boldsymbol{\\theta})\\)). hand, SLSE defined using fist-order difference function, also second-order difference function (.e. \\(y_i^2-\\mathbb{E}[y_i^2]=y_i^2-(g^2(\\boldsymbol{x}_i;\\boldsymbol{\\theta})+\\sigma^2))\\). One might think downsides SLSE talking advantages SLSE OLSE. SLSE disadvantages indeed. linear estimator closed-form solution. requires computational resources compared OLSE due nonlinearity. However, numerical results can easily computed SLSE nowadays. result, SLSE powerful alternative estimator considered research studies real-life applications.","code":""},{"path":"intro.html","id":"research-problem","chapter":"2 Introduction","heading":"2.3 Research problem","text":"introduced previous section, Wang Leblanc (2008) showed SLSE asymptotically efficient OLSE error distribution asymmetric. Optimal designs SLSE proposed Gao Zhou (2014). Bose Mukerjee (2015) Yin Zhou (2017) Gao Zhou (2017) investigations, including convexity analysis, numerical methods, transformation scale invariance properties - D-optimality criteria. commonly used design criteria SLSE studied literature. goal fill gap extending results design criteria, well exploring deriving theoretical results optimal designs SLSE.rest thesis organized follows. Chapter 2 describes detailed formulation optimal regression designs SLSE. derive several analytical results including equivalence theorem number support points optimal designs. Chapter 3 explains use numerical algorithms solve proposed optimal regression design problems via convex programming. also present several interesting applications optimal designs studied thesis. Chapter 4 provides concluding remarks discusses possible future research topics. MATLAB code given Appendix.","code":""},{"path":"intro.html","id":"main-contributions","chapter":"2 Introduction","heading":"2.4 Main Contributions","text":"summary main contributions thesis.studied c-optimality design criterion SLSE, studied .studied c-optimality design criterion SLSE, studied .applied Kiefer’s equivalence theorem (Kiefer 1974) obtain conditions -, c- D-optimal designs SLSE.applied Kiefer’s equivalence theorem (Kiefer 1974) obtain conditions -, c- D-optimal designs SLSE.obtained number support points optimal designs SLSE analytically various regression models.obtained number support points optimal designs SLSE analytically various regression models.studied generalized scale invariance property D-optimal designs SLSE.studied generalized scale invariance property D-optimal designs SLSE.given one efficient effective computing algorithm based program MATLAB computing optimal designs SLSE discrete design spaces.given one efficient effective computing algorithm based program MATLAB computing optimal designs SLSE discrete design spaces.","code":""},{"path":"chapter-SLSE.html","id":"chapter-SLSE","chapter":"3 Optimal Regression Designs Under SLSE","heading":"3 Optimal Regression Designs Under SLSE","text":"chapter, first review results properties optimal regression designs SLSE. derive several new analytical results optimal designs SLSE. begin recalling formulation optimal regression designs SLSE based three different criteria, -, D-, c-optimality. formulation -, D-optimality SLSE first proposed Gao Zhou (2014) -optimality investigated Yin Zhou (2017). formulate optimal design problems -optimality differently properties can extended c-optimality studied yet. Equivalence results verifying optimal designs also obtained. addition, analytical results derived number support points several regression models.","code":""},{"path":"chapter-SLSE.html","id":"design-criteria-under-slse","chapter":"3 Optimal Regression Designs Under SLSE","heading":"3.1 Design criteria under SLSE","text":"Let us introduce notations first. Assume \\(\\sigma_o\\) \\(\\boldsymbol{\\theta}_o\\) true parameter values \\(\\sigma\\) \\(\\boldsymbol{\\theta}\\), respectively. Let \\(S \\subset \\mathbb{R}^p\\) design space \\(\\boldsymbol{x}\\). Let \\(\\operatorname{tr}(\\cdot)\\) \\(\\det(\\cdot)\\) trace determinant functions matrix, respectively. Moreover, let \\(\\Xi\\) denote class probability measures \\(S\\). Define, \\(\\xi\\\\Xi\\),\\[\\begin{equation*}\n\\boldsymbol{g}_1=\\boldsymbol{g}_1(\\xi,\\boldsymbol{\\theta_o})=\\mathbb{E}_{\\xi}\\bigg[ \\frac{\\partial g(\\boldsymbol{x};\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\Big|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_o} \\bigg],\n\\end{equation*}\\]\n\\[\\begin{equation*}\n\\boldsymbol{G}_2=\\boldsymbol{G}_2(\\xi,\\boldsymbol{\\theta}_o)=\\mathbb{E}_{\\xi} \\bigg[ \\frac{\\partial g(\\boldsymbol{x};\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\\frac{\\partial g(\\boldsymbol{x};\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}^\\top}\\Big|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_o}\\bigg].\n\\end{equation*}\\]discrete probability measure \\(\\xi\\\\Xi\\), write \n\\[\n\\xi=\\begin{bmatrix}\n    \\boldsymbol{x}_1&\\boldsymbol{x}_2   &\\ldots &\\boldsymbol{x}_m\\\\\n    p_1     &p_2        &\\ldots &p_m\n\\end{bmatrix},\n\\]\n\\(\\boldsymbol{x}_1,\\boldsymbol{x}_2,\\dots,\\boldsymbol{x}_m\\) support points \\(S\\), \\(p_1,\\dots,p_m\\) probabilities associated points. Gao Zhou (2014), asymptotic variance-covariance matrix \\(\\hat{\\boldsymbol{\\theta}}\\) SLSE given \n\\[\\begin{equation}\n\\boldsymbol{\\mathbb{V}}({\\boldsymbol{\\hat{\\theta}}})=\\sigma^2_o(1-t)(\\boldsymbol{G}_2-t\\boldsymbol{g}_1\\boldsymbol{g}_1^\\top)^{-1},\n\\tag{3.1}\n\\end{equation}\\]\n\\(t=\\frac{\\mu_3^2}{\\sigma_o^2(\\mu_4-\\sigma_o^4)}\\), \\(\\mu_3=\\mathbb{E}[\\epsilon_i^3|\\boldsymbol{x}]\\) \\(\\mu_4=\\mathbb{E}[\\epsilon_i^4|\\boldsymbol{x}]\\). Note Gao Zhou (2014) discussed \\(t \\[0,1)\\) error distributions, \\(t=0\\) symmetric error distributions. Define matrix \\(\\boldsymbol{J}=\\boldsymbol{J}(\\xi,\\boldsymbol{\\theta}_o,t) = \\boldsymbol{G}_2-t\\boldsymbol{g}_1\\boldsymbol{g}_1^T\\). clear matrix \\(\\boldsymbol{J}\\) proportional inverse variance-covariance matrix (3.1). rest thesis, working design problems using matrix \\(\\boldsymbol{J}\\).discussed Chapter 2, aim minimize loss functions optimal design problems, loss functions D-, - c-optimality criteria SLSE can expressed \n\\[\\begin{equation}\n\\begin{aligned}\n  \\phi_D(\\xi,\\boldsymbol{\\theta_o},t)&=&\\det(\\boldsymbol{J}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)),\\\\\n  \\phi_A(\\xi,\\boldsymbol{\\theta_o},t)&=&\\operatorname{tr}(\\boldsymbol{J}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)),\\\\\n  \\phi_c(\\xi,\\boldsymbol{\\theta_o},t)&=&\\boldsymbol{c}_1^\\top\\boldsymbol{J}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)\\boldsymbol{c}_1,\n\\end{aligned}\n\\tag{3.2}\n\\end{equation}\\]\n\\(\\boldsymbol{J}\\) non-singular, \\(\\boldsymbol{c}_1\\) given vector \\(\\mathbb{R}^q\\). \\(\\boldsymbol{J}\\) singular, three loss functions defined \\(+\\infty\\). use \\(\\xi_D^*,~\\xi_A^*\\) \\(\\xi_c^*\\) denote - c-optimal designs, respectively. two measures \\(\\xi_1\\) \\(\\xi_2\\\\Xi\\), define \\(\\xi_{\\alpha}=(1-\\alpha)\\xi_1+\\alpha \\xi_2\\) \\(\\alpha \\[0,1]\\).Lemma 3.1  \\(\\log(\\phi_D(\\xi_{\\alpha},\\boldsymbol{\\theta_o},t))\\), \\(\\phi_A(\\xi_{\\alpha},\\boldsymbol{\\theta_o},t)\\) \\(\\phi_c(\\xi_{\\alpha},\\boldsymbol{\\theta_o},t)\\) convex functions \\(\\alpha\\).convexity results discussed Boyd, Boyd, Vandenberghe (2004) Wong Zhou (2019). Similar convexity results given Bose Mukerjee (2015). use \\(\\log(\\det(\\boldsymbol{J}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)))\\) D-optimal design rest thesis \\(\\log(\\cdot)\\) monotonic increasing function change optimality.Although formulated loss functions, issues associated formulation @ref{eq-loss-J}. reason \\(\\boldsymbol{J}\\) lacking linearity. construction \\(\\boldsymbol{J}\\), \\(\\boldsymbol{J}(\\xi_{\\alpha},\\boldsymbol{\\theta_o},t)\\) linear combination \\(\\boldsymbol{J}(\\xi_{1},\\boldsymbol{\\theta_o},t)\\) \\(\\boldsymbol{J}(\\xi_{2},\\boldsymbol{\\theta_o},t)\\). Thus, difficult obtain theoretical results using \\(\\boldsymbol{J}\\). solve issue, Gao Zhou (2017) proposed alternative expression characterizing loss functions. key define matrix\n\\[\\begin{equation}\n\\boldsymbol{B}(\\xi)=\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta_o},t)=\n\\begin{pmatrix}\n1               &   \\sqrt{t}\\boldsymbol{g_1}^\\top\\\\\n\\sqrt{t}\\boldsymbol{g}_1    &   \\boldsymbol{G}_2\n\\end{pmatrix},\n\\tag{3.3}\n\\end{equation}\\]\nplays important role following formulation. Note \\(\\boldsymbol{B}(\\xi_{\\alpha})\\) now affine function \\(\\alpha\\), .e.,\n\\[ \\boldsymbol{B}(\\xi_{\\alpha})=(1-\\alpha)\\boldsymbol{B}(\\xi_1)+\\alpha\\boldsymbol{B}(\\xi_2).\\]\nfact ultimately makes \\(\\boldsymbol{B}\\) much useful \\(\\boldsymbol{J}\\) study optimal designs SLSE. inverse \\(\\boldsymbol{B}\\) given \n\\[\\begin{equation}\n\\boldsymbol{B}^{-1}(\\xi)=\\boldsymbol{B}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)   =\\begin{pmatrix}\n\\frac{1}{r}             &   \\frac{-\\sqrt{t}}{r}\\boldsymbol{g}_1^\\top\\boldsymbol{G}_2^{-1}   \\\\\n\\frac{-\\sqrt{t}}{r}\\boldsymbol{G}_2^{-1}\\boldsymbol{g}_1    &\\boldsymbol{J}^{-1}\n\\end{pmatrix},\n\\tag{3.4}\n\\end{equation}\\]\n\\(r=1-t\\boldsymbol{g}_1^\\top\\boldsymbol{G}_2^{-1}\\boldsymbol{g}_1\\). Note \\(\\boldsymbol{J}\\) invertible, \\(\\boldsymbol{G}_2\\) must also invertible since \\(\\boldsymbol{G}_2=\\boldsymbol{J}+t\\boldsymbol{g}_1\\boldsymbol{g}_1^\\top\\) \\(t\\boldsymbol{g}_1\\boldsymbol{g}_1^\\top\\) positive semi-definite. Consequently, \\(\\boldsymbol{B}^{-1}\\) exists (3.4). Now, going present following lemmas characterize loss functions -, c- D-optimal design problems. Lemma 3.2 slightly different result Yin Zhou (2017), Lemma 3.3 result Gao Zhou (2017), Lemma 3.4 new result.Lemma 3.2  \\(\\boldsymbol{J}\\) invertible, \n\\[\\begin{equation*}\n  \\phi_A(\\xi,\\boldsymbol{\\theta}_o,t)=\\operatorname{tr}(\\boldsymbol{J}^{-1})=\\operatorname{tr}(\\boldsymbol{C}^\\top\\boldsymbol{B}^{-1}\\boldsymbol{C}),\n  \\end{equation*}\\]\n\\(\\boldsymbol{C}=0\\oplus \\boldsymbol{}_q\\), \\(\\boldsymbol{}_q\\) denotes \\(q\\times q\\) identity matrix, \\(\\oplus\\) denotes matrix direct sum operator.Proof. \\(\\eqref{B inverse}\\) \\(\\boldsymbol{C}=0\\oplus \\boldsymbol{I_q}\\), get\n\\[\\begin{align*}\n    \\boldsymbol{C}^\\top\\boldsymbol{B}^{-1}\\boldsymbol{C}&=  \\begin{pmatrix}\n                                0   &   0\\\\\n                                0   &   \\boldsymbol{}_q\n                                \\end{pmatrix}^T\\begin{pmatrix}\n            \\frac{1}{r}             &   \\frac{-\\sqrt{t}}{r}\\boldsymbol{g_1}^\\top\\boldsymbol{G}_2^{-1}   \\\\\n            \\frac{-\\sqrt{t}}{r}\\boldsymbol{g}_1\\boldsymbol{G}_2^{-1}    &\\boldsymbol{J}^{-1}\n            \\end{pmatrix}\\begin{pmatrix}\n                                0   &   0\\\\\n                                0   &   \\boldsymbol{}_q\n                                \\end{pmatrix}\\\\\n                            &= \\begin{pmatrix}\n                                0   &   0\\\\\n                                0   &   \\boldsymbol{J}^{-1}\n                                \\end{pmatrix},\n    \\end{align*}\\]\nimplies \\(\\operatorname{tr}(\\boldsymbol{C}^\\top\\boldsymbol{B}^{-1}\\boldsymbol{C})=\\operatorname{tr}(\\boldsymbol{J}^{-1})\\).Lemma 3.3  \\(\\boldsymbol{J}\\) invertible, \n\\[\\begin{equation*}\n    \\phi_D(\\xi,\\boldsymbol{\\theta_o},t)=\\det(\\boldsymbol{J}^{-1})=\\det(\\boldsymbol{B}^{-1}).\n\\end{equation*}\\]@ref{eq:B-matrix}, \n\\[\\begin{align*}\n    \\det(\\boldsymbol{B})    &= \\det(1-t\\boldsymbol{g_1}^T\\boldsymbol{G_2}^{-1}\\boldsymbol{g_1})\\det(\\boldsymbol{G_2})\\\\\n            &= \\det(\\boldsymbol{}-t\\boldsymbol{g_1}\\boldsymbol{g_1}^\\top\\boldsymbol{G_2}^{-1}) \\det(\\boldsymbol{G_2}) \\\\\n            &=  \\det(\\boldsymbol{G_2}-t\\boldsymbol{g_1}\\boldsymbol{g_1}^T)\\\\\n            &= \\det(\\boldsymbol{J}),\n    \\end{align*}\\]\ngives \\(\\det(\\boldsymbol{J}^{-1})=\\det(\\boldsymbol{B}^{-1})\\).Lemma 3.4  \\(\\boldsymbol{J}\\) invertible, \n\\[\\begin{equation*}\n    \\phi_c(\\xi,\\boldsymbol{\\theta_o},t)=\\boldsymbol{c}_1\\top\\boldsymbol{J}^{-1}\\boldsymbol{c}_1=\\boldsymbol{c}^\\top\\boldsymbol{B}^{-1}\\boldsymbol{c},\n\\end{equation*}\\]\n\\(\\boldsymbol{c_1}\\) vector \\(\\mathbb{R}^q\\) \\(\\boldsymbol{c}^\\top=(0,\\boldsymbol{c}_1^\\top)\\).Thus, Lemmas \\(\\ref{lemma:loss }\\), \\(\\ref{lemma:loss D}\\) \\(\\ref{lemma:loss c}\\), alternative expressions loss functions \\(\\eqref{loss J}\\) \n\\[\\begin{equation} \\label{loss B}\n\\begin{aligned}\n    \\phi_D(\\xi,\\boldsymbol{\\theta_o},t))&=&\\det(\\boldsymbol{B}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)),\\\\\n    \\phi_A(\\xi,\\boldsymbol{\\theta}_o,t)&=&\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)\\boldsymbol{C}),\\\\\n    \\phi_c(\\xi,\\boldsymbol{\\theta_o},t)&=&\\boldsymbol{c}^T\\boldsymbol{B}^{-1}(\\xi,\\boldsymbol{\\theta_o},t)\\boldsymbol{c},\n\\end{aligned}\n\\end{equation}\\]\n\\(\\boldsymbol{C}= 0 \\oplus \\boldsymbol{I_q}\\), \\(\\boldsymbol{c_1}\\\\mathbb{R}^q\\) \\(\\boldsymbol{c}^T=(0,\\boldsymbol{c_1}^T)\\). \\(\\boldsymbol{B}\\) singular, three loss functions defined \\(+\\infty\\).","code":""},{"path":"chapter-SLSE.html","id":"equivalence-theorem-for-optimal-designs-under-slse","chapter":"3 Optimal Regression Designs Under SLSE","heading":"3.2 Equivalence theorem for optimal designs under SLSE","text":"section derive optimality conditions optimal designs SLSE follows equivalence theorem Kiefer Wolfowitz (1959) Kiefer (1974). also analyze minimum number support points optimal designs various regression models, theoretical results obtained. Note study approximate designs thesis. advantages working approximate designs instead exact design well documented Kiefer Brown (1985).Define vector \\(f(\\boldsymbol{x,\\theta_o})=\\frac{\\partial g(\\boldsymbol{x};\\boldsymbol{\\theta})}{ \\partial \\boldsymbol{\\theta}}\\Big|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta_o}} \\\\mathbb{R}^q\\) matrix\n\\[\\begin{equation}\n\\boldsymbol{M}(\\boldsymbol{x})=\\boldsymbol{M}(\\boldsymbol{x},\\boldsymbol{\\theta_o},t)=\\begin{pmatrix}\n1       &   \\sqrt{t}f^T(\\boldsymbol{x},\\boldsymbol{\\theta_o})\\\\\n\\sqrt{t}f(\\boldsymbol{x},\\boldsymbol{\\theta_o}) &f(\\boldsymbol{x},\\boldsymbol{\\theta_o})f^T(\\boldsymbol{x},\\boldsymbol{\\theta_o})\n\\end{pmatrix}_{(q+1)\\times (q+1)}.\n\\tag{3.5}\n\\end{equation}\\]\\(\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta_o},t)=\\mathbb{E}_{\\xi}[\\boldsymbol{M}(\\boldsymbol{x},\\boldsymbol{\\theta_o},t)]\\). Define dispersion functions\n\\[\\begin{equation} \\label{fuction:dispersion}\n\\begin{aligned}\n  &d_D(\\boldsymbol{x},\\xi,t) = \\operatorname{tr}(\\boldsymbol{B}^{-1}\\boldsymbol{M}(\\boldsymbol{x}))-(q+1),\\\\\n  &d_A(\\boldsymbol{x},\\xi,t) = \\operatorname{tr}(\\boldsymbol{M}(\\boldsymbol{x})\\boldsymbol{B}^{-1}\\boldsymbol{C}^T\\boldsymbol{C}\\boldsymbol{B}^{-1})-\\operatorname{tr}(\\boldsymbol{C}\\boldsymbol{B}^{-1}\\boldsymbol{C}^T),\\\\\n  &d_c(\\boldsymbol{x},\\xi,t) = \\boldsymbol{c}^T\\boldsymbol{B}^{-1}\\boldsymbol{M}(\\boldsymbol{x})\\boldsymbol{B}^{-1}\\boldsymbol{c}-\\boldsymbol{c}^T\\boldsymbol{B}^{-1}\\boldsymbol{c},\\\\\n\\end{aligned}\n\\end{equation}\\]\n\\(\\boldsymbol{B}=\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta_o},t)\\) invertible.Theorem 3.1  suppose dispersion functions evaluated \\(\\boldsymbol{\\theta_o}\\). \\(\\xi_D^*\\), \\(\\xi_A^*\\) \\(\\xi_c^*\\) optimal probability measures D-, - c- optimality, respectively, \\(\\boldsymbol{B}\\) invertible \\(\\boldsymbol{x}\\S\\),\n\\[\\begin{align}\nd_D(\\boldsymbol{x},\\xi_D^*,t) &\\leq 0, \\tag{3.6} \\\\\nd_A(\\boldsymbol{x},\\xi_A^*,t) &\\leq 0, \\tag{3.7} \\\\\nd_c(\\boldsymbol{x},\\xi_c^*,t) &\\leq 0. \\tag{3.8}\n\\end{align}\\]Proof. proof, use \\(\\boldsymbol{B}(\\xi)\\) \\(\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta_o},t)\\) \\(\\boldsymbol{M}(\\boldsymbol{x})\\) \\(\\boldsymbol{M}(\\boldsymbol{x},\\boldsymbol{\\theta_o},t)\\). Suppose \\(\\xi^*\\) optimal design criterion. Define \\(\\xi_\\alpha=(1-\\alpha)\\xi^*+\\alpha \\xi\\) \\(\\xi\\) arbitrary probability measure. proof based Kiefer’s general equivalence theorem , optimal condition can derived \\(\\frac{\\partial \\phi(\\xi_\\alpha)}{\\partial \\alpha}\\big|_{\\alpha=0}\\geq 0\\) measure \\(\\xi \\\\Xi\\), \\(\\phi\\) loss function.first prove (3.6). Let \\(\\xi_D^*\\) optimal measure D-optimality. \n\\[\\begin{align*}\n    \\frac{\\partial \\log(\\phi_D(\\xi_\\alpha))}{\\partial \\alpha}\\Big|_{\\alpha=0}   \n    &=-\\operatorname{tr}(\\boldsymbol{B}^{-1}(\\xi_D^*)(-\\boldsymbol{B}(\\xi_D^*)+\\boldsymbol{B}(\\xi)))\\\\\n    &=-\\operatorname{tr}(\\boldsymbol{B}^{-1}(\\xi_D^*)\\boldsymbol{B}(\\xi))+\\operatorname{tr}(\\boldsymbol{I_{q+1}})\\\\\n    &=-\\operatorname{tr}(\\boldsymbol{B}^{-1}(\\xi_D^*)\\boldsymbol{B}(\\xi))+(q+1))\\\\\n    &=-\\operatorname{tr}(\\boldsymbol{B}^{-1}(\\xi_D^*)\\mathbb{E}_{\\xi}[\\boldsymbol{M}(\\boldsymbol{x})]-(q+1))\\\\\n    &=-\\mathbb{E}_{\\xi}[d_D(\\boldsymbol{x},\\xi_D^*,t)]\\\\\n    &\\geq 0,~\\text{} \\xi \\text{ } S,\n    \\end{align*}\\]\nimplies \\(d_D(\\boldsymbol{x},\\xi_D^*,t)\\leq 0\\), \\(\\boldsymbol{x} \\S\\).prove (3.7), let \\(\\xi_A^*\\) optimal measure -optimality. \n\\[\\begin{align*}\n    \\frac{\\partial \\phi_A(\\xi_\\alpha)}{\\partial \\alpha}\\Big|_{\\alpha=0}\n    &=-\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi_A^*)[\\boldsymbol{B}(\\xi)-\\boldsymbol{B}(\\xi_A^*)]\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{C})\\\\\n    &=-\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{B}(\\xi)\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{C})+\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{C})\\\\\n    &=-\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi_A^*)\\mathbb{E}_{\\xi}[\\boldsymbol{M}(\\boldsymbol{x})]\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{C}) +\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi^*)\\boldsymbol{C})\\\\\n    &= - (\\operatorname{tr}(\\mathbb{E}_{\\xi}[\\boldsymbol{M}_{\\xi}(\\boldsymbol{x},\\boldsymbol{\\theta_o})]\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{C}\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi^*))-\\operatorname{tr}(\\boldsymbol{C}^T\\boldsymbol{B}^{-1}(\\xi_A^*)\\boldsymbol{C}) )\\\\\n    &= -\\mathbb{E}_{\\xi}[d_A(\\boldsymbol{x},\\xi_A^*,t)]\\\\\n    &\\geq 0,~ \\text{} \\xi \\text{ } S,\n    \\end{align*}\\]\nimplies \\(d_A(\\boldsymbol{x},\\xi_A^*,t)\\leq 0\\), \\(\\boldsymbol{x} \\S\\).Lastly, prove (3.6), let \\(\\xi_c^*\\) optimal measure c-optimality. \n\\[\\begin{align*}\n    \\frac{\\partial \\phi_c(\\xi_\\alpha)}{\\partial \\alpha}\\Big|_{\\alpha=0}\n    &=-\\boldsymbol{c}^T\\boldsymbol{B}^{-1}(\\xi_c^*)[-\\boldsymbol{B}(\\xi)+\\boldsymbol{B}(\\xi_c^*)]\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{c}\\\\\n    &=-\\boldsymbol{c}^T\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{B}(\\xi)\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{c}+\\boldsymbol{c}^T\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{B}(\\xi_c^*)\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{c}\\\\\n    &=-\\boldsymbol{c}^T\\boldsymbol{B}^{-1}(\\xi_c^*)\\mathbb{E}_{\\xi}[\\boldsymbol{M}(\\boldsymbol{x})]\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{c}+\\boldsymbol{c}^T\\boldsymbol{B}(\\xi_c^*)^{-1}\\boldsymbol{c}\\\\\n    &=-\\mathbb{E}_{\\xi}[\\boldsymbol{c}^T\\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{M}(\\boldsymbol{x}) \\boldsymbol{B}^{-1}(\\xi_c^*)\\boldsymbol{c}+\\boldsymbol{c}^T\\boldsymbol{B}(\\xi_c^*)^{-1}\\boldsymbol{c}]\\\\\n    &=-\\mathbb{E}_{\\xi}[d_c(\\boldsymbol{x},\\xi_c^*,t)]\\\\\n    &\\geq 0,~ \\text{} \\xi \\text{ } S,\n    \\end{align*}\\]\nimplies \\(d_c(\\boldsymbol{x},\\xi_c^*,t)\\leq 0\\), \\(\\boldsymbol{x} \\S\\).","code":""},{"path":"chapter-SLSE.html","id":"section-support-pt","chapter":"3 Optimal Regression Designs Under SLSE","heading":"3.3 Results on the number of support points","text":"Using results Theorem 3.1 , can explore properties optimal designs. Yin Zhou (2017) Gao Zhou (2017), discussions number support points based computational results. However, little discussion number support points theoretically Gao Zhou (2017), still large gap filled . Hence derive several results number support points various models, including polynomial models, fractional polynomial models, Michaelis-Menton model, Peleg model trigonometric models.polynomial regression model degree \\(q\\) (\\(q \\ge 1\\)) without intercept given \n\\[\\begin{equation}\ny_i=\\theta_1 x_i+\\theta_2 x_i^2+\\cdots+\\theta_q x_i^q+\\epsilon_i,~x_i\\S=[-1,+1],~=1,2,\\cdots,n.\n\\tag{3.9}\n\\end{equation}\\]\nPolynomial regression models widely used response regressors curvilinear relationship. Complex nonlinear relationships can well approximated polynomials small range explanatory variables . different kinds polynomial models orthogonal polynomial models, multi-variable polynomial models, one variable polynomial models. Polynomial models often used design experiment response surface methodology, many applications industry. example, see G. E. P. Box Draper (1987), G. E. Box et al. (1978) Khuri Cornell (1996).- D-optimal designs (3.9) SLSE symmetric \\(S\\) [Yin Zhou (2017),Gao Zhou (2017)}. (3.9), \n\\(\\boldsymbol{f}(x,\\boldsymbol{\\theta})= \\left(x,x^2,\\cdots, x^q \\right)^\\top\\) \n\\[\\begin{equation}\n\\boldsymbol{M}(x,\\boldsymbol{\\theta_o},t)=\\begin{pmatrix}\n1           &\\sqrt{t}x  &\\sqrt{t}x^2    &...&\\sqrt{t}x^q\\\\\n\\sqrt{t}x   &   x^2     &x^3    &\\cdots &x^{q+1}\\\\\n\\vdots      &\\vdots     &\\vdots &\\vdots&    \\vdots\\\\\n\\sqrt{t}x^q &   x^{q+1} &\\dots  &\\dots  &x^{2q}\n\\end{pmatrix}_{(q+1)\\times(q+1)}.\n\\tag{3.10}\n\\end{equation}\\]Theorem 3.2  Let \\(n_A\\) \\(n_D\\) denote minimum number support points - D-optimal designs SLSE, respectively. (3.9), \n\\[\\begin{equation} \\label{support}\nn_A ~ = ~ \\text{q q+1},\n\\end{equation}\\]\n\n\\[\\begin{equation} \\label{support2}\nn_D ~ = ~ \\text{q q+1}.\n\\end{equation}\\]Proof. proof includes following three parts.(). @ref(thm:dispersion} (3.10), can see \\(d_A(x,\\xi_A^*,t)\\) \\(d_D(x,\\xi_D^*,t)\\) polynomial functions \\(x\\) highest degree \\(2q\\). fundamental theorem algebra, exactly \\(2q\\) roots \\(x\\) equations \\(d_A(x,\\xi_A^*,t)=0\\) \\(d_D(x,\\xi_D^*,t)=0\\). However, \\(2q\\) real roots.(ii). construction, determinant \\(\\boldsymbol{B}\\) matrix zero determinant \\(\\boldsymbol{G_2}\\) zero. Therefore, least q support points \\(\\xi\\).(iii). boundary points support points, number support points \\(2q-2\\) interval \\((-1, +1)\\). equivalence theorem, know dispersion functions less equal zero (.e. \\(d_A(x,\\xi_A^*,t)\\leq 0\\) \\(d_D(x,\\xi_D^*,t)\\leq 0\\)), support points \\((-1,+1)\\) multiplicity two. total, \\(2+\\frac{(2q-2)}{2}=q+1\\) distinct support points.Thus, number support points \\(\\xi_A^*\\) \\(\\xi_D^*\\) either \\(q\\) \\(q+1\\).\n\\end{proof}Example 3.1  Consider (3.9) \\(q=2\\) \\(S=[-1,+1]\\). Let \\(\\eta_m=\\mathbb{E}[x^m]\\) \\(m\\)-th moment distribution \\(\\xi(x)\\). Gao Zhou (2014) showed D-optimal design symmetric case (.e. \\(\\eta_1=\\eta_3=0\\)), given \\[\\begin{equation*}\n\\xi_D^* = \\begin{cases}\n    \\begin{bmatrix}\n    -1          &   +1\\\\\n    \\frac{1}{2} &   \\frac{1}{2}\n    \\end{bmatrix},      & \\mbox{$t\\[0,\\frac{2}{3})$}, \\\\\n    \\begin{bmatrix}\n    -1           &  0       &+1\\\\\n    \\frac{1}{3t} &\\frac{3t-2}{3t}           &\\frac{1}{3t}\n    \\end{bmatrix},                                  & \\mbox{$t\\[\\frac{2}{3},1)$}.\\\\\n\\end{cases}\n\\end{equation*}\\]now study -optimal design. taking advantage symmetric result Yin Zhou (2017), \n\\[\n    \\boldsymbol{B}(\\xi)=\n    \\begin{pmatrix}\n    1           &0      &\\sqrt{t}\\eta_2\\\\\n    0           &\\eta_2 &0  \\\\\n    \\sqrt{t}\\eta_2 &    0   &   \\eta_4\n    \\end{pmatrix},~\\text{}\n    \\boldsymbol{B}^{-1}(\\xi)=\\begin{pmatrix}\n        \\frac{\\eta_4}{\\eta_4-t\\eta_2}&0         &\\frac{\\sqrt{t}\\eta_2}{t\\eta_2^2-\\eta_4}\\\\\n        0       &\\frac{1}{\\eta_2}       &0\\\\\n       \\frac{\\sqrt{t}\\eta_2}{t\\eta_2^2-\\eta_4}&0&\\frac{1}{\\eta_4-t\\eta_2^2}\n    \\end{pmatrix}.\n\\]\nDette Studden (1997), \\(S=[-1,+1]\\), even moments distributions must satisfy \\(0\\leq \\eta_2^2 \\leq \\eta_4 \\leq \\eta_2 \\leq 1\\). loss function can expressed \n\\[\n  \\phi_A(\\xi)=\\operatorname{tr}(\\boldsymbol{C}^\\top \\boldsymbol{B}^{-1}\\boldsymbol{C})= \\frac{1}{\\eta_2}+\\frac{1}{\\eta_4-t\\eta_2^2}.\n\\]\noptimal design problem can written \n\\[\\begin{equation*}\n        \\begin{aligned}\n            & \\underset{\\eta_2,\\eta_4}{\\text{min}}\n            & & \\frac{1}{\\eta_2}+\\frac{1}{\\eta_4-t\\eta_2^2} \\\\\n            & \\text{s.t.}\n            & & 0\\leq \\eta_2^2 \\leq \\eta_4 \\leq \\eta_2 \\leq 1.\n        \\end{aligned}\n    \\end{equation*}\\]\norder minimize loss function, first fix \\(\\eta_2\\). trivial see make \\(\\eta_4\\) big possible, leads \\(\\eta_4=\\eta_2\\), ceiling. Now question becomes\n\\[\\begin{equation*}\n        \\begin{aligned}\n            & \\underset{\\eta_2}{\\text{min}}\n            & & \\frac{1}{\\eta_2}+\\frac{1}{\\eta_2-t\\eta_2^2} \\\\\n            & \\text{s.t.}\n            & & 0\\leq  \\eta_2 \\leq 1.\n        \\end{aligned}\n    \\end{equation*}\\]\nyields \\(\\eta_2=\\frac{2-\\sqrt{2}}{t}\\) \\(\\eta_2=\\frac{2+\\sqrt{2}}{t}\\) \\(t\\ne 0\\). Note \\(t\\[0,+1)\\) \\(\\eta_2\\[0,+1]\\), latter solution excluded within feasible region. Consequently,\n\\[\\begin{equation*}\n\\eta_2 = \\begin{cases}\n    \\min{(\\frac{2-\\sqrt{2}}{t},1)},     & \\mbox{$t \\(0,1)$}, \\\\\n    1,                                  & \\mbox{$t = 0$}.\\\\\n\\end{cases}\n\\end{equation*}\\]\nsymmetric property, -optimal design \n\\[  \\xi_A^*=\n    \\begin{bmatrix}\n    -1                  &   0           &   +1\\\\\n    \\frac{\\eta_2}{2}    &   1-\\eta_2    &   \\frac{\\eta_2}{2}\n    \\end{bmatrix}.\n\\]\nthree cases:\\\n\\(t=0\\),\n\\[  \\xi_A^*=\n    \\begin{bmatrix}\n    -1              &   +1\\\\\n    \\frac{1}{2}     &   \\frac{1}{2}\n    \\end{bmatrix}.\n\\]\n\\(t\\(0,2-\\sqrt{2})\\),\n\\[  \\xi_A^*=\n    \\begin{bmatrix}\n    -1              &   +1\\\\\n    \\frac{1}{2}     &   \\frac{1}{2}\n    \\end{bmatrix}.\n\\]\n\\(t\\[2-\\sqrt{2},1)\\),\n\\[  \\xi_A^*=\n    \\begin{bmatrix}\n    -1                  &   0           &   +1\\\\\n    \\frac{2-\\sqrt{2}}{2t}   &   1-\\frac{2-\\sqrt{2}}{t}  &   \\frac{2-\\sqrt{2}}{2t}\n    \\end{bmatrix}.\n\\]\nsummary, -optimal design (3.9) \\(q=2\\) \n\\[\\begin{equation*}\n\\xi_A^* = \\begin{cases}\n    \\begin{bmatrix}\n    -1          &   +1\\\\\n    \\frac{1}{2} &   \\frac{1}{2}\n        \\end{bmatrix},      & \\mbox{$t\\[0, \\leq 2-\\sqrt{2})$}, \\\\\n    \\begin{bmatrix}\n    -1          &   0       &+1\\\\\n    \\frac{2-\\sqrt{2}}{2t}   &\\frac{2t+\\sqrt{2}-2}{t}            &\\frac{2-\\sqrt{2}}{2t}\n\\end{bmatrix},                                  & \\mbox{$t\\[2-\\sqrt{2},1)$}.\\\\\n\\end{cases}\n\\end{equation*}\\]results , clear observe - D-optimal designs either \\(2\\) \\(3\\) support points consistent Theorem 3.2.\n\\(\\Box\\)show result Theorem 3.1, plot \\(d_A(x,\\xi^*,t)\\) \\(d_D(x,\\xi^*,t)\\) Figure [TOFILL] using optimal designs Example 3.1. clear \\(d_A(x,\\xi^*,t) \\leq 0\\) \\(d_D(x,\\xi^*,t)\\leq 0\\) \\(x\\S\\), consistent Theorem \\(\\ref{theorem:dispersion}\\). Also \\(d_A(x,\\xi_A^*,t)=0\\) support points \\(\\xi_A^*\\) \\(d_D(x,\\xi_D^*,t)=0\\) support points \\(\\xi_D^*\\). \\(t\\) increases, origin becomes support point changes number support points 2 3. consistent Theorem 3.2.design space \\(S\\) asymmetric, say \\(S=[-1,b]\\), \\(b\\(-1,1)\\), number support points - D-optimality either \\(q\\) \\(q+1\\) \\(q\\\\mathbb{Z^+}\\). \\(t=0\\), \\(n_D=q\\) \\(q\\\\mathbb{Z^+}\\). derivation similar Theorem 3.2 omitted. show computational results Chapter ??.Fractional polynomial model (FPM) given \n\\[\\begin{equation}\ny=\\theta_1x^{q_1}+\\theta_2x^{q_2}+\\cdots+\\theta_px^{q_p}+\\epsilon,~q_i\\\\mathbb{Q},~\\forall ,\n\\tag{3.11}\n\\end{equation}\\]\nprovides flexible parameterization wide range applications many disciplines. instance, Cui et al. (2009) used FPM longitudinal epidemiological studies, Royston Altman (1995) applied model analyze medical data. model also studied optimal designs. example, Torsney Alahmadi (1995) used FPM study effects concentration \\(x\\) viscosity \\(y\\) model given \n\\[\\begin{equation}\n    y=\\theta_1x+\\theta_2x^{\\frac{1}{2}}+\\theta_3x^2+\\epsilon,~x\\S=(0.0,0.2].\n    \\tag{3.12}\n\\end{equation}\\]\nLet \\(z=x^{1/2}\\), @ref(exm:frac_poly) becomes polynomial model order \\(4\\) new design space \\(S'=(0.0,\\sqrt{0.2}]\\). Now, can apply result polynomial model (3.9) conclude \\(n_A\\) \\(n_D\\) \\(4\\) \\(5\\). Moreover, notice three parameters model \\(\\eqref{ex:frac_poly}\\). order invertible matrix \\(\\boldsymbol{G_2}(\\xi,\\boldsymbol{\\theta}_o)\\) \\(3\\times 3\\) matrix, need least \\(3\\) support points. Thus, number support point - D-optimal design least 3. summary, number support points - D-optimal designs either \\(3\\), \\(4\\), \\(5\\). purpose example illustrating FPM can transformed polynomial model can use result number support points polynomial models.Michaelis-Menton model one best-known models proposed Leonor Michaelis Maud Menten (Michaelis Menten 1913) studies enzyme reactions enzyme substrate concentration. model given \n\\[\\begin{equation}\ny_i=\\frac{\\theta_1x_i}{\\theta_2+x_i}+\\epsilon_i.~ =1,2,\\dots,n,~ x_i \\(0,                                                                k_o],~\\theta_1,\\theta_2\\geq 0.\n\\tag{3.13}\n\\end{equation}\\]\n, \\(\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)^\\top\\). Define \\(z=\\frac{1}{\\theta_2+x}\\). \\(\\boldsymbol{f}(x,\\boldsymbol{\\theta})=(\\frac{x}{\\theta_2+x},\\frac{-\\theta_1x}{(\\theta_2+x)^2})^T= (1-z\\theta_2, -z\\theta_1 +\\theta_1 \\theta_2z^2)^\\top\\), \n\\[\\begin{equation}\n\\boldsymbol{M}(x,\\boldsymbol{\\theta_o},t)=\n\\begin{pmatrix}\n1                       & \\sqrt{t} (1-z\\theta_2)        &   \\sqrt{t}(-\\theta_1 z(1-z\\theta_2))\\\\\n\\sqrt{t} (1-z\\theta_2)  &(1-z\\theta_2)^2            &   -z\\theta_1(1-z\\theta)^2\\\\\n\\sqrt{t}(-\\theta_1 z(1-z\\theta_2))&-z\\theta_1(1-z\\theta)^2&(z\\theta_1(1-z\\theta_2))^2\n\\end{pmatrix}.\n\\end{equation}\\]can clearly see , transformation, \\(z\\S'=[\\frac{1}{\\theta_1+k_o},\\frac{1}{\\theta_1}]\\), since \\(\\boldsymbol{f}(x,\\boldsymbol{\\theta_o})\\) \\(\\boldsymbol{M}(x,\\boldsymbol{\\theta},t)\\) polynomial functions \\(z\\), \\(\\phi_D\\) \\(\\phi_A\\) can described polynomial functions highest degrees \\(4\\). Now, similar discussion model (3.9), can find results \\(n_A\\) \\(n_D\\) model @ref(eq:model-mm} presented following Lemma.Lemma 3.5  Michaelis-Menton model (3.13), number support points D-optimal design -optimal design either \\(2\\) \\(3\\) \\(t\\[0,1)\\). Moreover, 2 support points D-optimality \\(t=0\\).proof Lemma 3.5 similar Theorem \\(\\ref{theorem:support}\\) omitted. easy observe \\(d_D(0,\\xi_D^*,0)<0\\), boundary point, 0, support point D-optimality leads conclusion \\(n_D=2\\). Moreover, \\(n_c\\le 3\\) \\(t\\[0,1)\\).Peleg model statistical model used investigate relationship water absorption various kinds food. model given \n\\[\\begin{equation}\ny_i=y_o+\\frac{x_i}{\\theta_1+\\theta_2 x_i}+\\epsilon_i,\\quad =1,2,\\cdots,n,\n\\tag{3.14}\n\\end{equation}\\]\n\\(y_o\\) represents initial moisture food, \\(y_i\\) current level moisture current time \\(x_i\\), \\(\\theta_1\\) Peleg’s moisture rate constant, \\(\\theta_2\\) asymptotic moisture time increases. Note parameters \\(\\theta_1\\) \\(\\theta_2\\) must positive. Optimal experimental designs studied model using OLSE, example, Paquet-Durand, Zettel, Hitzmann (2015). (3.14), \\(\\boldsymbol{f}(x,\\boldsymbol{\\theta})=(\\frac{-x}{(\\theta_1+\\theta_2x_i)^2},\\frac{-x^2}{(\\theta_1+\\theta_2x)^2})^T\\). let \\(z=\\frac{1}{\\theta_1+\\theta_2x}\\). \\(S=[0,d]\\), new design space transformation \\(S'=[\\frac{1}{\\theta_1+\\theta_2d},\\frac{1}{\\theta_1}]\\). Now, \\(\\boldsymbol{f}(x,\\boldsymbol{\\theta})=(\\frac{-z(1-\\theta_1z)}{\\theta_2},\\frac{-(1-\\theta_1z)^2}{\\theta_2^2})^T\\). Since \\(\\theta_1\\) \\(\\theta_2\\) parameters, \\(\\boldsymbol{f}\\) depends \\(z\\) . Hence, \\(\\boldsymbol{M}(x,\\boldsymbol{\\theta},t)\\) becomes polynomial function \\(z\\) degree \\(4\\). Therefore, number support points, \\(n_A\\) \\(n_D\\) also either 2 3 Peleg model. addition, \\(n_c\\) either 1, 2 3 \\(t\\[0,1)\\).Next consider trigonometric models without intercept. intercept term included, proven optimal designs SLSE OLSE . \\(k\\)th order trigonometric model given \n\\[\\begin{equation}\n    y_i=\\sum_{j=1}^{k}[cos(jx_i)\\theta_{1j}+sin(jx_i)\\theta_{2j}]+\\epsilon_i,~=1,2,\\cdots,n,\n    \\tag{3.15}\n\\end{equation}\\]\n\\(x_i\\S_b=[-b\\pi,b\\pi],~0<b\\leq 1\\). Let \\(\\boldsymbol{\\theta}=(\\theta_{11},\\theta_{12},\\cdots,\\theta_{1k},\\theta_{21},\\cdots,\\theta_{2k})^T\\). (3.5), get\n\\[\\begin{equation}\n\\boldsymbol{M}(x,\\boldsymbol{\\theta_o},t)=\n\\begin{pmatrix}\n1       &\\sqrt{t}cos(x) &   \\dots       &\\sqrt{t}sin(kx)\\\\\n\\sqrt{t}cos(x)  &cos^2(x)   &\\dots  &cos(x)sin(kx)\\\\\n\\vdots  &\\vdots &   \\ddots  &\\vdots\\\\\n\\sqrt{t}sin(kx) &cos(x)sin(kx)&\\dots    &sin^2(kx)\n\\end{pmatrix}_{(2k+1)\\times(2k+1)}.\n\\tag{3.16}\n\\end{equation}\\]\nconsider two cases design space, () \\(b=1\\), full circle (ii) \\(0<b<1\\), partial circle.case (), following Lemma helpful finding number support points (3.15).Lemma 3.6  trigonometric functions, \\(j, u =1,2,\\cdots,k,~u\\ne j\\), \\(\\int_{-\\pi}^{\\pi}cos(jx)dx=\\int_{-\\pi}^{\\pi}sin(jx)dx=0\\),\\(\\int_{-\\pi}^{\\pi}cos^2(jx)dx=\\int_{-\\pi}^{\\pi}sin^2(jx)dx=\\pi\\),\\(\\int_{-\\pi}^{\\pi}cos(jx)cos(ux)dx=\\int_{-\\pi}^{\\pi}sin(jx)sin(ux)dx=0\\),\\(\\int_{-\\pi}^{\\pi}sin(jx)cos(ux)dx=\\int_{-\\pi}^{\\pi}cos(jx)sin(ux)dx=0\\). also holds \\(u=j\\).Theorem 3.3  model (3.15) design space \\(S=[-\\pi,\\pi]\\), uniform distribution \\(S\\) - D-optimal designs.Proof. uniform distribution \\(\\xi^*\\), \\(\\boldsymbol{B}(\\xi^*,\\boldsymbol{\\theta_o},t)=1\\oplus \\frac{1}{2} \\boldsymbol{I_{2k}}\\) Lemma ??. (3.16), obtain\n\\[\\begin{equation*}\n\\begin{aligned}\n&tr(\\boldsymbol{M}(x,\\boldsymbol{\\theta_o},t)\\boldsymbol{B}^{-1}(\\xi^*,\\boldsymbol{\\theta_o},t))-(q+1)\\\\    &=1+2cos^2(x)+2cos^2(2x)+\\dots+2sin^2(kx)-(2k+1)\\\\\n&=1+2[cos^2(x)+sin^2(x)]+\\dots+2[cos^2(kx)+sin^2(kx)]-(2k+1)\\\\\n        &=1+2k-(2k+1)\\\\\n        &=0,\\quad \\text{} x\\S.\n\\end{aligned}\n\\end{equation*}\\]\nimplies \\(d_D(x,\\xi^*,t)=0\\) \\(x\\S\\). Theorem 3.1, \\(\\xi^*\\) D-optimal design.-optimality, easy get\n\\[\\begin{equation*}\n\\begin{aligned}\n&tr(\\boldsymbol{M}(x,\\boldsymbol{\\theta_o},t)\\boldsymbol{B}^{-1}(\\xi^*,\\boldsymbol{\\theta_o},t)\\boldsymbol{C}^T\\boldsymbol{C}\\boldsymbol{B}^{-1}(\\xi^*,\\boldsymbol{\\theta_o},t))-\\operatorname{tr}(\\boldsymbol{C}\\boldsymbol{B}^{-1}(\\xi^*,\\boldsymbol{\\theta_o},t))\\\\&=4cos^2(x)+4cos^2(2x)+\\dots+4sin^2(kx)-4k\\\\\n&=4[cos^2(x)+sin^2(x)]+\\dots+4[cos^2(kx)+sin^2(kx)]-4k\\\\\n&=4k-4k\\\\\n&=0,\\quad \\text{} x\\S,\n\\end{aligned}\n\\end{equation*}\\]\ngives \\(d_A(x,\\xi^*,t)=0\\) \\(x \\S\\). Thus, Theorem 3.1, \\(\\xi^*\\) -optimal design.case (ii), \\(0<b<1\\), partial circle \\(S=[-b\\pi,+b\\pi]\\), let \\(z=cos(x)\\). Now, instead using \\(x\\) directly, study number support point \\(x\\) \\(z\\) \\(S'=[cos(b\\pi),1]\\). Note cosine even function, point \\(z\\S'\\) corresponds two symmetric points around \\(0\\), \\(\\pm x\\S\\). Gao Zhou (2017) discussed elements \\(\\boldsymbol{M}(x,\\boldsymbol{\\theta_o},t)\\) (3.16) can written polynomial functions \\(z\\). Hence, functions \\(d_A(x,\\xi,t)\\), \\(d_D(x,\\xi,t)\\) \\(d_c(x,\\xi,t)\\) polynomial functions \\(z\\) degree \\(2k\\). Using similar arguments polynomial models, -, D- c-optimal designs, terms \\(z\\S'\\), exceed \\(k+1\\) support points.","code":""},{"path":"chapter-SLSE.html","id":"scale-invariance-property-of-d-optimal-design","chapter":"3 Optimal Regression Designs Under SLSE","heading":"3.4 Scale invariance property of D-optimal design","text":"D-optimality one used design criteria due many advantages. One good property criterion invariant scaling independent variables using OLSE (Berger Wong 2009). Recently discussions invariance properties including scale invariance shift invariance optimal designs SLSE. Examples can found Gao Zhou (2014) Yin Zhou (2017). section, focus finding new property scale invariance D-optimal designs SLSE nonlinear regression models.linear regression models, D-optimal designs often scale invariant. hand, model nonlinear, scale invariance property longer available. optimal designs nonlinear models called locally optimal, since depend true parameter vector \\(\\boldsymbol{\\theta_o}\\). Thus, D-optimal designs constructed \\(\\boldsymbol{\\theta_o}\\) \\(S\\). Wong Zhou (2019) proposed generalized scale invariance (GSI) concept studying scale invariance property D-optimal designs nonlinear models generalized linear models. GSI property also useful studying D-optimal designs SLSE designs may constructed scaled design space \\(S^V\\) instead original design space \\(S\\).Denote D-optimal design given model, true parameter vector \\(\\boldsymbol{\\theta}_o\\) design space \\(S\\) \\(\\xi_D^*(S,\\boldsymbol{\\theta}_o)\\). Also, denote scaled design space original design space \\(S\\) \\(S^{V}=\\{\\boldsymbol{V}\\boldsymbol{x}|\\boldsymbol{x}\\S\\}\\).Definition 3.1  matrix \\(\\boldsymbol{V}\\) called scale matrix diagonal matrix defined \\(\\boldsymbol{V}=diag(v_1,v_2,\\cdots,v_p)\\), \\(v_i\\) positive.Definition 3.2  Transforming \\(\\boldsymbol{x}\\) \\(\\boldsymbol{V} \\boldsymbol{x}\\) called scale transformation.Definition 3.3  \\(\\xi_D^*(S,\\boldsymbol{\\theta}_o)\\) said scale invariant model exists parameter vector \\(\\tilde{\\boldsymbol{\\theta_o}}\\) D-optimal design \\(\\xi_D^*(S^V,\\tilde{\\boldsymbol{\\theta_o}})\\) can obtained \\(\\xi_D^*(S,\\boldsymbol{\\theta_o})\\) using scale transformations.property design, \\(\\xi_D^*(S,\\boldsymbol{\\theta_o})\\), defined generalized scale invariance. Note \\(\\tilde{\\boldsymbol{\\theta_o}}\\) \\(\\boldsymbol{\\theta_o}\\) closed related elements \\(v_1,\\cdots,v_p\\) scale matrix \\(\\boldsymbol{V}\\). model linear, GSI \\(\\xi_D^*(S,\\boldsymbol{\\theta_o})\\) becomes traditional scale invariance since depend \\(\\boldsymbol{\\theta_o}\\). many nonlinear regression models, GSI property holds D-optimal designs OLSE SLSE. following lemma provides condition check property.Lemma 3.7  given model scale matrix \\(\\boldsymbol{V}\\), exists parameter vector \\(\\tilde{\\boldsymbol{\\theta}_o}\\) invertible diagonal matrix \\(\\boldsymbol{K}\\) depend \\(\\boldsymbol{x}\\), \\(\\boldsymbol{f}(\\boldsymbol{V}\\boldsymbol{x},\\tilde{\\boldsymbol{\\theta}_o})=\\boldsymbol{K}\\boldsymbol{f}(\\boldsymbol{x},\\boldsymbol{\\theta}_o)\\) \\(\\boldsymbol{x}\\S\\), design \\(\\xi_D^*(S,\\boldsymbol{\\theta}_o)\\) generalized scale invariance property.\n\\end{lemma}Proof. design space \\(S\\) parameter vector \\(\\boldsymbol{\\theta_o}\\), \\(\\xi_D^*(S,\\boldsymbol{\\theta_o})\\) minimizes \\(\\phi_D^*(\\xi,\\boldsymbol{\\theta_o})=\\det(\\boldsymbol{B}^{-1}(\\xi,\\boldsymbol{\\theta_o}))\\) \\(\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta}_o)=\\mathbb{E}[\\boldsymbol{M}(\\boldsymbol{x},\\boldsymbol{\\theta}_o)]\\) \\(\\boldsymbol{M}(\\boldsymbol{x},\\boldsymbol{\\theta}_o)\\) given \\(\\eqref{M matrix}\\), expectation taken respect \\(\\xi(\\boldsymbol{x})\\) \\(S\\).design space \\(S^V=\\{\\boldsymbol{V} \\boldsymbol{x}|\\boldsymbol{x}\\S\\}\\) parameter vector \\(\\tilde{\\boldsymbol{\\theta_o}}\\), minimize following function find \\(\\xi_D^*(S^V,\\tilde{\\boldsymbol{\\theta}_o})\\), \\(\\phi_D^*(\\xi,\\tilde{\\boldsymbol{\\theta}_o})=\\det(\\boldsymbol{B}^{-1}(\\xi,\\tilde{\\boldsymbol{\\theta}_o}))\\), \\(\\boldsymbol{B}(\\xi,\\tilde{\\boldsymbol{\\theta}_o})=\\mathbb{E}[\\boldsymbol{M}(\\boldsymbol{z},\\tilde{\\boldsymbol{\\theta}_o})]\\), expectation taken respect \\(\\xi(\\boldsymbol{z})\\) \\(S^V\\). \\(\\boldsymbol{z}\\S^V\\) can written \\(\\boldsymbol{V}\\boldsymbol{x}\\) followed definition, \\(\\boldsymbol{x}\\S\\). Thus, \\(\\boldsymbol{M}(\\boldsymbol{z},\\tilde{\\boldsymbol{\\theta}_o})=\\boldsymbol{M}(\\boldsymbol{V}\\boldsymbol{x},\\tilde{\\boldsymbol{\\theta}_o})\\).assumption Lemma 3.7 (3.5), get \\[\n\\boldsymbol{M}(\\boldsymbol{V}\\boldsymbol{x},\\tilde{\\boldsymbol{\\theta}_o})=(1 \\oplus \\boldsymbol{K}) \\boldsymbol{M}(\\boldsymbol{x},\\boldsymbol{\\theta}_o)(1 \\oplus \\boldsymbol{K}),~~~\\boldsymbol{x}\\S.\n\\]\nTherefore, \\(\\boldsymbol{B}(\\xi,\\tilde{\\boldsymbol{\\theta}_o})= (1 \\oplus \\boldsymbol{K}) \\boldsymbol{B}(\\xi,\\boldsymbol{\\theta}_o)(1 \\oplus \\boldsymbol{K})\\) \\(\\phi_D(\\xi,\\tilde{\\boldsymbol{\\theta}_o})=\\frac{\\phi_D(\\xi,\\boldsymbol{\\theta}_o)}{\\det^2(\\boldsymbol{K})}\\). implies can minimize \\(\\phi_D(\\xi,\\boldsymbol{\\theta}_o)\\) obtain \\(\\xi_D^*(S^V,\\tilde{\\boldsymbol{\\theta}_o})\\), \\(\\xi_D^*(S^V,\\tilde{\\boldsymbol{\\theta}_o})\\) scale transformation \\(\\xi_D^*(S,\\boldsymbol{\\theta}_o)\\).Example 3.2  Consider Peleg regression model (3.14). model, since \\(p=1\\), scale matrix \\(V=v_1>0\\) scalar \\(\\boldsymbol{\\theta}_o=(,b)^\\top\\). Now, let \\(\\tilde{\\boldsymbol{\\theta}_o}=(,\\frac{b}{v_1})^\\top\\) can obtain\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol{f}(\\boldsymbol{V}\\boldsymbol{x},\\tilde{\\boldsymbol{\\theta}_o})  \n        &= \\boldsymbol{f}(v_1x,\\tilde{\\boldsymbol{\\theta}_o})\\\\\n        &= \\begin{pmatrix}\\frac{-v_1x}{(+\\frac{b}{v_1}v_1x)^2}, & \\frac{-(v_1x)^2}{(+\\frac{b}{v_1}v_1x)^2}\\end{pmatrix}^\\top\\\\\n        &=\\begin{pmatrix} v_1\\frac{-x}{(+bx)^2}, & v_1^2\\frac{-x^2}{(+bx)^2}\\end{pmatrix}^\\top\\\\\n        &= \\begin{pmatrix} v_1  &0\\\\0&v_1^2\\end{pmatrix}\\begin{pmatrix} \\frac{-x}{(+bx)^2}, & \\frac{-(v_1x)^2}{(+bx)^2}\\end{pmatrix}^\\top\\\\\n        &=\\boldsymbol{K}\\boldsymbol{f}(\\boldsymbol{x},\\boldsymbol{\\theta_o}).\n\\end{aligned}\n\\end{equation*}\\]\nHence, Peleg model GSI property based Lemma 3.7 choosing \\(\\boldsymbol{K}=diag(v_1,v_1^2)\\) \\(\\tilde{\\boldsymbol{\\theta_o}}=(,\\frac{b}{v_1})^T\\). \\(\\Box\\)worth noting matrix \\(\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta}_o,t)\\) ill-conditioned (.e. condition number matrix large), numerical algorithm computing \\(\\boldsymbol{B}^{-1}(\\xi,\\boldsymbol{\\theta}_o,t)\\) may fail numerical inverse inaccurate imprecise. situations, GSI property may helpful. provide one example demonstrate usefulness GSI property.Example 3.3  Piecewise polynomial regression using knots frequently used various applications. See Dette, Melas, Pepelyshev (2008) references therein. investigated optimal designs OLSE piecewise polynomial regression model unknown knots, obtained results number support points D-optimality various properties designs. consider one model used, cubic spline regression model unknown knots, given \n\\[\\begin{equation}\ny=\\theta_1+\\theta_2x+\\theta_3x^2+\\theta_4x^3+\\theta_5(x-\\lambda)_{+}^3+\\epsilon,~x\\[0,b]\n\\tag{3.17}\n\\end{equation}\\]\n\\((x-\\lambda)_{+} = \\max(0,(x-\\lambda))\\). Model \\(\\eqref{model:spline_regression}\\) nonlinear parameter vector \\(\\boldsymbol{\\theta}_o=(\\theta_1,\\cdots,\\theta_5,\\lambda)^T\\), \\(\\boldsymbol{f}(x,\\boldsymbol{\\theta}_o)=(1,x,x^2,x^3,(x-\\lambda)_+^3,-3\\theta_5(x-\\lambda)_+^2)^T\\). now start illustrate GSI property model. Consider scale matrix \\(\\boldsymbol{V}=v_1>0\\) let \\(\\tilde{\\boldsymbol{\\theta}_o}=(\\theta_1,\\cdots,\\theta_5,v_1\\lambda)^T\\). , \\(x\\[0,b]\\),\\[\\begin{equation*}\n\\begin{aligned}\n\\boldsymbol{f}(\\boldsymbol{V}\\boldsymbol{x},\\tilde{\\boldsymbol{\\theta_o}})  \n        &= \\boldsymbol{f}(v_1x,\\tilde{\\boldsymbol{\\theta_o}})\\\\\n        &= (1, v_1x,  (v_1x)^2,  (v_1x)^3,  (v_1x-v_1\\lambda)_+^3,  -3\\theta_5(v_1x-v_1\\lambda)_+^2)^T\\\\\n        &= (1,  v_1,x,  (v_1x)^2,  (v_1x)^3,  v_1^3(x-\\lambda)_+^3,  -3\\theta_5v_1^2(x-\\lambda)_+^2 )^T\\\\\n        &= diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2) (1, x,  x^2, x^3, (x-\\lambda)_+^3, -3\\theta_5(x-\\lambda)_+^2)^T\\\\\n        &=\\boldsymbol{K}\\boldsymbol{f}(\\boldsymbol{x},\\boldsymbol{\\theta_o}),\\quad \\text{} \\boldsymbol{K}=diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2).\n\\end{aligned}\n\\end{equation*}\\]\nHence, Lemma 3.7, D-optimal design SLSE GSI property. Moreover, model linear \\(\\boldsymbol{\\theta}_o'=(\\theta_1,\\theta_2,\\cdots,\\theta_5)^\\top\\), D-optimal designs OLSE SLSE depend \\(\\boldsymbol{\\theta}_o'\\) Yin Zhou (2017). Thus, \\(S=[0,b]\\) \\(S^{V}=[0,v_1b]\\), D-optimal designs \\(\\xi_D^*(S,\\boldsymbol{\\theta}_o)\\) \\(\\xi_D^*(S^V,\\tilde{\\boldsymbol{\\theta}_o})\\) can obtained using corresponding \\(\\boldsymbol{\\theta}_o\\) \\(\\tilde{\\boldsymbol{\\theta}}\\). property can help us dramatically finding D-optimal designs matrix \\(\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta}_o,t)\\) ill-conditioned. Numerical results D-optimal designs presented Example \\(\\ref{example:spline}\\) discuss numerical algorithms Chapter ??. \\(\\Box\\)also want note model (3.17), intercept included D-optimal designs OLSE SLSE (Gao Zhou 2014). Moreover, result may easily extended piecewise regression models multiple unknown knots.","code":""},{"path":"chapter-applications.html","id":"chapter-applications","chapter":"4 Numerical Algorithm and Applications","heading":"4 Numerical Algorithm and Applications","text":"studied optimality conditions number support points optimal designs SLSE Chapter 3. However, still challenging find optimal designs analytically, except simple regression models. Often numerical algorithms used compute optimal designs, various algorithms studied including Titterington (1978), S. D. Silvey, Titterington, Torsney (1978), Dette, Pepelyshev, Zhigljavsky (2008), Torsney Martín-Martín (2009). optimal designs SLSE, Bose Mukerjee (2015) used multiplicative algorithms finding optimal designs whereas Gao Zhou (2017) used convex programming semi-definite programming (SDP) MATLAB. want extend computational strategies finding optimal designs SLSE. chapter, discuss effective algorithm compute optimal designs based CVX programming MATLAB.","code":""},{"path":"chapter-applications.html","id":"optimization-problem","chapter":"4 Numerical Algorithm and Applications","heading":"4.1 Optimization problem","text":"first discretize design space \\(S \\subset \\mathbb{R}^p\\), discretized space denoted \\(S_N\\subset S\\), \\(N\\) number points \\(S_N\\). magnitude \\(N\\) can large, often designs highly efficient already moderate \\(N\\). discussions choice \\(N\\) given examples chapter discretization can done many different ways, discretize design space using equally spaced gird points sake simplicity. one dimensional design space, say \\(S=[,b]\\), design points \\(+(-1)\\frac{(b-)}{N},=1,2,...,N\\). higher dimensional design spaces, use equally spaced grid points variable, \\(S_N\\) formed Cartesian product.Suppose \\(\\xi^*\\) optimal probability measure \\(\\Xi_N\\), \\(\\Xi_N\\) includes distributions \\(S_N\\). denote \\(S_N=\\{\\boldsymbol{u}_1,\\boldsymbol{u}_2,...,\\boldsymbol{u}_N\\}\\subset S\\). probability measure \\(\\xi \\\\Xi_N\\) can express \n\\[\n    \\xi=\\begin{bmatrix}\n    \\boldsymbol{u}_1    &\\boldsymbol{u}_2   &...&\\boldsymbol{u}_N\\\\\n    w_1     &w_2    &...&w_N\n    \\end{bmatrix},\n\\]\\(w_i\\) weight \\(\\boldsymbol{u_i}\\), non-negative \\(\\sum_{=1}^Nw_i=1\\). optimal regression design problem can expressed \n\\[\\begin{equation}\n\\begin{aligned}\n& \\underset{w_1, ..., w_N, \\delta}{\\text{min}}\n& &  \\delta \\\\\n& \\text{s.t.}\n& & \\sum_{=1}^N w_i=1,\\\\\n& & & -w_i\\leq 0,~ \\text{}= 1,2, ..., N,\\\\\n& & & \\phi \\leq \\delta,\n\\end{aligned}\n\\tag{4.1}\n\\end{equation}\\]\n\\(\\phi\\) loss function, can \\(\\phi_A\\), \\(\\log(\\phi_D)\\) \\(\\phi_c\\) defined \\(\\eqref{eq:loss-B}\\). Notice \\(\\xi \\\\Xi_N\\), \\(\\boldsymbol{B}(\\xi,\\boldsymbol{\\theta}_o,t)=\\mathbb{E}_{\\xi}[\\boldsymbol{M(\\boldsymbol{x},\\boldsymbol{\\theta}_o},t)]=\\sum_{=1}^N \\boldsymbol{M}(\\boldsymbol{u}_i,\\boldsymbol{\\theta}_o,t)w_i\\), \\(\\boldsymbol{M}(\\boldsymbol{u_i},\\boldsymbol{\\theta}_o,t)\\) defined (3.5). also note design point selected weight greater small number \\(\\delta_1\\) (small positive number, say \\(10^{-5}\\)) practice. Lastly, report \\(d_D^{\\max}=\\max\\{d_D |\\boldsymbol{x}\\S\\}\\) (\\(d_A^{\\max},~d_c^{\\max}\\)) D-optimality (-, c-optimality), \\(d_D,d_A\\) \\(d_c\\) defined (??). \\(d_D^{\\max}<\\delta_2\\) (small positive number, say \\(10^{-4}\\)), numerical solution optimization problem D-optimal design Theorem 3.1. Similarly can check - c-optimal designs. Problem (4.1) convex optimization problem. discuss algorithm solve (4.1) next section.","code":""},{"path":"chapter-applications.html","id":"section-algorithm","chapter":"4 Numerical Algorithm and Applications","heading":"4.2 Computational Algorithm","text":"numerical algorithms studied SLSE. Bose Mukerjee (2015) applied multiplicative algorithms finding optimal designs binary design points. Gao Zhou (2017) computed D-optimal designs using moments distribution \\(\\xi \\\\Xi\\). Yin Zhou (2017) formulated design problems convex optimization problem applied convex optimization programs CVX SeDuMi MATLAB find D- -optimal designs, respectively. work, -optimal design problems solved SeDuMi hard write code MATLAB. newly proposed formulation (4.1), able use CVX find -optimal designs, much easier code. Moreover, CVX programming can also applied find c-optimal designs, previous algorithms applied .Figure \\(\\ref{Flow chart}\\) illustrates simple algorithm six steps apply CVX finding optimal designs. Users input variables, partition design space \\(N\\) grid points, compute \\(\\boldsymbol{M}\\) matrix first three steps. Steps (), (ii) (iii) depend regression model design space. Step (iv) solves optimal design problem CVX program, depends optimality criterion. details using CVX MATLAB code can found Appendix \\(\\ref{appendix:algorithm}\\) Appendix \\(\\ref{appendix:matlab}\\). present several examples using algorithm show optimal designs Section 4.3.","code":""},{"path":"chapter-applications.html","id":"section:applications","chapter":"4 Numerical Algorithm and Applications","heading":"4.3 Applications","text":"compute -, D- c-optimal designs various regression models. Notice intercept term presents, resulting optimal designs OLSE SLSE Gao Zhou (2014). Since many studies optimal regression designs OLSE literature already, consider models without intercept. examples computed via CVX package MATLAB. provide results different values \\(t\\), \\(t\\) related measure skewness error distribution. related MATLAB code can found Appendix (appendix:matlab?).examples thesis computed PC Intel Core I5 6500 4 cores CPU, 3.20 GHz. MATLAB version R2018a academic student version CVX version 2.1 build number 1123. RAM platform particular machine 16 Gigabyte Windows 10 Professional version, respectively.Example 4.1  example Gompertz growth model briefly described Chapter ?? showcase output. , use model show effects N D-optimal designs SLSE \\(\\boldsymbol{\\theta_o}=(1,1,1)^\\top\\), \\(S=[0.0,10.0]\\) \\(t=0\\).Table \\(\\ref{table:gompertz1}\\) gives D-optimal designs various values \\(N\\). Note D-optimal design Figure \\(\\ref{fig:gompertz}\\) Chapter \\(\\ref{chapter:introduction}\\) \\(N=2001\\). can see optimal design changes \\(N\\) increases. However, \\(N\\ge 1001\\), optimal designs change much converge distribution three support points equal weights. also show change loss function \\(N\\) changes Figure \\(\\ref{fig:gom_loss}\\). \\(N\\rightarrow +\\infty\\), loss function converges . example, moderate \\(N\\) already gives highly efficient design shown Figure \\(\\ref{fig:gom_loss}\\). Similar results obtained - c-optimal designs, representative results given Table \\(\\ref{table:gompertz2}\\), \\(\\boldsymbol{c_1}=(2,0.5,1)^T\\) used c-optimal designs, \\(t= 0.0, 0.3, 0.5, 0.7\\) \\(0.9\\).clear \\(d_A^{\\max}=\\max\\limits_{x}(d_A(x,\\xi_A^*,t)\\) (\\(d_c(x,\\xi_c^*,t)\\)) small, satisfies optimality conditions Theorem \\(\\ref{theorem:dispersion}\\). also point situations two numerical support points close , probably representing one theoretical support point two numerical support points. instance, \\(t=0.0\\), two support points \\(1.315\\) \\(1.320\\) close -optimal design. discretize design space points (.e. choose larger \\(N\\)), can find particular point. instance, -optimal design \\(N=9001\\) \\[\n    \\xi_A^*=\\begin{bmatrix}\n                0.000   &   1.318   &   10.000\\\\\n                0.354   &   0.385   &   0.261\n    \\end{bmatrix},\n\\]\nthree support points \\(1.318\\) indeed \\(1.315\\) \\(1.320\\).\ncomputational time changes 1.65 seconds 86.16 seconds \\(N=51\\) \\(N=20001\\), increases quite bit computational time still less two minutes.\n\\(\\Box\\)Example 4.2  consider polynomial regression model \\(\\eqref{model:poly}\\) \\(q=5\\). \\(\\mathbf{f}(x;\\mathbf{\\theta}) = (x,x^2,... ,x^q)^\\top\\) depend \\(\\mathbf{\\theta}\\). - D-optimal designs shown Figure \\(\\ref{fig:poly_5}\\). \\(N=2001\\), can see number support points always six model. \\(q\\) even, number support points D- -optimal designs equals \\(q\\) small \\(t\\), equals \\(q+1\\) larger \\(t\\). consistent Theorem \\(\\ref{theorem:support}\\).","code":""},{"path":"chapter:discussions.html","id":"chapter:discussions","chapter":"5 Discussions","heading":"5 Discussions","text":"reviewed current development optimal regression designs SLSE derived theoretical properties optimal designs. can characterize loss function -optimal design convex optimization framework differently, result can extended c-optimality criterion SLSE. also derived optimality conditions based equivalence results -, D- c-optimality criteria. Furthermore studied obtained number support points several regression models SLSE analytically.nonlinear models, optimal designs often depend value parameter vector \\(\\boldsymbol{\\theta}_o\\), difficult solve optimal regression design problems analytically. investigated CVX based numerical algorithm compute optimal designs using properties obtained. Using approximate design approach discretizing design space user-specified number gird points, can find optimal designs efficiently effectively CVX based algorithm. Several applications given thesis showcasing effectiveness algorithm.Generalized scale invariance property D-optimality useful property also studied thesis. can applied linear nonlinear regression models. property can helpful many situations. instance, matrix \\(\\boldsymbol{B}\\) ill-conditioned, numerical algorithm may fail. may able use property rescale design space find D-optimal design scaled design space. scaling helpful avoid computing issues related ill-conditioned matrix.Although algorithm effective efficient models presented thesis, still challenges construct optimal designs complicated models. discuss .dimension design space \\(S\\) high (.e. many design variables), instance, \\(10\\) \\(x's\\) mixture experiment, algorithm may unstable. Sometimes, since keep point weight \\(10^{-4}\\) using numerical algorithm, can many points small positive weights. points may significant optimal designs. Also, difficult partition high dimensional space tiny gird points, computation time increases exponentially \\(N\\) increases.matrix \\(\\boldsymbol{B}\\) ill-conditioned, numerical algorithm fail find optimal designs precisely. D-optimal designs, discussed models, may use generalized scale invariant properties rescale ill-conditioned matrix obtain optimal designs. However, models D-optimality property. Furthermore, design criteria, - c-, generally property. Therefore, investigations required address issue future.Although extended theoretical properties SLSE c-optimality criterion studied , still many design criteria SLSE, E-, G- minimax criteria studied.","code":""},{"path":"final-words.html","id":"final-words","chapter":"6 Final Words","heading":"6 Final Words","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
