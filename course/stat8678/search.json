[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Description",
    "text": "Description\nThis course covers programming using the SAS statistical software package, and it provides an introduction to data analysis stressing the implementation using SAS.\nTopics include two main parts:\n\nSAS Programming: data management and manipulation, basic procedures, macro programming;\nData Analysis: descriptive statistical analysis, one- and two-sample inference, basic categorical data analysis, regression analysis, and other selected topics.\n\n\nPrerequisites\nMATH 4544/6544 ‚Äì Biostatistics, or equivalent.\n\n\nInstructor\nChi-Kuang Yeh, Assistant Professor in the Department of Mathematics and Statistics, Georgia State University.\n\nOffice: Suite 1407, 25 Park Place.\nEmail: cyeh@gsu.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#office-hour",
    "href": "index.html#office-hour",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Office Hour",
    "text": "Office Hour\n10:00‚Äì13:00 on Monday, or by appointment.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#grade-distribution",
    "href": "index.html#grade-distribution",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\nAssignments: 60%\nExam: 20%\nProject: 20%",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assignment",
    "href": "index.html#assignment",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Assignment",
    "text": "Assignment\n\nA1, due on Jan 28, 2026\nA2, due on Feb 8, 2026\nA3, due on Feb 15, 2026\nA4, TBD",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#midterm",
    "href": "index.html#midterm",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Midterm",
    "text": "Midterm\n\nMarch 4, 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-and-corresponding-lectures",
    "href": "index.html#topics-and-corresponding-lectures",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Topics and Corresponding Lectures",
    "text": "Topics and Corresponding Lectures\nThose chapters are based on the lecture notes. This part will be updated frequently.\n\n\n\nStatus\nTopic\nLecture\n\n\n\n\n‚úÖ\nWelcome and Overview\n1\n\n\n\nüìò Part 1: Basics\n\n\n\n‚úÖ\nBasic SAS Operation\n2\n\n\n‚úÖ\nSAS Syntax\n3\n\n\n‚úÖ\nImport and Export Data\n4\n\n\n‚úÖ\nRandom Variable\n5\n\n\n\nüìò Part 2: Statistical Analysis\n\n\n\n\nOne Sample Problem as Example\n\n\n\n‚úÖ\nIntroduction to Statistical Inference I\n6\n\n\n‚úÖ\nIntroduction to Statistical Inference II\n7\n\n\n‚úÖ\nOne Sample Nonparametric Test\n8\n\n\n‚úÖ\nOne Sample Proportion Test\n9\n\n\n‚úÖ\nIntroduction to SAS Marco\n10\n\n\n‚è≥\nChi-Square GoF test\n11\n\n\n‚è≥\nPower Analysis and Calculation\n12\n\n\n‚è≥\nTBA\n13\n\n\n‚è≥\nExam\n14",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-textbooks",
    "href": "index.html#recommended-textbooks",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Recommended Textbooks",
    "text": "Recommended Textbooks\n\nStatistics 480: Introduction to SAS, The Pennsylvania State University.\nSAS Training, SAS Institute.\nSAS Resources, University of California, Los Angeles.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "STAT 8678 - SAS Programming & Data Analysis",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nSpecial thanks to Li-Hsiang Lin for providing the base materials given on this website.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html",
    "href": "part1_1_intro_operation.html",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "",
    "text": "1.1 Introduction to SAS\nQ: What is SAS?\nSAS (Statistical Analysis Software) is a prominent tool in the field of Data Analytics, offering a comprehensive suite for data manipulation, mining, management, and retrieval across various sources, coupled with robust statistical analysis capabilities. It excels in a range of functions including data management, statistical analysis, report generation, business modelling, application development, and data warehousing. SAS is user-friendly, featuring a point-and-click interface for those without technical expertise, while also providing deeper functionality through the SAS programming language. This software is instrumental in employing qualitative methods and processes that enhance employee productivity and business profitability.\nWithin SAS, data extraction and categorization into tables are pivotal for identifying and understanding data trends. This versatile suite supports advanced analytics, business intelligence, predictive analysis, and data management, facilitating effective operation in dynamic and compet- itive business environments. Additionally, SAS‚Äôs platform-independent nature allows it to operate seamlessly across various operating systems, including Linux, Windows, Mac, and Ubuntu. SAS provides extensive support to programmatically transform and analyze data in the comparison of drag and drop interface of other Business Intelligence tools. It provides very fine control over data manipulation and analysis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#introduction-to-sas",
    "href": "part1_1_intro_operation.html#introduction-to-sas",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "",
    "text": "1.1.1 SAS Installzation\nGeorgia State University (GSU) has purchased license, so we can access SAS University Edition for free!\nTo install SAS University Edition, choose from the following options:\n\nOption 1:\nDownload on your personal PC: Free SAS license available to GSU students, faculty, and staff via Technology Services (download required; check system requirements): Download from https://technology.gsu.edu/technology-services/software-equipment/university-licensed-software/ (Need to log-in from your GSU Account)\n\n\nGet Help for the Installation from &lt;https://gsutech.service-now.com/sp&gt;\n\nOption 2:\n\nOn Campus Access: SAS can be found on all GSU Library PCs: Floors 1-4 (not available on Library Macs, because there is no Mac version of SAS)\nGraduate Biostatistics Computer Lab (SPH): 6th floor of the Urban Life building (swipe card access required)\nCommon MILE Lab whose opening time is\n\nMonday & Wednesday: 9 ‚Äì 18\nTuesday & Thursday: 9 ‚Äì 17\nFriday: 9 ‚Äì 15\n\n\nOption 3:\nAccess via VLab, GSU‚Äôs Remote Desktop Environment. Download and Connect to Cisco AnyConnect Client to connect to GSU‚Äôs VPN (secureaccess.gsu.edu). Once connected to the VPN, login to VLab at: https://vlab.gsu.edu/ to access SAS.\nOption 4:\nAccess via SAS OnDemand for Academics/SAS Studio. If you do not already have one, create a SAS profile at https://welcome.oda.sas.com/ Then, sign in with credentials and click SAS¬ÆStudio to access the web-based SAS environment.\n\n\n\n1.1.2 SAS Windows\nOnce SAS has started, the screen will look similar to the following: The main SAS window is divided into several sub-windows:\n\n\nThe menu and toolbar along the top of the window\nThe explorer/results browser along the left hand side, where you can a listing of the results of successful SAS program.\nThe log to the top right. This gives you information about possible errors after you have run your SAS program.\nThe program editor below the log on the bottom right, where you create your SAS program.\nThe windows bar along the bottom for you to switch all windows.\n\nThe Editor (Program Editor) window is a text editor that facilitates writing SAS programs (code). The Log window displays system messages, errors, and resource usage and is thus used to review program statements. The Output window displays output from statistical procedures run within the SAS program; however this is no longer the default. In SAS 9.3 output is sent to the Results Viewer which opens automatically when you run a procedure that generates output. The Results window displays a map of the Output window, and is useful for navigating the results of complicated analyses. Finally, the Explorer window contains all of the data sets in the current SAS session.\nThese windows can be moved or resized as desired. Only one SAS window is active at a time. The active window will have a shaded title bar at the top of the window, and a highlighted windows bar at the bottom of the screen. In the above example, the Program Editor is the active window, with an ‚ÄùUntitled‚Äù program name. Note that the menu options for the SAS toolbar along the top of the screen depend on which window is currently active. (The active window can be changed by clicking on that window with the mouse, or by selecting the desired window from the Window menu.)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#sas-program",
    "href": "part1_1_intro_operation.html#sas-program",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "1.2 SAS Program",
    "text": "1.2 SAS Program\nThe programming structure of SAS consists of three significant steps:\n\nDATA step: create and modify a SAS data set for follow-up analysis\nPROC step: conduct data analysis\nOUTPUT step: show the analysis results\n\n\n*Syntax of the SAS program:;\nDATA dataset name; /* Name of the data set. */\nINPUT var1,var2; /* Defines the variables in this data set. */\nNEW_VAR; /* Creates a new variable. */\nLABEL; /* Assign labels to variables. */\nDATALINES; /* Enters the data. */\nRUN;",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#sas-dataset",
    "href": "part1_1_intro_operation.html#sas-dataset",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "1.3 SAS Dataset",
    "text": "1.3 SAS Dataset\nSAS dataset is used to organize data values in a tabular form, i.e., in the form of rows for observations and columns for variables.\n\nA SAS data set is a matrix whose each column is for each variable and whose each row for each observation (e.g., subject).\n\nData sets can be entered in the SAS programming code or can be read in from a variety of external sources, such as text files, csv files, and Microsoft Excel. In subsequent classes we will discuss reading in data sets from external files. Once a data set has been created, commands or procedures can operate on these data sets.\nOther than these steps programming structure also includes data set, label, variables, values, and run.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#sas-examples",
    "href": "part1_1_intro_operation.html#sas-examples",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "1.4 SAS Examples",
    "text": "1.4 SAS Examples\n\n1.4.1 Creating a Dataset\nOur first task in using SAS will be to create a small dataset and ‚Äúprint‚Äù that dataset to the output window. As we mentioned in previous paragraph SAS programs usually start with a DATA step where the dataset is created. Once the dataset is available, various procedures can be run on the dataset. The example below is written in the SAS Program window. The program creates a dataset called ‚ÄúPeople‚Äù with 3 variables (columns) which are ‚Äògender‚Äô, ‚Äòheight‚Äô, and ‚Äòweight‚Äô and 14 observations (rows). Note that the values of the variables on each line are separated by one or more blanks. A few other things that you should note:\n\nAll SAS statements end with a semicolon (;)\nMore than one SAS statement can be put on a line, or a SAS statement can continue across several lines, if every statement ends with a semicolon.\nData listed as part of the program is also terminated with a semicolon. Data does not have to be entered in the program; it can also be read from files that are external to the SAS program (more on that next week)\ngender is a character variables as indicated by the $, and height and weight are numeric variables.\nOnce the dataset is created, various SAS procedures (called PROCs) can be used to analyze the data and present results. We will start with a listing of the data created with a procedure called PROC PRINT.\n\ntitle1 'STAT 8678 Example 1';\ntitle2 'Your name';\nDATA people;\nINPUT gender $ height weight;\n\nDATALINES;\nm 63 125\nm 76 195\nf 62 109\nm 75 186\nf 67 115\nf 60 120\nm 75 205\nm 71 185\nm 63 140\nf 59 135\nf 65 125\nm 68 167\nm 72 220\nf 66 155\n;\n\nPROC PRINT DATA=people;\nRUN;\nThe LOG window gives information on the execution of the program. If your program did not execute properly you should examine the log for error messages that may explain the failure. The program would then be modified if necessary and rerun.\nSAS creates a new window called the Results Viewer when the program is executed and produces output. This window is in HTML format and a new tab for the window is created below the left-hand windows.\n\n\n1.4.2 Sorting Data\nThe data can be sorted (in our case by gender) using PROC SORT by adding the following lines to the program. We can then go ahead and print our new dataset sorted by gender with the proc print step.\n\n\n\n\n\n\nNoteReminder\n\n\n\nAny procedural step we do must begin with PROC and every line must end with a semicolon and the command run;\n\n\nPROC SORT data = people;\nBY gender;\nRUN;\n\nPROC PRINT data=people;\nTITLE3 \"Raw data sorted only by gender\";\nRUN;\n\n\n\n\n\n\nNoteNote\n\n\n\nAny time we use PROC SORT our original dataset is sorted. SAS does not create a copy then sort!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#generating-summary-statistics",
    "href": "part1_1_intro_operation.html#generating-summary-statistics",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "1.5 Generating Summary Statistics",
    "text": "1.5 Generating Summary Statistics\nThe last procedure to be executed in this exercise is PROC UNIVARIATE. This procedure will allow us to see summary statistics for any quantitative variable. The output from PROC UNIVARIATE will be important for the early part of this statistical methods class. It will provide measures of central tendency (mean, median, mode) and measures of dispersion (variance, standard deviation, range) as well as other basic statistics.\nPROC UNIVARIATE DATA=people PLOT;\n  BY gender;\n  TITLE3 \"Univariate procedure output done separately by gender\";\n  TITLE4 \"The analysis was done for two quantitative variables\";\n  VAR HEIGHT WEIGHT;\nRUN;\nThe code below applies the procedure only to the variable gender. It can be run on any quantitative variable and it could be run on several variables at the same time by listing several variables in the VAR statement, which would provide a separate analysis for each variable.\nThe results for PROC UNIVARIATE will be listed in the results viewer.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#other-notes",
    "href": "part1_1_intro_operation.html#other-notes",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "1.6 Other notes",
    "text": "1.6 Other notes\nOther Note\n\nSAS does not distinguish between upper-case letters or lower-case letters in the program, either can be used. However, it does distinguish between upper and lower case in datasets, so the character strings ‚ÄúCarol‚Äù, ‚Äúcarol‚Äù and ‚ÄúCAROL‚Äù would be considered different values of the variable ‚Äúname‚Äù in the program above.\nComments: Additionally, you may add comments anywhere in your program either by beginning the statement with an asterisk (*) and ending it with a semicolon (;) or by beginning with /* and ending with */. These comments may be thought of as marginal notes, and will show in the program editor and log, but not in the output window.\n\nShortcuts:\n\nF3 or the ‚Äúrunning man‚Äù: submits/run your program;\nF4: recalls text once the program editor;\nF5: directs user to the program editor;\nF6: directs user to the log;\nF7: directs user to the output;\nCtrl+E clears content in the current window.\nAlso, text can be copied with Ctrl+C, cut with Ctrl+X, or pasted with Ctrl+V.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_1_intro_operation.html#data-type",
    "href": "part1_1_intro_operation.html#data-type",
    "title": "1¬† Introduction to Basic SAS Operation",
    "section": "1.7 Data Type",
    "text": "1.7 Data Type\nWe can classify variables into quantitative variables and qualitative variables:\n\nQualitative variables yield non-numerical information. Qualitative variables are often referred to as categorical variables, such as blood type. Qualitative variables can be further classified as\n\nA nominal variable is a qualitative variable where no ordering is possible or implied in the levels, such as gender.\nA ordinal variable is a qualitative variable with an order implied in the levels, such as health ( poor, reasonable, good, or excellent)\n\nQuantitative variables yield numerical measurements. Quantitative variables can be further classified as discrete or continuous.\n\nA discrete variable can assume only a countable number of values, such as headache severity scores.\nA continuous variable is one that can take any one of an uncountable number of values in an interval, such as weight.\n\n\n\n\nQuestion:\nWhat is the type of each variable in the following dataset?\n\nAGE: The respondent‚Äôs age in years\nGENDER: The respondent‚Äôs sex coded 1 for male and 2 for female\nHAPPY: The respondent‚Äôs general happiness, coded:1 for ‚ÄúNot too happy‚Äù2 for ‚ÄúPretty happy‚Äù3 for ‚ÄúVery happy‚Äù\nTVHOURS: The average number of hours the respondent watched TV during a day\n\n\n\n\nSurvey Respondent Summary\n\n\nRespondent\nAGE\nGen\nHAPPY\nTVHOURS\n\n\n\n\n1\n41\n1\n2\n0\n\n\n2\n25\n2\n1\n0\n\n\n3\n43\n1\n2\n4\n\n\n4\n38\n1\n2\n2\n\n\n5\n53\n2\n3\n2\n\n\n6\n43\n2\n2\n6\n\n\n7\n56\n2\n2\n2\n\n\n\n\n\nAnswer:\n\nAge: Continuous variable;\nSEX: qualitative;\nHAPPY: Discrete variable;\nTVHOURS: Continuous variable",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Basic SAS Operation</span>"
    ]
  },
  {
    "objectID": "part1_2_sas_syntax.html",
    "href": "part1_2_sas_syntax.html",
    "title": "2¬† Working with SAS Syntax",
    "section": "",
    "text": "2.1 SAS Statments\nIn general, writing a SAS program is to write a sequence of steps, and a step is a sequence of SAS statements:\nSAS statements have these characteristics (Check the highlight context in the following examples):\nSAS programming statements are easier to read if you begin DATA, PROC, and RUN statements in column one and indent the other statements. This makes it more structured. Also, consistent spacing also makes a SAS program easier to read.\nOverall, we can type SAS statements in the following ways:\nSometimes we may want to make comments/notes to easy remember our SAS statements or to let other people easily understand what our code want to say. These comments are text that SAS ignores during processing. You can use comments anywhere in a SAS program to document the purpose of the program, explain segments of the program, or mark SAS code as non-executing text.\nThere are two ways to insert comments in a SAS program:\nAvoid placing the /* comment symbols in columns 1 and 2. On some operating environments, SAS might interpret these symbols as a request to end the SAS job or session. An example is given below:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Working with SAS Syntax</span>"
    ]
  },
  {
    "objectID": "part1_2_sas_syntax.html#sas-statments",
    "href": "part1_2_sas_syntax.html#sas-statments",
    "title": "2¬† Working with SAS Syntax",
    "section": "",
    "text": "Usually begin with an identifying keyword.\nAlways end with a semicolon.\n\n\n\n\n\n\nOne or more blanks can be used to separate words.\nThey can begin and end in any column.\nA single statement can span multiple lines.\nSeveral statements can be on the same line.\n\n\n\n\n\nUsing an asterisk (*) to begin the comment and a semicolon (;) to end the comment.\n\n* This is a comment in SAS;\n\nUsing slash-asterisk (/*) to begin the comment and asterisk-slash (*/) to end the comment.\n\n/* This is a comment in SAS */",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Working with SAS Syntax</span>"
    ]
  },
  {
    "objectID": "part1_2_sas_syntax.html#diagnosing-and-correcting-syntax-errors",
    "href": "part1_2_sas_syntax.html#diagnosing-and-correcting-syntax-errors",
    "title": "2¬† Working with SAS Syntax",
    "section": "2.2 Diagnosing and Correcting Syntax Errors",
    "text": "2.2 Diagnosing and Correcting Syntax Errors\nSyntax errors occur when program statements do not conform to the rules of the SAS language.\nExamples of syntax errors:\n\nmisspelled keywords\nunmatched quotation marks\nmissing semicolons\ninvalid options\n\nWhen SAS encounters a syntax error, SAS prints a warning or an error message to the log; for example,\n\nWhen SAS encounters a syntax error, SAS underlines the error and the following information is written to the SAS log:\n\nthe word ERROR or WARNING\nthe location of the error + an explanation of the error\n\n\nThis program has three syntax errors. What are the errors?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Working with SAS Syntax</span>"
    ]
  },
  {
    "objectID": "part1_2_sas_syntax.html#solution-to-the-example",
    "href": "part1_2_sas_syntax.html#solution-to-the-example",
    "title": "2¬† Working with SAS Syntax",
    "section": "2.3 Solution to the example",
    "text": "2.3 Solution to the example\n\nExercise 1: 5\nExercise 2:",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Working with SAS Syntax</span>"
    ]
  },
  {
    "objectID": "part1_3_import_export.html",
    "href": "part1_3_import_export.html",
    "title": "3¬† Import and Export Dataset",
    "section": "",
    "text": "3.1 Reading from External Files\nOne of the strengths of SAS as a data analysis tool is its ability to read data from many sources, subset or combine data sets, and modify the datasets to accomplish various tasks. The most common types of external data sets used in SAS are EXCEL files (XLS extent), comma separated value files (CSV extent) and various space separate text files (PRN or TXT extent). A CSV file is actually a text file and can be read in any text reader (NOTEPAD or WORDPAD in Windows). In fact, the SAS files themselves, as well as the LOG and the LST files produced by a SAS by a batch submit, are also simple text files.\nThe PROC IMPORT statement is the best way to enter external data sets. The CSV file we will be using is called ‚Äúgrades.csv‚Äù. Download and save it in your favourite folder and mark the complete path to it. Then use the following code to import it, making sure you put the correct path on the DATAFILE argument.\nThe IMPORT statement reads the dataset and stores it as the value designated by ‚ÄúOUT‚Äù in this case it will be saved as ‚ÄúGrades‚Äù in the library ‚ÄúWork‚Äù.\nThe DBMS statement defines the type of input SAS should be reading. The following table gives you all the possible choices. The REPLACE argument forces SAS to overwrite any older datasets with the same name.\nThe GETNAMES = YES or NO statement for spreadsheets and delimited external files, determines whether to generate SAS variable names from the column names in the input file‚Äôs first row of data. If you specify GETNAMES = NO or if the column names are not valid SAS names, PROC IMPORT uses the variable names VAR0, VAR1, VAR2, and so on. You may replace the equals sign with a blank.\nThe DATAROW argument tells SAS where to start reading for input data. In our case it is row 2 since row 1 is used for variable names.\nWe use the print procedure to see the dataset,\nThe first few rows are shown as follows",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Import and Export Dataset</span>"
    ]
  },
  {
    "objectID": "part1_3_import_export.html#reading-from-external-files",
    "href": "part1_3_import_export.html#reading-from-external-files",
    "title": "3¬† Import and Export Dataset",
    "section": "",
    "text": "PROC IMPORT OUT= GRADES_temp\n  DATAFILE= \"Put Your Path Here/grades_temp.csv\" DBMS=CSV REPLACE;\n  GETNAMES=YES;\n  DATAROW=2;\nRUN;\n\n\n\n\n\nPROC PRINT DATA=GRADES_temp;\nRUN",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Import and Export Dataset</span>"
    ]
  },
  {
    "objectID": "part1_3_import_export.html#export-a-file-from-sas",
    "href": "part1_3_import_export.html#export-a-file-from-sas",
    "title": "3¬† Import and Export Dataset",
    "section": "3.2 Export a File from SAS",
    "text": "3.2 Export a File from SAS\nAfter reading a dataset into SAS, we may need to conduct some initial step/analysis to re-organize dataset for doing further data analysis. In the grade example, the first row shows the maximum points available for each quiz. We need to remove this row so that our analysis is correct. This can be done by the following SAS code\nDATA GRADES;\n  SET GRADES_temp NOBS=COUNT\n    IF _n_        &lt;= 1 THEN DELETE;\nRUN;\nNote: GRADES_temp is the name of the old dataset and GRADES is the name of the new dataset after the first row is deleted.\nAfter re-organize the dataset, we may want to export the updated dataset for use in the future:\nThis can be done by\nPROC EXPORT DATA= GRADES\n  OUTFILE= \"Put Your Path Here/grade_v2.csv\" DBMS=CSV REPLACE;\n  REPLACE;\nRUN;\nWe will talk about more reorganizing skills in SAS in the next topic.\n\nWhich statement is true concerning the DATALINES statement based on reading the comment?\n\nThe DATALINES statement is used when reading data located in a raw data file.\nThe DATALINES statement is used when reading data located directly in the program.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Import and Export Dataset</span>"
    ]
  },
  {
    "objectID": "part1_3_import_export.html#import-export-in-sas-virtual-studio",
    "href": "part1_3_import_export.html#import-export-in-sas-virtual-studio",
    "title": "3¬† Import and Export Dataset",
    "section": "3.3 Import & Export in SAS Virtual Studio",
    "text": "3.3 Import & Export in SAS Virtual Studio\nThe key is\n\nUpload data before conducting ‚ÄúImport‚Äù\nDownload data after conducting ‚ÄúExport‚Äù\n\n\nA example given below will be demonstrated during the class.\n/* Generated Code (IMPORT) */\n/* Source File: grades_temp.csv */\n/* Source Path: /home/u63733881/sasuser.v94 */\n/* Code generated on: 1/20/24, 3:49 PM */\n\n%web_drop_table(WORK.IMPORT);\n\nFILENAME REFFILE '/home/u63733881/sasuser.v94/grades_temp.csv';\n\nPROC IMPORT DATAFILE=REFFILE\n    DBMS=CSV\n    OUT=WORK.grades_temp;\n    GETNAMES=YES;\nRUN;\n\nPROC CONTENTS DATA=WORK.grades_temp; \nRUN;\n\nPROC print data=WORK.grades_temp;\nRUN;\n\nDATA WORK.grades;\nSET WORK.grades_temp NOBS=COUNT;\nIF _n_ &lt;= 1 THEN DELTE;\nRUN;\n\nPROC PRINT DATA=WORK.grades;\nRUN;\n\nPROC EXPORT data=WORK.grades\n    OUTFILE = \"/home/u63733881/sasuser.v94/grades_2.csv\"\n    DBMS = csv\n    REPLACE;\nRUN;",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Import and Export Dataset</span>"
    ]
  },
  {
    "objectID": "part1_3_import_export.html#solution-to-the-example",
    "href": "part1_3_import_export.html#solution-to-the-example",
    "title": "3¬† Import and Export Dataset",
    "section": "3.4 Solution to the example",
    "text": "3.4 Solution to the example\nAnswer of the example: b",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Import and Export Dataset</span>"
    ]
  },
  {
    "objectID": "part1_4_rv.html",
    "href": "part1_4_rv.html",
    "title": "4¬† Random Variables",
    "section": "",
    "text": "4.1 What Is a Random Variable?\nA random variable (RV) is a numerical quantity whose value depends on the outcome of a random experiment.\nWe typically denote a random variable by an uppercase letter, such as \\(X\\) and its realized value by a lowercase letter, such as \\(X = x\\). The random variable can be continuous or discrete.\nIn practice, we often observe multiple realizations, or running the random experiment multiple times, say \\(n\\) times or \\(n\\) realizations. We denote these realizations as: \\[X_1 = x_1, \\; X_2 = x_2, \\; \\ldots, \\; X_n = x_n.\\] The number \\(n\\) is called the sample size.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "part1_4_rv.html#what-is-a-random-variable",
    "href": "part1_4_rv.html#what-is-a-random-variable",
    "title": "4¬† Random Variables",
    "section": "",
    "text": "Define whether the following random variables are discrete or continuous, and the possible values that \\(X\\) takes.\n\nNumber of emails received by a server in one hour (\\(X = 0, 1, 2, \\ldots\\): discrete)\nTime (in minutes) until a machine fails (\\(X = x \\in [0, \\infty)\\): continuous)\nTotal number of defects on a manufactured item (\\(X = 0, 1, 2, \\ldots\\): discrete)\nDaily maximum temperature in Atlanta (in degrees Fahrenheit) (\\(X = x \\in (-\\infty, \\infty)\\): continuous)\nWhether a randomly selected loan defaults within one year (\\(X = 1 \\text{ if default, } 0 \\text{ otherwise}\\): discrete)\n\n\n\nDefine whether the following random variables are discrete or continuous, and the possible values that \\(X\\) takes.\n\nNumber of transactions made by a customer in a day\nResponse time (in seconds) of a web service request\nCount of hospital admissions in a city per week discrete)\nProportion of time a system is idle during a day",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "part1_4_rv.html#discrete-vs-continuous-random-variables",
    "href": "part1_4_rv.html#discrete-vs-continuous-random-variables",
    "title": "4¬† Random Variables",
    "section": "4.2 Discrete vs Continuous Random Variables",
    "text": "4.2 Discrete vs Continuous Random Variables\n\n4.2.1 Discrete Random Variables\nA discrete random variable takes values in a finite or countable set.\nExamples:\n\nNumber of heads in three coin flips\nNumber of students passing an exam\nNumber of events occurring in a fixed time period\n\nA discrete random variable is described by a probability mass function (PMF):\n\n\n\nValue of \\(X\\)\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n‚Ä¶\n\\(x_m\\)\n\n\n\n\nProbability\n\\(p_1\\)\n\\(p_2\\)\n\\(p_3\\)\n‚Ä¶\n\\(p_m\\)\n\n\n\nThese probabilities satisfy:\n\n\\(0 \\le p_i \\le 1\\),\n\\(\\sum_{i=1}^m p_i = 1\\).\n\nWe calculate the probability of events modelled by discrete random variables by summing up the probability \\(p_i\\) for the values \\(x_i\\) that make up the event.\n\nSuppose the length \\(X\\) (in minutes) of an international phone call has distribution:\n\n\n\nX\n1\n2\n3\n4\n\n\n\n\nP(X)\n0.2\n0.5\n0.2\n0.1\n\n\n\nThen, calculate the following probabilities\n\n\\(P(X \\le 2)\\)\n\\(P(X &lt; 2)\\)\n\\(P(X &gt; 1)\\)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "part1_4_rv.html#common-discrete-distributions",
    "href": "part1_4_rv.html#common-discrete-distributions",
    "title": "4¬† Random Variables",
    "section": "4.3 Common Discrete Distributions",
    "text": "4.3 Common Discrete Distributions\n\n4.3.1 Bernoulli Distribution\nUsed for binary outcomes:\n\nsuccess / failure\nyes / no\n1 / 0\n\nNotation: \\(X \\sim \\text{Ber}(p)\\), where \\(p\\) is the probability of success.\nExample:\nWhether a patient has diabetes (1 = yes, 0 = no).\nHow to specify the probability \\(p\\) will be discussed in the following lecture. This is related to the procedure of statistical inference.\n\n\n4.3.2 Poisson Distribution\nUsed to model counts of events over time or space.\nNotation: \\(X \\sim \\text{Poi}(\\lambda)\\)\nwhere \\(\\lambda\\) is the mean rate.\nExamples:\n\nNumber of trades per day\nNumber of system failures per week\nNumber of arrivals to a service queue\n\nSimilar to the probability \\(p\\) from the Bernoulli distribution, how to specify the rate \\(\\lambda\\) will be discussed in the following lecture.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "part1_4_rv.html#continuous-random-variables",
    "href": "part1_4_rv.html#continuous-random-variables",
    "title": "4¬† Random Variables",
    "section": "4.4 Continuous Random Variables",
    "text": "4.4 Continuous Random Variables\nA continuous random variable can take any value \\(x\\) in an interval.\nExamples:\n\nHeight of individuals\nTime until failure of a component\nTest scores treated as continuous\n\nProbabilities are defined using density functions, not point probabilities. Some common continuous distributions are as follows.\n\n4.4.1 Normal Distribution\nThe normal distribution is also called the Gaussian distribution. It is characterized by two parameters: the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). It is unimodal and symmetric around the mean which is the centre of the mass. A continuous random variable \\(X\\) that has a normal distribution is said to be normal or normally distributed.\nNotation: \\(X \\sim N(\\mu, \\sigma^2)\\). Sometimes the standard deviation may be used instead of the variance, which is the square of the variance.\nInstitution of the normal distribution:\n\nmean: \\(\\mu\\)\nvariance: \\(\\sigma^2\\)\n\n\n\n\n\n\n\n\n\n\nA distribution plot of the normal distribution with mean 0 and standard deviation 1 is shown above. It is often referred as the standard normal distribution. In practice, we would like to ‚Äústandardized‚Äù the data to have mean 0 and standard deviation 1 for the subsequent analysis.\nNormal distribution play an important role in statistical inference. One of the important property is the 68-95-99.7 rule: - About 68% of the data falls within one standard deviation of the mean - About 95% of the data falls within two standard deviations of the mean - About 99.7% of the data falls within three standard deviations of the mean\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Exponential Distribution\nThe exponential distribution is another common distribution where a few outcomes are most likely with a rapidly decreasing probability for larger values. It is often used to model waiting times or lifetimes of objects. It is similar to the geometric distribution in the discrete case.\nTwo ways to specify the parameter.\n\n\\(X \\sim \\text{Exp}(\\lambda)\\), where \\(\\lambda\\) is the rate parameter.\n\\(X \\sim \\text{Exp}(\\beta)\\), where \\(\\beta\\) is the scale parameter, and \\(\\beta = 1/\\lambda\\).\n\nThe notation is either \\(X \\sim \\text{Exp}(\\lambda)\\) or \\(X \\sim \\text{Exp}(\\beta)\\). But to be sure which parameterization is used, we need to check the definition of the distribution.\n\nPlease define adequate random variables for the following data example, and thaink about what distribution can be used to model the data.\n\nSuppose we want to know whether the rate of diabetes of a certain area is too high. The researchers randomly discuss with 10 individuals in a certain area to know whether they have diabetes. The data are collected as ‚Äúyes‚Äù or ‚Äúno‚Äù answers as follows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nOutcome\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n\n\nA comapny that manufactures light blulb claims that a particular type of light bulb will last 850 hours on average. To justify the claim more scientifically, a researcher randomly selects 10 light bulbs of that type and measures the lifetimes (in hours) of the light bulbs as follows:\n\n\n\n\nLight Bulb\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\n\n831\n832\n840\n819\n822\n836\n829\n817\n\n\n\n\nIn the following classes we will assume the dataset we have can be well-modelled represented for the underline studies. The data collection, however, is beyond the scope of the class. For those interested in data collection, please refer to the survey sampling or experimental design area.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "part1_4_rv.html#solution-to-the-exercise",
    "href": "part1_4_rv.html#solution-to-the-exercise",
    "title": "4¬† Random Variables",
    "section": "4.5 Solution to the exercise",
    "text": "4.5 Solution to the exercise\nAnswer of the exercise\nExercise 1\n\nNumber of transactions made by a customer in a day (\\(X = 0, 1, 2, \\ldots\\): discrete)\nResponse time (in seconds) of a web service request (\\(X = x &gt; 0\\): continuous)\nCount of hospital admissions in a city per week (\\(X = 0, 1, 2, \\ldots\\): discrete)\nProportion of time a system is idle during a day (\\(X = x \\in [0,1]\\): continuous)\n\nExercise 2:\n\n\\(P(X \\le 2) = 0.7\\)\n\\(P(X &lt; 2) = 0.2\\)\n\\(P(X &gt; 1) = 0.8\\)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "part2_1_intro_inference1.html",
    "href": "part2_1_intro_inference1.html",
    "title": "5¬† Introduction to Statistical Inference I",
    "section": "",
    "text": "5.1 Probability versus Statistics\nThere are three common goals of statistical inference:",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Statistical Inference I</span>"
    ]
  },
  {
    "objectID": "part2_1_intro_inference1.html#probability-versus-statistics",
    "href": "part2_1_intro_inference1.html#probability-versus-statistics",
    "title": "5¬† Introduction to Statistical Inference I",
    "section": "",
    "text": "In probability, we assume that random variables\n\\(X_1, \\ldots, X_n\\) follow a distribution with known parameters.\nUnder this model, we can calculate probabilities of events of interest.\nIn statistics, although we still use a distribution to model\n\\(X_1, \\ldots, X_n\\), the parameters of the distribution are assumed to be unknown.\nOur primary goal is to use observed data, the realization\n\\(X_1 = x_1, \\ldots, X_n = x_n\\)‚Äîto make inference about these unknown parameters.\n\n\n\nPoint estimation\nInterval estimation\nHypothesis testing\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there may be multiple valid methods for achieving each goal, even when working with the same dataset.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Statistical Inference I</span>"
    ]
  },
  {
    "objectID": "part2_1_intro_inference1.html#example-one-sample-mean-problem",
    "href": "part2_1_intro_inference1.html#example-one-sample-mean-problem",
    "title": "5¬† Introduction to Statistical Inference I",
    "section": "5.2 Example: One-Sample Mean Problem",
    "text": "5.2 Example: One-Sample Mean Problem\nThe term one sample does not mean that there is only one observation. Instead, it means that there is one population under study.\nIn this problem, we are interested in making inference about the population mean, denoted by \\(\\mu\\).\n\n5.2.1 Problem Formulation\nSuppose we want to conduct statistical inference on the mean length of a certain type of court case. Let \\[\n\\mu = \\text{the mean length of a certain type of court case}.\n\\]\nHowever, the true value of \\(\\mu\\) is unknown because we cannot observe all realizations of this process. As a result, statistical inference is required.\nIf \\(\\mu\\) were known, no inference would be necessary. What we can do in practice is to collect data. Suppose we randomly select 20 court cases of the same type from historical records and observe their case lengths (in days):\n\\[\n43,\\; 90,\\; 84,\\; 87,\\; 116,\\; 95,\\; 86,\\; 99,\\; 93,\\; 92,\n\\] \\[\n121,\\; 71,\\; 66,\\; 98,\\; 79,\\; 102,\\; 60,\\; 112,\\; 105,\\; 98.\n\\]\nFrom a statistical perspective, these observed values are treated as realizations of random variables. Specifically, we denote\n\\[\nX_1 = 43,\\; X_2 = 90,\\; \\ldots,\\; X_{20} = 98.\n\\]\nTo model the data, we assume that the random variables \\[\nX_1, X_2, \\ldots, X_{20}\n\\] are independent and identically distributed according to a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), that is,\n\\[\nX_i \\stackrel{\\text{i.i.d.}}{\\sim} N(\\mu, \\sigma^2),\n\\qquad i = 1, \\ldots, 20.\n\\]\nHere, both \\(\\mu\\) and \\(\\sigma^2\\) are unknown parameters. In this example, our primary interest lies in estimating the population mean \\(\\mu\\), while the variance \\(\\sigma^2\\) is treated as a nuisance parameter.\nWe can use the maximum likelihood estimator (MLE) or the method of moments estimator (MME) to construct a point estimator of \\(\\mu\\). Under this model, however, they are coincide. The resulting estimator is the sample mean, denoted by\n\\[\n\\hat{\\mu} = \\bar{X}_{20},\n\\]\nwhere\n\\[\n\\bar{X}_{20} = \\frac{1}{20} \\sum_{i=1}^{20} X_i.\n\\]\nUnder the assumed model, we may use either the maximum likelihood estimator or the method of moments to construct a point estimator for the population mean \\(\\mu\\). In this model, both methods lead to the same estimator: the sample mean. We denote this estimator by\n\\[\n\\hat{\\mu} = \\bar{X}_n.\n\\]\nPay careful attention to the notation. We use a capital letter \\(\\bar{X}_n\\) to emphasize that the estimator itself is a random variable. Since the estimator is a function of the random variables \\(X_1, \\ldots, X_n\\), it is also random and therefore follows a probability distribution. The probability distribution of an estimator is called its sampling distribution. We will return to the concept of sampling distributions later in the course.\n\n\n5.2.2 Formal Definitions\nTo be more precise, we introduce the following definitions.\n\nA statistic is any function of the random variables \\(X_1, \\ldots, X_n\\). Because it is a function of random variables, a statistic is itself a random variable and therefore has a probability distribution. If we use the parametric model to describe it, then there is a distribution that depends on the unknown parameters.\nA (point) estimator of a parameter \\(\\theta\\), denoted by \\(\\hat{\\theta}\\), is a statistic used to estimate \\(\\theta\\). (Note, an estimator is a random variable because it is a statistics).\nA (point) estimate is the numerical value obtained by evaluating the estimator using the observed data \\(x_1, \\ldots, x_n\\). In particular, we use lowercase notation to indicate an estimate. For example, \\(\\bar{x}_n\\) denotes the observed value of the estimator \\(\\bar{X}_n\\).\n\nSo far, we have constructed a point estimator (and hence a point estimate). To construct a confidence interval or perform hypothesis testing, we need to know the sampling distribution of the estimator \\(\\bar{X}_n\\).\nUnder the normal model with unknown variance, the sampling distribution can be expressed as\n\\[\n\\frac{\\bar{X} - \\mu}{S / \\sqrt{n}} \\sim t_{n-1},\n\\]\nwhere\n\\[\nS = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n - 1}},\n\\]\nand \\(t_{n-1}\\) denotes the Student‚Äôs \\(t\\) distribution with \\(n - 1\\) degrees of freedom.\n\n\n5.2.3 Confidence Interval for \\(\\mu\\)\nBased on this sampling distribution, a \\((1 - \\alpha) \\times 100\\%\\) confidence interval for \\(\\mu\\) is given by\n\\[\n\\left[\n\\bar{X} - t_{n-1,\\alpha/2} \\frac{S}{\\sqrt{n}},\n\\;\n\\bar{X} + t_{n-1,\\alpha/2} \\frac{S}{\\sqrt{n}}\n\\right].\n\\]\n\n\n5.2.4 Hypothesis Testing for \\(\\mu\\)\nUsing the same sampling distribution, we can conduct one of the following hypothesis tests:\n\nTwo-sided test \\[\nH_0: \\mu = \\mu_0\n\\quad \\text{vs.} \\quad\nH_1: \\mu \\neq \\mu_0\n\\]\nLeft-tailed test \\[\nH_0: \\mu = \\mu_0\n\\quad \\text{vs.} \\quad\nH_1: \\mu &lt; \\mu_0\n\\]\nRight-tailed test \\[\nH_0: \\mu = \\mu_0\n\\quad \\text{vs.} \\quad\nH_1: \\mu &gt; \\mu_0\n\\]\n\nIn some textbooks, equivalent hypotheses are written as\n\n\\[\nH_0: \\mu = \\mu_0\n\\quad \\text{vs.} \\quad\nH_1: \\mu \\neq \\mu_0\n\\]\n\\[\nH_0: \\mu \\ge \\mu_0\n\\quad \\text{vs.} \\quad\nH_1: \\mu &lt; \\mu_0\n\\]\n\\[\nH_0: \\mu \\le \\mu_0\n\\quad \\text{vs.} \\quad\nH_1: \\mu &gt; \\mu_0\n\\]\n\nFortunately, SAS can perform point estimation, confidence interval construction, and hypothesis testing simultaneously, provided we clearly specify:\n\nThe statistical question\n(one-sample mean, one-sample variance, two-sample problem, etc.)\nThe inferential goal\n(point estimation, confidence interval, hypothesis testing)\nThe statistical model and method\n(for example, why we use the \\(t\\) distribution for the one-sample mean problem instead of the normal distribution or another alternative)\n\nOne of the main goals of this course is to help you build a mental ‚Äúlibrary‚Äù of statistical tools. Later, when you encounter a statistical question, you will be able to identify an appropriate method and then use SAS to obtain numerical results for interpretation.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Statistical Inference I</span>"
    ]
  },
  {
    "objectID": "part2_1_intro_inference1.html#revisit-the-data-example-by-using-sas",
    "href": "part2_1_intro_inference1.html#revisit-the-data-example-by-using-sas",
    "title": "5¬† Introduction to Statistical Inference I",
    "section": "5.3 Revisit the data example by using SAS",
    "text": "5.3 Revisit the data example by using SAS\nThe court length data can be read by the following DATA step:\nDATA TIME;\n  INPUT TIME @@;\nDATALINES;\n43 90 84 87 116 95 86 99 93 92\n121 71 66 98 79 102 60 112 105 98\n;\nRUN;\nThe only variable in the DATA set, time, is assumed to be normally distributed. The trailing at signs (@@) indicate that there is more than one observation on a line. The following statements invoke PROC TTEST for a one-sample t test:\nPROC TTEST H0=80 PLOTS(SHOWH0) SIDES=2 ALPHA=0.1;\n  VAR TIME;\nRUN;\n\nTHE VAR statement indicates that the time variable is being studies\nthe H0= option specifies that the mean of the time variable should be compared to the null value rather than the default value of 0\nthe PLOTS(SHOWH0) option requests that this null value be displayed on all relevant graphs\nthe SIDE=2 option specifies that a the focus of the research question, namely whether the mean count case length is not equal to 80days, rather than less or greater than 80 days (in which case you would use the SIDE=L or SIDE=U options, respectively)\nthe ALPHA=0.1 option requests 90% confidence interval rather than the default 95% confidence interval\n\nThe output is presented in the table below.\n\n\n\n\n\n\n\nNote\n\n\n\nSome SAS procedures produce graphs as automatically as they produce tables if the ODS GRAPHICS option is used. The graphs are integrated with tables in the ODS output.\n\n\n\nODS GRAPHICS ON;\n\nPROC TTEST H0=80 PLOTS(SHOWH0) SIDES=U ALPHA=0.1;\n  VAR TIME;\nRUN;\n\nODS GRAPHICS OFF;\nyou will see the following results which include the same table you see in the last figure but with two more figures. We will talk those two Figures in the following classes, especially the QQ plot.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Statistical Inference I</span>"
    ]
  },
  {
    "objectID": "part2_1_intro_inference1.html#more-about-hypothesis-and-confidence-intervals",
    "href": "part2_1_intro_inference1.html#more-about-hypothesis-and-confidence-intervals",
    "title": "5¬† Introduction to Statistical Inference I",
    "section": "5.4 More about hypothesis and confidence intervals",
    "text": "5.4 More about hypothesis and confidence intervals\n\n5.4.1 How to construct a confidence interval?\nConsider a 95% confidence interval for the population mean \\(\\mu\\). Under the normal approximation (or by the Central Limit Theorem), we have\n\\[\nP(-1.96 &lt; Z &lt; 1.96) \\approx 0.95,\n\\]\nwhere \\(Z\\) is a standard normal random variable.\nEquivalently, this can be written as\n\\[\nP\\left(\n-1.96\n&lt;\n\\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\n&lt;\n1.96\n\\right)\n\\approx 0.95.\n\\]\n\n\n5.4.2 Rearranging the Inequality\nRearranging the terms inside the probability statement yields\n\\[\nP\\left(\n\\bar{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}}\n&lt;\n\\mu\n&lt;\n\\bar{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\n\\right)\n\\approx 0.95.\n\\]\n\n\n\n5.4.3 Large-Sample Confidence Interval\nThus, a large-sample 95% confidence interval for \\(\\mu\\) is given by\n\\[\n\\left[\n\\bar{X} - 1.96 \\frac{\\sigma}{\\sqrt{n}},\n\\;\n\\bar{X} + 1.96 \\frac{\\sigma}{\\sqrt{n}}\n\\right].\n\\]\nMore compactly, we often write this interval as\n\\[\n\\bar{X} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}.\n\\]",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Statistical Inference I</span>"
    ]
  },
  {
    "objectID": "part2_1_intro_inference1.html#hypothesis-testing-and-confidence-intervals-always-agree",
    "href": "part2_1_intro_inference1.html#hypothesis-testing-and-confidence-intervals-always-agree",
    "title": "5¬† Introduction to Statistical Inference I",
    "section": "5.5 3.2 Hypothesis Testing and Confidence Intervals Always Agree",
    "text": "5.5 3.2 Hypothesis Testing and Confidence Intervals Always Agree\n\n5.5.1 Interpretation of the \\(p\\)-value\nThe \\(p\\)-value is the probability of observing data at least as favorable to the alternative hypothesis \\(H_A\\) as the data actually observed, assuming the null hypothesis \\(H_0\\) is true.\nIn this example, the observed sample mean is either greater than \\(3.56\\) or less than \\(3.18\\), and the null hypothesis assumes the true population mean is \\(\\mu = 3.37\\).\n\n\n5.5.2 Computing the \\(p\\)-value\nWe compute the \\(p\\)-value as\n\\[\nP(\\bar{X} &gt; 3.56 \\text{ or } \\bar{X} &lt; 3.18 \\mid \\mu = 3.37).\n\\]\nThis can be written as the sum of two tail probabilities:\n\\[\nP(\\bar{X} &gt; 3.56 \\mid \\mu = 3.37) + P(\\bar{X} &lt; 3.18 \\mid \\mu = 3.37).\n\\]\nStandardizing using the normal distribution yields\n\\[\nP\\left(\nZ &gt; \\frac{3.56 - 3.37}{0.31 / \\sqrt{147}}\n\\right)\n+\nP\\left(\nZ &lt; \\frac{3.18 - 3.37}{0.31 / \\sqrt{147}}\n\\right).\n\\]\nEvaluating the standardized values gives\n\\[\nP(Z &gt; 7.43) + P(Z &lt; -7.43).\n\\]\nThis probability is extremely small:\n\\[\n10^{-13} \\approx 0.\n\\]\n\n\n5.5.3 Connection to Confidence Intervals\nBecause the \\(p\\)-value is essentially zero, we strongly reject \\(H_0\\) at any reasonable significance level.\nEquivalently, the hypothesized value \\(\\mu = 3.37\\) does not lie inside the corresponding confidence interval for \\(\\mu\\).\n\n\n\n\n\n\n\n\n\nThis illustrates a key principle:\n\nHypothesis testing and confidence intervals always lead to the same conclusion when they are constructed at compatible significance levels.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Statistical Inference I</span>"
    ]
  },
  {
    "objectID": "part2_2_intro_inference2.html",
    "href": "part2_2_intro_inference2.html",
    "title": "6¬† Introduction to Statistical Inference II",
    "section": "",
    "text": "6.1 1. Recall: One-Sample Mean Problem\nIn the previous class, we studied inference for a population mean and show how to run SAS to obtain the statistical inference result from one sample \\(t\\) test. The example for illusrtratioe is to study the mean length of a certain type of the court case, i.e., \\[\n\\mu = \\text{the mean length of a certain type of court case}.\n\\] The data for the study is\n\\[\n43,\\; 90,\\; 84,\\; 87,\\; 116,\\; 95,\\; 86,\\; 99,\\; 93,\\; 92,\n\\] \\[\n121,\\; 71,\\; 66,\\; 98,\\; 79,\\; 102,\\; 60,\\; 112,\\; 105,\\; 98.\n\\] and we implement the statistical model: \\[\nX_i \\stackrel{\\text{i.i.d.}}{\\sim} N(\\mu, \\sigma^2),\n\\qquad i = 1, \\ldots, 20.\n\\]\nWith this set up, we can use SAS, such as through the following code\nand obtain the statistical results. The results let us know\nfrom \\(t\\)-distribution for the one sample mean problem.\nFrom the example, we may think the beginning of conducting statistical analysis procedure as\nIn this lecture, we will continue the remaining two steps:",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Introduction to Statistical Inference II</span>"
    ]
  },
  {
    "objectID": "part2_2_intro_inference2.html#recall-one-sample-mean-problem",
    "href": "part2_2_intro_inference2.html#recall-one-sample-mean-problem",
    "title": "6¬† Introduction to Statistical Inference II",
    "section": "",
    "text": "ODS GRAPHICS ON;\n\nPROC TTEST H0=80 PLOTS(SHOWH0) SIDES=U ALPHA=0.1;\n  VAR TIME;\nRUN;\n\nODS GRAPHICS OFF;\n\n\nA point estimate of \\(\\mu\\)\nAn interval estimate of \\(\\mu\\)\nA hypothesis testing under \\(H_0:\\mu=80\\) versus \\(H_1\\),\n\n\n\n\nFormulate the statistical problem\nCollection the adequate dataset and think the dataset as realization of some random variables\nThink some statistical models (i.e., assumptions) to be considered for the random variables. This step may include some [iv]\npreliminary data analysis\n\n\n\nModel diagnostics\n\n\n\nInterpret the results",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Introduction to Statistical Inference II</span>"
    ]
  },
  {
    "objectID": "part2_2_intro_inference2.html#model-diagnostics",
    "href": "part2_2_intro_inference2.html#model-diagnostics",
    "title": "6¬† Introduction to Statistical Inference II",
    "section": "6.2 Model Diagnostics",
    "text": "6.2 Model Diagnostics\nIn this step, we pause and think carefully about the assumptions underlying our analysis.\nQuestion:\n\nWhat statistical models are we implementing?\n\nIn this course, the key assumptions for the one-sample mean problem are:\n\nA normality assumption\nAn independence assumption\n\nThe independence assumption is primarily determined by how the data are collected. Once the data have been sampled, this assumption is generally not testable from the data alone.\nWe therefore assume that the sampling process was conducted correctly and focus our attention on checking the normality assumption.\nThere are two broad classes of methods:\n\nGraphical (visual) diagnostics\nFormal hypothesis testing methods\n\n\n6.2.1 Graphical Diagnostics for Normality\nCommon graphical tools include:\n\nHistogram and Density Plots:\nA kernel density estimate (KDE) provides a smooth estimate of the underlying probability density function, analogous to a histogram but without binning. KDEs represent the data using a continuous curve and are particularly useful for visualizing distributional shape.\nQ‚ÄìQ Plot (Quantile‚ÄìQuantile Plot)\nA Q‚ÄìQ plot compares empirical quantiles of the observed data with theoretical quantiles from a normal distribution.\nIf the normality assumption is reasonable, the points should fall approximately along a straight line.\n\n\n\n\n\n6.2.2 Formal Hypothesis Testing for Normality\nIn addition to graphical methods, we can also assess the normality assumption using formal numerical tests.\nOne commonly used method is the Kolmogorov‚ÄìSmirnov (K‚ÄìS) test, which evaluates whether a sample plausibly comes from a specified distribution, such as the normal distribution.\nThe K‚ÄìS test is widely used because many statistical procedures rely on the assumption that the data are normally distributed. When this assumption is violated, standard inference procedures may no longer be valid. The following step-by-step example demonstrates how to perform a Kolmogorov‚ÄìSmirnov test for normality using SAS.\nStep 1: Create the dataset\nDATA time;\n    INPUT time @@;\n    DATALINES;\n43 90 84 87 116 95 86 99 93 92\n121 71 66 98 79 102 60 112 105 98\n;\nRUN;\nStep 2: Perform the normality test\n/* Perform Kolmogorov‚ÄìSmirnov test */\nPROC UNIVARIATE DATA=time;\n    HISTOGRAM time / NORMAL(mu=est sigma=est);\nRUN;\n\n\n\n\n\n\n\nNoteRemarks\n\n\n\nGraphical diagnostics are informal but highly informative.\nThey allow us to detect skewness, heavy tails, and outliers that may invalidate normal-based inference.\nFormal hypothesis tests for normality will be introduced next, but should always be interpreted in conjunction with these visual tools.\n\n\n\n\n\n\n\n\nNoteImportant Note\n\n\n\nFor the best practice:\nNever rely on a single normality test. Always combine numerical tests with visual inspection.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Introduction to Statistical Inference II</span>"
    ]
  },
  {
    "objectID": "part2_2_intro_inference2.html#interpretation-of-results",
    "href": "part2_2_intro_inference2.html#interpretation-of-results",
    "title": "6¬† Introduction to Statistical Inference II",
    "section": "6.3 Interpretation of Results",
    "text": "6.3 Interpretation of Results\n\n6.3.1 Point Estimation\nPoint estimation is the process of using sample data to compute a single numerical value that estimates an unknown population parameter, such as the population mean.\nWhen we report a point estimate, it is desirable to understand whether the estimator has good statistical properties. In particular, we often examine whether a point estimator is:\n\nConsistent: As the sample size increases, the estimator becomes closer to the true parameter value.\nUnbiased: The expected value of the estimator equals the true population parameter. For example, the sample mean is an unbiased estimator of the population mean.\nEfficient (or best unbiased): Among all unbiased and consistent estimators, it has the smallest variance, meaning the estimator varies less from sample to sample.\n\nHow to rigorously verify these properties is a major topic in theoretical statistics courses. In practice, when these properties are unknown or difficult to assess, the safest interpretation of a point estimate is simply to report it with its associated unit. For example, using the court length data, we may state:\n\nA reasonable estimate for the average court length is approximately 89.85 days.\n\n\n\n6.3.2 Confidence Intervals\nA confidence interval (CI) provides a range of plausible values for an unknown population parameter. A common‚Äîbut informal‚Äîinterpretation of a 95% confidence interval is that we are ‚Äú95% confident‚Äù the true parameter lies within the interval. While this interpretation is widely used, the strictly correct interpretation is based on repeated sampling:\n\nIf the same study were repeated infinitely many times, and a 95% confidence interval were constructed each time, then approximately 95% of those intervals would contain the true parameter value.\n\nAs an example, suppose the 90% confidence interval for the population mean court length is \\([15.2,\\; 26.2].\\) This means that, under the confidence interval procedure used, intervals constructed in this way would contain the true mean in 90% of repeated samples.\nFactors Affecting Confidence Interval Width\nThe width of a confidence interval depends on several factors:\n\nSample size\nLarger samples typically produce narrower (more precise) confidence intervals.\nVariability of the outcome\nFor continuous outcomes, higher variability (larger standard deviation) leads to wider intervals.\nOutcome type\n\nFor binary outcomes, precision depends on the event probability.\nFor time-to-event outcomes, precision depends on the number of observed events.\n\n\nAll of these factors influence the standard error of the estimator, which directly determines the width of the confidence interval.\n\n\n6.3.3 Hypothesis Testing\nHypothesis testing evaluates whether observed data are consistent with a specified statistical assumption, typically called the null hypothesis.\nThe result of a hypothesis test allows us to decide whether the assumption is supported by the data or whether there is sufficient evidence to reject it. The strength of this evidence is quantified by the p-value.\nA p-value is defined as:\n\nThe probability of observing data at least as extreme as the data actually observed, assuming the null hypothesis is true.\n\nFor example, if a hypothesis test yields a p-value of 0.01, this means that‚Äîif the null hypothesis were true‚Äîthere would be only a 1% chance of observing data this extreme.\nA small p-value provides evidence against the null hypothesis, while a large p-value indicates that the data are consistent with it.\nIf the observed data are very unlikely to occur under the conditions described by the null hypothesis, then the null hypothesis is unlikely to be true. In such cases, we reject the null hypothesis in favor of the alternative hypothesis, and the result is said to be statistically significant.\nA commonly used decision rule is based on a significance level of 0.05. If the p-value is less than or equal to 0.05, this is typically taken as evidence against the null hypothesis, and we reject it in favor of the alternative hypothesis. However, a p-value cannot be used to prove that a hypothesis is true. Instead, it quantifies the strength of evidence against the null hypothesis.\n\nConsider the court length data. Suppose we conduct a hypothesis test with \\[\nH_0: \\mu = 80\n\\quad \\text{versus} \\quad\nH_1: \\mu &gt; 80,\n\\] and obtain a p-value of 0.0164.\nIf we choose a significance level of 0.05, then since \\(0.0164 &lt; 0.05,\\)\nwe reject the null hypothesis. At the 5% significance level, there is strong statistical evidence that the population mean court length is not equal to 80.\nWhen the p-value Is Large\nIf the p-value is greater than 0.05, we do not reject the null hypothesis. This does not mean that the null hypothesis is true. Rather, it means that the data do not provide strong enough evidence to conclude that the population mean court length differs from 80.\nIn hypothesis testing, failing to reject the null hypothesis should be interpreted as insufficient evidence, not as confirmation of the null hypothesis.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Introduction to Statistical Inference II</span>"
    ]
  },
  {
    "objectID": "part2_3_1sample_nonparametric_test.html",
    "href": "part2_3_1sample_nonparametric_test.html",
    "title": "7¬† One Sample Nonparametric Test",
    "section": "",
    "text": "7.1 Motivation\nA parametric test specifies certain assumptions about the distribution of responses in the population from which the sample is drawn. The validity and interpretability of parametric test results depend critically on whether these assumptions are satisfied.\nIn contrast, a nonparametric test is based on a model that imposes only very general conditions and does not assume a specific parametric form for the population distribution.\nFor this reason, nonparametric tests are often referred to as distribution-free tests.\nAlthough nonparametric methods are not completely assumption-free, the assumptions they require are generally fewer and weaker than those of parametric tests.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>One Sample Nonparametric Test</span>"
    ]
  },
  {
    "objectID": "part2_3_1sample_nonparametric_test.html#motivation",
    "href": "part2_3_1sample_nonparametric_test.html#motivation",
    "title": "7¬† One Sample Nonparametric Test",
    "section": "",
    "text": "7.1.1 Key Characteristics of Nonparametric Tests\nNonparametric test statistics typically rely on simple features of the data, such as:\n\nSigns of measurements\n\nRanks or orderings\n\nCategory or frequency counts\n\nAs a result:\n\nLinear transformations (stretching or compressing the scale) do not affect the test statistic.\nThe null distribution of the test statistic can often be derived without specifying the population distribution.\n\nNonparametric tests therefore avoid assumptions such as:\n\nNormality\nHomogeneity of variance\n\nMoreover, nonparametric methods usually compare medians rather than means, which makes them less sensitive to outliers.\nAdvantages of Nonparametric Tests\n\nApplicable to all measurement scales\nParticularly useful when the sample size is very small, unless the distribution is known\nEasier to learn and compute\nRequire fewer assumptions\nMore robust due to weaker modeling assumptions\nDo not require explicit population parameters\nIn some cases, results can be as exact as parametric procedures\n\nDisadvantages of Nonparametric Tests\n\nOften less powerful than parametric tests when parametric assumptions hold\nProvide less information about population parameters\nInterpretation is usually framed in terms of location or rank, not means\nSome procedures do not extend easily to complex models\n\nSummary\n\nParametric tests rely on strong distributional assumptions but can be powerful and informative.\nNonparametric tests trade efficiency for robustness and flexibility.\nIn practice, nonparametric methods serve as valuable alternatives when assumptions are questionable or sample sizes are limited.\n\nIn the next section, we will study specific one-sample nonparametric procedures and implement them in SAS.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>One Sample Nonparametric Test</span>"
    ]
  },
  {
    "objectID": "part2_3_1sample_nonparametric_test.html#one-sample-nonparametric-test-in-sas",
    "href": "part2_3_1sample_nonparametric_test.html#one-sample-nonparametric-test-in-sas",
    "title": "7¬† One Sample Nonparametric Test",
    "section": "7.2 One Sample Nonparametric Test in SAS",
    "text": "7.2 One Sample Nonparametric Test in SAS\nBase SAS provides two commonly used one-sample nonparametric tests through the PROC UNIVARIATE procedure:\n\nSign test\nWilcoxon signed-rank test\n\nBoth tests are designed for situations in which we want to make inference about the location of a population, typically interpreted as the median rather than the mean.\nSuppose we are interested in testing whether the median resting pulse rate of marathon runners differs from a specified value. If the normality assumption required for a one-sample t-test is questionable, nonparametric alternatives provide a robust solution.\nBy default, both tests examine the hypothesis that the median of the population from which the sample is drawn is equal to a specific value,s which is zero by dafault. However, we note that\n\nWilcoxon signed-rank test\n\nAssumes the population distribution is symmetric\nGenerally more powerful when the symmetry assumption holds\n\nSign test\n\nDoes not require symmetry\nUses only the sign of deviations from the hypothesized median\nMore robust but typically less powerful\n\n\nBoth tests can also be extended to paired (related) samples, which will be discussed later when we cover comparisons of two related samples.\nBoth the sign test and the Wilcoxon signed-rank test are automatically available in PROC UNIVARIATE.\n/* Syntax: PROC UNIVARIATE */\nPROC UNIVARIATE &lt;options&gt;;\n    BY &lt;variables&gt;;\n    CDFPLOT &lt;variables&gt; &lt;/ options&gt;;\nRUN;\n\nCLASS variable-1 &lt;(v-options)&gt; &lt;variable-2 &lt;(v-options)&gt;&gt;&lt;/ KEYLEVEL=value1 (value1 value2\n)&gt;;FREQ variable;HISTOGRAM &lt;variables&gt; &lt;/ options&gt;;ID variables;INSET keyword-list &lt;/ options&gt;;OUTPUT\n&lt;OUT=SAS-data-set&gt; &lt;keyword1=names ...keywordk=names&gt; &lt;percentile-options&gt;;PPPLOT &lt;variables&gt;\n&lt;/ options&gt;;PROBPLOT &lt;variables&gt; &lt;/ options&gt;;QQPLOT &lt;variables&gt; &lt;/ options&gt;;VAR variables;WEIGHT\nvariable;\n\nThe PROC UNIVARIATE statement invokes the UNIVARIATE procedure, which provides detailed descriptive statistics, distributional summaries, and diagnostic plots for numerical variables.\nThe VAR Statement\n\nSpecifies the numeric variables to be analyzed.\nRequired if the OUTPUT statement is used.\nIf omitted, all numeric variables in the data set are analyzed.\n\nThe PLOT statement (CDFPLOT, HISTOGRAM, PPPLOT, PROBPLOT, and QQPLOT) create graphical displays\nthe INSET statement enhances these displays by adding a table of summary statistics directly on the graph.\n\nYou can specify one or more of each of the plot statements, the INSET statement, and the OUTPUT statement. If you use a VAR statement, the variables listed in a plot statement must be a subset of the variables listed in the VAR statement.\nYou can specify a BY statement to obtain separate analyses for each BY group. The FREQ statement specifies a variable whose values provide the frequency for each observation. The ID statement specifies one or more variables to identify the extreme observations. The WEIGHT statement specifies a variable whose values are used to weight certain statistics.\nYou can use a CLASS statement to specify one or two variables that group the data into classification levels. The analysis is carried out for each combination of levels in the input data set, or within each BY group if you also specify a BY statement. You can use the CLASS statement with plot statements to create comparative displays, in which each cell contains a plot for one combination of classification levels.\n\nWe revisit the court length example to demonstrate how one-sample nonparametric tests can be used as an alternative or complement to the one-sample t-test.\nSuppose that, for some reason, we believe the t-test may not be an appropriate choice‚Äîperhaps due to concerns about normality‚Äîor we simply wish to double-check our conclusions using nonparametric methods. In this case, we can apply the nonparametric procedures available in PROC UNIVARIATE.\nStep 1: Input the Data\nWe begin by entering the court length data into SAS.\nDATA time;\n    INPUT time @@;\n    DATALINES;\n43  90  84  87  116  95  86  99  93  92\n121 71  66  98  79  102 60  112 105 98\n;\nRUN;\nStep 2: Perform a One-Sample Nonparametric Test\nTo test whether the population median court length differs from 80 days, we use PROC UNIVARIATE with the MU0= option.\n/* Perform one-sample nonparametric test */\nPROC UNIVARIATE DATA=time MU0=80;\n    VAR time;\nRUN;\nStep 3: Interpret the Output\nFrom the output, SAS provides: + Test statistics + p-values for both nonparametric tests\nThese results allow us to assess whether there is statistical evidence that the population median court length differs from 80 days, without relying on the normality assumption required by the t-test.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>One Sample Nonparametric Test</span>"
    ]
  },
  {
    "objectID": "part2_3_1sample_nonparametric_test.html#discussion-one-sided-vs-two-sided-tests",
    "href": "part2_3_1sample_nonparametric_test.html#discussion-one-sided-vs-two-sided-tests",
    "title": "7¬† One Sample Nonparametric Test",
    "section": "7.3 Discussion: One-Sided vs Two-Sided Tests",
    "text": "7.3 Discussion: One-Sided vs Two-Sided Tests\nQuestion:\nIs this a one-sided or a two-sided test?\nIn this example, SAS reports only two-sided p-values by default for the nonparametric tests in PROC UNIVARIATE. If a one-sided test is desired, SAS does not directly provide the result.\nHowever, a simple (though not exact) workaround can be used to approximate the one-sided p-value from the two-sided p-value.\n\n7.3.1 Approximate One-Sided p-Value Calculation\n\nLet\n\\[\np^* = \\frac{\\text{two-sided p-value}}{2}.\n\\]\nThen proceed according to the alternative hypothesis:\n\n\n7.3.1.1 Case 1: Right-sided test\nTesting\n\\[\nH_1: \\mu &gt; 80 \\quad (\\text{or } \\mu &gt; \\mu_0)\n\\]\n\nIf the sample mean \\(\\bar{x} &gt; \\mu_0\\), then\n\\[\n\\text{one-sided p-value} = p^*.\n\\]\nIf the sample mean \\(\\bar{x} &lt; \\mu_0\\), then\n\\[\n\\text{one-sided p-value} = 1 - p^*.\n\\]\n\n\n\n7.3.1.2 Case 2: Left-sided test\nTesting\n\\[\nH_1: \\mu &lt; 80 \\quad (\\text{or } \\mu &lt; \\mu_0)\n\\]\n\nIf the sample mean \\(\\bar{x} &lt; \\mu_0\\), then\n\\[\n\\text{one-sided p-value} = p^*.\n\\]\nIf the sample mean \\(\\bar{x} &gt; \\mu_0\\), then\n\\[\n\\text{one-sided p-value} = 1 - p^*.\n\\]\n\n\n\n\n7.3.2 Why Does This Work?\nA two-sided p-value measures evidence in both directions away from the null hypothesis. Dividing it by two isolates the probability mass in one tail of the sampling distribution.\nHowever, this adjustment is only valid when: - The test statistic is symmetric under the null hypothesis - The observed statistic is in the direction specified by the alternative\nBecause these conditions are not always guaranteed for nonparametric tests, this method should be viewed as an approximation, not an exact one-sided test.\n\n\n\n\n\n\n\n\n\n\n\nKey takeaway:\nSAS nonparametric procedures default to two-sided inference. While approximate one-sided p-values can be obtained manually, interpretation should be done with care.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>One Sample Nonparametric Test</span>"
    ]
  },
  {
    "objectID": "part2_4_1sample_proportion_test.html",
    "href": "part2_4_1sample_proportion_test.html",
    "title": "8¬† One Sample Proportion Test",
    "section": "",
    "text": "8.1 Motivation\nSuppose we observe a categorical variable with two levels, such as\nIn this setting, we are often interested in making inference about the population proportion of interest, such as the proportion of people who smoke.\nWe denote this population proportion by \\(p\\).\nCommon inferential goals include:\nSpecifically, we may test\n\\[\n\\begin{aligned}\nH_0 &: p = p_0, \\\\[0.5em]\nH_1 &:\n\\begin{cases}\np \\neq p_0, & \\text{(two-sided)},\\\\\np &lt; p_0,    & \\text{(left-sided)},\\\\\np &gt; p_0,    & \\text{(right-sided)}.\n\\end{cases}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>One Sample Proportion Test</span>"
    ]
  },
  {
    "objectID": "part2_4_1sample_proportion_test.html#motivation",
    "href": "part2_4_1sample_proportion_test.html#motivation",
    "title": "8¬† One Sample Proportion Test",
    "section": "",
    "text": "SMOKES = 1: smoker\n\nSMOKES = 2: non-smoker\n\n\n\n\n\nEstimating the population proportion \\(p\\)\nConstructing a confidence interval for \\(p\\)\nPerforming a hypothesis test for \\(p\\)\n\n\n\n\n8.1.1 Sampling Distribution of the Sample Proportion\nLet \\(\\hat{p}\\) denote the sample proportion, computed as\n\\[\n\\hat{p} = \\frac{X}{n},\n\\]\nwhere\n\n\\(X\\) is the number of successes,\n\\(n\\) is the sample size.\n\nWhen the sample size is sufficiently large (typically \\(n \\ge 30\\)), the sampling distribution of \\(\\hat{p}\\) is approximately normal:\n\\[\n\\hat{p} \\sim N\\left(p, \\frac{p(1-p)}{n}\\right).\n\\]\nThis normal approximation forms the basis for both confidence intervals and hypothesis testing for a population proportion.\n\n\n8.1.2 Confidence Interval for a Proportion\nA \\((1-\\alpha)\\times 100\\%\\) confidence interval for \\(p\\) is given by\n\\[\n\\left[\n\\hat{p} - Z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\n\\quad\n\\hat{p} + Z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\right].\n\\]\nHere, \\(Z_{\\alpha/2}\\) is the standard normal quantile satisfying\n\\[\nP(Z \\ge Z_{\\alpha/2}) = \\alpha/2, \\quad Z \\sim N(0,1).\n\\] Moreover, this normal approximation of \\(\\hat{p}\\) is also used to perform hypothesis testing for \\(p\\).\n\n\n8.1.3 Standard Error of the Sample Proportion\nThe standard error of \\(\\hat{p}\\) is\n\\[\n\\text{SD}(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}.\n\\]\nThis expression highlights an important principle:\n\nThe uncertainty in an estimate decreases as the sample size increases.\n\n\n\n8.1.4 Interpretation: Effect of Sample Size\nConsider two examples where the estimated proportion is the same.\n\nA small college has 8 physics majors, and 5 of them graduate within four years.\n\\[\n\\hat{p} = \\frac{5}{8} = 0.625\n\\]\nThe standard error is\n\\[\n\\text{SE}(\\hat{p}) = \\sqrt{\\frac{0.6 \\times 0.4}{8}} \\approx 0.17.\n\\]\nAn English department graduates 50 out of 80 students.\n\\[\n\\hat{p} = \\frac{50}{80} = 0.625\n\\]\nThe standard error is\n\\[\n\\text{SE}(\\hat{p}) = \\sqrt{\\frac{0.6 \\times 0.4}{80}} \\approx 0.05.\n\\]\nDoes this make sense?\n\n\n\n\n\n\n\nNoteNote\n\n\n\nIn practice, when the normal approximation may not be reliable (e.g., small \\(n\\)), we will instead rely on exact binomial methods, which do not require large-sample assumptions.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>One Sample Proportion Test</span>"
    ]
  },
  {
    "objectID": "part2_4_1sample_proportion_test.html#computation",
    "href": "part2_4_1sample_proportion_test.html#computation",
    "title": "8¬† One Sample Proportion Test",
    "section": "8.2 Computation",
    "text": "8.2 Computation\nSuppose a college has six majors, labelled A, B, C, D, E, and F. For each major, we observe:\n+ the number of students who graduated within four years (Grads), and\n+ the total number of students in that major (Total).\nDATA Grads;\n  INPUT Major $ Grads Total @@;\n  DATALINES;\nA 10 22  B 10 32  C 17 25\nD  4  7  E  8 14  F 16 28\n;\nRUN;\n\nIt is easy to write a short DATA step to compute the empirical proportions and a 95% confidence interval for each major.\nDATA GradRate;\n  SET Grads;\n\n  /* Empirical proportion */\n  p = Grads / Total;\n\n  /* Standard error under normal approximation */\n  StdErr = SQRT(p * (1 - p) / Total);\n\n  /* 95% Wald confidence interval */\n  z = QUANTILE(\"normal\", 1 - 0.05/2);\n\n  LCL = MAX(0, p - z * StdErr);   /* Lower bound */\n  UCL = MIN(1, p + z * StdErr);   /* Upper bound */\n\n  LABEL p   = \"Proportion\"\n        LCL = \"Lower 95% CI\"\n        UCL = \"Upper 95% CI\";\nRUN;\n\nThe output shows that although majors D, E, and F have the same four-year graduation rate (57%), the estimate for the D group, which has only seven students, has twice as much variability as the estimate for the F group, which has four times as many students.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>One Sample Proportion Test</span>"
    ]
  },
  {
    "objectID": "part2_4_1sample_proportion_test.html#automating-the-computations-with-proc-freq",
    "href": "part2_4_1sample_proportion_test.html#automating-the-computations-with-proc-freq",
    "title": "8¬† One Sample Proportion Test",
    "section": "8.3 Automating the Computations with PROC FREQ",
    "text": "8.3 Automating the Computations with PROC FREQ\nIn the previous section, we computed the Wald confidence interval for each major using a DATA step. That approach is straightforward, but it becomes inconvenient if we want other binomial confidence intervals (e.g., exact/Clopper‚ÄìPearson, Wilson, etc.). A convenient alternative is to use PROC FREQ with the BINOMIAL option. However, PROC FREQ expects the data in an event / nonevent format (a binary outcome with a frequency count), rather than the events / trials format (Grads, Total) we currently have.\nSo we first convert each major into two rows: + Graduated=‚ÄúYes‚Äù with Count = Grads + Graduated=‚ÄúNo‚Äù with Count = Total - Grads\nStep 1: Convert events/trials into event/nonevent format\n/*--------------------------------------------------------------\n  Convert Event/Trials format (Grads, Total)\n  into Event/Nonevent format (Graduated, Count)\n--------------------------------------------------------------*/\ndata GradFreq;\n  set Grads;\n\n  Graduated = \"Yes\"; \n  Count = Grads;\n  output;\n\n  Graduated = \"No\";\n  Count = Total - Grads;\n  output;\nrun;\n\nproc print data=GradFreq;\nrun;\n\nStep 2: Run PROC FREQ to compute binomial CIs by major\n/*--------------------------------------------------------------\n  Use PROC FREQ to analyze each major separately and compute\n  binomial confidence intervals for Pr(Graduated=\"Yes\")\n--------------------------------------------------------------*/\nproc freq data=GradFreq noprint;\n  by notsorted Major;\n\n  tables Graduated / binomial(level=\"Yes\" CL=wald);\n  weight Count;\n\n  output out=FreqOut binomial;\nrun;\nStep 3: Print a clean summary table\nproc print data=FreqOut noobs label;\n  var Major N _BIN_ L_BIN U_BIN;\n\n  label _BIN_ = \"Proportion\"\n        L_BIN = \"Lower 95% CI\"\n        U_BIN = \"Upper 95% CI\";\nrun;",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>One Sample Proportion Test</span>"
    ]
  },
  {
    "objectID": "part2_4_1sample_proportion_test.html#visualization-of-binomial-proportion",
    "href": "part2_4_1sample_proportion_test.html#visualization-of-binomial-proportion",
    "title": "8¬† One Sample Proportion Test",
    "section": "8.4 Visualization of Binomial Proportion",
    "text": "8.4 Visualization of Binomial Proportion\nIt is often helpful to visualize estimated proportions together with their confidence intervals.\nWhen plotting several proportions on the same graph, it is a good idea to sort the groups in a meaningful way‚Äîmost commonly by the estimated proportion itself. If two groups have the same estimated proportion, the sample size can be used as a tie-breaker.\n\n8.4.1 Why visualization matters\nA single number rarely tells the whole story. Plotting proportions with confidence intervals allows us to:\n\nCompare graduation rates across majors at a glance\nAssess uncertainty in each estimate\nSee how sample size affects precision\n\nGroups with smaller sample sizes tend to have wider confidence intervals, reflecting greater uncertainty.\n\n\n8.4.2 Adding a reference line\nIt is often informative to add a reference line representing the overall proportion, regardless of group membership.\nFor the graduation data in this example, the overall proportion of students who graduate in four years is\n\\[\n\\hat p_{\\text{overall}} = \\frac{65}{128} \\approx 0.5078.\n\\]\nThis reference line helps contextualize each major‚Äôs graduation rate relative to the overall average.\n\nTip:\nYou can obtain the overall proportion in SAS by repeating the PROC FREQ analysis without a BY statement.\n\n\n\n8.4.3 Including sample size on the plot\nBecause uncertainty depends strongly on sample size, it is good practice to display the number of observations per group directly on the graph.\nIn SAS, this can be done using the YAXISTABLE statement, which adds a table aligned with the y-axis showing the sample size for each group.\n\n\n8.4.4 Interpreting the plot\nA typical plot of binomial proportions with confidence intervals shows:\n\nPoints: estimated proportions\n\nError bars: 95% confidence intervals\n\nVertical reference line: overall proportion\n\nTable entries: number of students in each major\n\nSuch a graph clearly illustrates that majors with fewer students (for example, very small departments) have much wider confidence intervals than majors with larger enrollments.\n\n\n8.4.5 Practical visualization advice\n\nIf there are 10 or more categories, consider using alternating background color bands to make it easier to associate confidence intervals with groups.\nAlways label axes clearly (e.g., Proportion on the x-axis).\nInclude the confidence level (e.g., ‚Äú95% CI‚Äù) in the caption or title.\n\n\n\n\n8.4.6 Summary\nThis section demonstrates how visualization complements numerical output:\n\nPROC FREQ provides estimated proportions and confidence intervals.\nGraphing proportions with confidence intervals makes comparisons intuitive.\nIncluding sample sizes helps readers understand why some intervals are wider than others.\n\nWell-designed graphics are one of the most effective ways to communicate results from binomial proportion analyses.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>One Sample Proportion Test</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html",
    "href": "part2_5_sas_marco.html",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "",
    "text": "9.1 Motivation\nIn many applications, we may care more about the population variance \\(\\sigma^2\\) than the population mean \\(\\mu\\). For instance, in a manufacturing process, a large variance in product measurements indicates unstable quality, even if the mean is acceptable. This is not the only instance, as there are many interesting scientific questions that involve the population variance both in academia and in industry.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#motivation",
    "href": "part2_5_sas_marco.html#motivation",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "",
    "text": "9.1.1 Examples\n\nA SCUBA instructor records the dive depths of students during checkout. Although everyone should be at the same depth, the instructor is interested in how much the depths vary.\nThe instructor believes the standard deviation is 3 feet, while the assistant thinks it is less than 3 feet.\nThe hypotheses are:\n\\[\n  H_0 : \\sigma^2 = 3^2\n  \\qquad \\text{vs} \\qquad\n  H_1 : \\sigma^2 &lt; 3^2\n  \\]\n\n\nA company produces metal pipes of a standard length. Twenty years ago, the pipe lengths were normally distributed with standard deviation 1.1 cm.\nThe company now tests a random sample of 30 pipes and wants to construct a 95% confidence interval for \\(\\sigma^2\\) to determine whether the production quality has changed.\n\nFor the examples above, the quantity of the interest is the population variance. But we need something to estiamte it. Let \\(s^2\\) denote the sample variance. Then the pivotal quantity\n\\[\n\\frac{(n-1)s^2}{\\sigma^2},\n\\]\nwhich follows a chi-square distribution:\n\\[\n\\frac{(n-1)s^2}{\\sigma^2} \\sim \\chi^2_{n-1},\n\\] where\n\n\\(n\\) : sample size\n\n\\(s^2\\) : sample variance\n\n\\(\\sigma^2\\) : population variance\n\nYou may think that \\(s\\) as the randomvariable in this test. The distribution is called chi-square distribution which depends on a parameter named the degrees of freedom denoted by \\(\\mathrm{df}=n-1\\). A test of a single variance may be right-tailed, left-tailed or two-sided. This chi-square result is the foundation for both confidence intervals and hypothesis tests for \\(\\sigma^2\\).\n\n\n\n\n\n\nNoteNote\n\n\n\nA \\((1-\\alpha)\\times 100\\%\\) confidence interval for the population variance is\n\\[\n\\left(\n\\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2,\\;n-1}},\n\\;\n\\frac{(n-1)s^2}{\\chi^2_{\\alpha/2,\\;n-1}}\n\\right),\n\\]\nwhere \\(\\chi^2_{p,\\;n-1}\\) denotes the \\(p\\)-th quantile of the \\(\\chi^2_{n-1}\\) distribution.\nI won‚Äôt expect you to know the expression here for now.\n\n\n\nProblem\nUnfortunately, if you check the built-in SAS procedures, you will find that there is no basic procedure that directly handles the one-sample variance inference problem.\nThe good news is that SAS provides a powerful MACRO programming language, which allows us to write our own SAS procedures.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#sas-macro-program",
    "href": "part2_5_sas_marco.html#sas-macro-program",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "9.2 SAS Macro Program",
    "text": "9.2 SAS Macro Program\nWhen you write a program that will be run repeatedly, you may want to seriously consider using macros, because:\n\nMARCROS allow centralized changes:\nYou can make a change in one location, and SAS will cascade that change throughout your program.\nMACROS promote code reuse:\nYou can write a section of code once and reuse it many times, either within the same program or across different programs.\nMARCROS enable data-driven programming:\nSAS can decide what actions to take based on actual data values.\n\nIn general, macro code takes longer to write and debug than standard SAS code. Therefore, macros are usually not recommended for programs that will only be run a few times.\nHowever, if you find yourself writing similar code repeatedly, macros can significantly improve efficiency and reduce errors.\nMacros can help in several ways:\n\nYou can make a single small change, and SAS will propagate that change throughout your program.\nYou can reuse code blocks, avoiding duplication.\nYou can build data-driven programs, allowing SAS to adapt automatically to different datasets or inputs.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#how-macros-work",
    "href": "part2_5_sas_marco.html#how-macros-work",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "9.3 How Macros Work",
    "text": "9.3 How Macros Work\nWhen you submit a standard SAS program, SAS compiles and executes it immediately. When you write macro code, there is an additional step:\n\nSAS first sends MARCO statements to the macro processor.\nThe macro processor resolves the macro code, generating standard SAS code.\nSAS then compiles and executes the generated code.\n\nBecause you are writing a program that writes another program, this process is sometimes referred to as meta-programming.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#designing-your-own-macros",
    "href": "part2_5_sas_marco.html#designing-your-own-macros",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "9.4 Designing Your Own Macros",
    "text": "9.4 Designing Your Own Macros\nTo design effective SAS macros, we need to understand two fundamental concepts:\n\n9.4.1 Macros vs.¬†Macro Variables\nAs you construct macro programs, you will work with two basic building blocks:\n\nMACROS v.s.\nMACRO variables\n\nTo design your own SAS macros, we need to clearly distinguish between macros and macro variables. These are the two basic building blocks of SAS macro programming.\nConceptually, the workflow looks like this:\n\nYou write macro code\nThe macro processor resolves it\nThe output is ordinary SAS code, which is then compiled and executed\n\nNaming Conventions\nYou can tell macros and macro variables apart by their prefixes:\n\nMacro variables start with an ampersand: &\nMacros start with a percent sign: %\n\nExamples:\n\n&alpha, &n\n%myMacro, %DO, %IF\n\nMacro Variables\nA macro variable is similar to a data variable, but with important differences:\n\nIt does not belong to a dataset\nIt has only one value\nThat value is always character\nThe value is substituted directly into your program\n\nA macro variable‚Äôs value can be:\n\nA variable name\nA number\nText\nAny string you want substituted into your SAS code\n\nMacro\nA macro is a larger unit of code that can contain:\n\nDATA steps\nPROC steps\nMacro logic such as\n%IF‚Äì%THEN‚Äì%ELSE, %DO‚Äì%END\n\nMacros often‚Äîbut not always‚Äîuse macro variables.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#think-globally-and-locally-scope-of-macro-variables",
    "href": "part2_5_sas_marco.html#think-globally-and-locally-scope-of-macro-variables",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "9.5 Think Globally and Locally: Scope of Macro Variables",
    "text": "9.5 Think Globally and Locally: Scope of Macro Variables\nMacro variables come in two scopes:\n\nLocal macro variables\nGlobal macro variables\n\n\n9.5.1 Local Macro Variables\n\nDefined inside a macro\nExist only within that macro\nCannot be referenced outside the macro\n\n\n\n9.5.2 Global Macro Variables\n\nDefined in open code (outside any macro)\nCan be used anywhere in the program\n\n\n\n9.5.3 Common Mistakes to Avoid\n\nUsing a local macro variable outside its macro\nAccidentally creating both local and global macro variables with the same name\n\nKeeping scope in mind will save you a lot of debugging time.\n\n\n\n\n\n\nNote\n\n\n\nThe macro processor does not resolve macro references inside single quotes.\n\n‚ùå '&var' ‚Üí macro not resolved\n‚úÖ \"&var\" ‚Üí macro resolved\n\nRule of thumb:\n\nUse double quotes when quoted strings contain macro variables.\n\n\n\nSubstituting Text with %LET\nThe %LET statement assigns a value to a macro variable.\n%LET alpha = 0.05;\n\n%MACRO macro_name(param1, param2, ..., paramk);\n   /* macro code */\n%MEND macro_name;",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#writing-sas-macros-with-car-dataset",
    "href": "part2_5_sas_marco.html#writing-sas-macros-with-car-dataset",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "9.6 Writing SAS Macros with Car dataset",
    "text": "9.6 Writing SAS Macros with Car dataset\nBefore writing our own macros, let us first understand how macro variables work through a simple example.\n\nSAS provides a built-in global macro variable called SYSDATE, which represents the system date.\nSuppose we want to print the system date automatically in the title of a SAS report every time the report is generated. The advantage is that we do not need to manually update the date in the code. We use the built-in SAS dataset CARS from the SASHELP library.\nPROC PRINT DATA=SASHELP.CARS;\n  WHERE MAKE = 'Audi' AND TYPE = 'Sports';\n  TITLE \"Sales as of &SYSDAY &SYSDATE\";\nRUN;\n\nSo far, we have used macro variables to make our SAS programs more flexible.\nNow we take one more step forward and define a macro program, which is a reusable block of SAS code. Macro variables are referenced in SAS statements using the ampersand (&) character.\nNow suppose we want our program to be more flexible. Instead of hard-coding the car MAKE and TYPE, we define macro variables so that we can easily change them without modifying multiple lines of code.\n%LET MAKE_NAME = Audi;\n%LET TYPE_NAME = Sports;\n\nPROC PRINT DATA=SASHELP.CARS;\n  WHERE MAKE = \"&MAKE_NAME\" AND TYPE = \"&TYPE_NAME\";\n  TITLE \"Sales as of &SYSDAY &SYSDATE\";\nRUN;\n\nReusing the Same Program with Different Values\nNow, suppose we want to view Audi vehicles of type Wagon. We only need to change one line of code.\n/* Change only the macro variable values */\n%LET MAKE_NAME = 'Audi';\n%LET TYPE_NAME = 'Wagon';\n\n/* The SAS code below remains unchanged */\nPROC PRINT DATA = SASHELP.CARS;\n    WHERE MAKE = &MAKE_NAME AND TYPE = &TYPE_NAME;\n    TITLE \"Sales as of &SYSDAY &SYSDATE\";\nRUN;\n\nMore on Macro program\nThe following program defines a macro called show_result.\nThis macro takes two arguments:\n\nmake_: the car manufacturer\ntype_: the vehicle type\n\n%MACRO show_result(make_, type_);\n\nPROC PRINT DATA=SASHELP.CARS;\n  WHERE MAKE = \"&make_\" AND TYPE = \"&type_\";\n  TITLE \"Sales as of &SYSDAY &SYSDATE\";\nRUN;\n\n%MEND;\nCalling a Macro Once a macro is defined, it can be called just like a function.\n%show_result(BMW, SUV);\n\n\nSo after seeing the first example, we have a more clear sense about why Use Macro Programs?\nMacro programs allow you to:\n\nAvoid rewriting the same code repeatedly\nCentralize logic in one place\nMake programs easier to maintain and extend\nWrite data-driven and parameterized SAS programs\n\nThis idea becomes especially powerful when we need to repeat the same analysis for many datasets or parameter values.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_5_sas_marco.html#revisit-the-one-sample-variance-test",
    "href": "part2_5_sas_marco.html#revisit-the-one-sample-variance-test",
    "title": "9¬† SAS Macro Program in One Sample Variance Problem",
    "section": "9.7 Revisit the One-Sample Variance Test",
    "text": "9.7 Revisit the One-Sample Variance Test\nAs discussed earlier, SAS does NOT provide a built-in procedure for conducting one-sample variance inference (confidence intervals or hypothesis tests). However, SAS allows us to overcome this limitation by writing our own macro procedures. Fortunately, a one-sample variance inference macro is commonly used and can be written in a general form. In this course, we will use a macro named vartest.sas, which is shared in the iCollege and below.\n%macro vartest(version,\n                 data= _last_  ,\n                 var=         ,\n                 alpha=  0.05   ,\n                 var0=,\n                 sigma0=\n              );\n\n%if &version ne %then %put VARTEST macro Version 1.1;\n%let opts = %sysfunc(getoption(notes))\n            _last_=%sysfunc(getoption(_last_));\n%if &data=_last_ %then %let data=&syslast;\noptions nonotes;\n\n%if &sigma0=0 or &var0=0 %then %do;\n  %put ERROR: The null hypothesis variance (VAR0=) or standard deviation;\n  %put %str(      ) (SIGMA0=) must be greater than zero.;\n  %goto exit;\n%end;\n\nproc summary data=&data;\n  var &var;\n  output out=_varn(keep=var n std) n=n var=var std=std;\nrun;\n\ndata _vartest;\n  set _varn;\n  df=n-1;\n  %if &sigma0 ne %then %str(nullvar=&sigma0 * &sigma0;);\n  %if &var0 ne %then %str(nullvar=&var0;);\n  level=(1-&alpha)*100;\n  call symput('level',trim(left(level)));\n  %if &sigma0 ne or &var0 ne %then %do;\n    chisq=((df)*(var))/nullvar;\n    pvalue=1-probchi(chisq,df);\n  %end;\n  uclvar=df*var/cinv(&alpha/2,df); uclstd=sqrt(uclvar);\n  lclvar=df*var/cinv(1-&alpha/2,df); lclstd=sqrt(lclvar);\n  label var=\"Sample Variance\" std=\"Sample Std Dev\" \n        chisq=\"Chi-Square\" pvalue=\"Prob&gt;Chi-Square\"\n        level=\"Confidence Level (%)\"\n        uclvar=\"UCL for Variance\" lclvar=\"LCL for Variance\"\n        uclstd=\"UCL for Std Dev\" lclstd=\"LCL for Std Dev\";\n  drop df  %if &sigma0 ne or &var0 ne %then nullvar;  ;\nrun;\n\ndata _ci;\n  set _vartest;\n  stat=\"Variance\"; estimate=var; lcl=lclvar; ucl=uclvar; output;\n  stat=\"Std Dev\";  estimate=std; lcl=lclstd; ucl=uclstd; output;\n  label stat=\"Statistic\"\n        estimate=\"Estimate\"\n        lcl=\"Lower Confidence Limit\"\n        ucl=\"Upper Confidence Limit\";\n  run;\n\nproc print data=_ci label noobs;\n  var stat estimate lcl ucl;\n  title \"&level.% Confidence Intervals for Variance and Std Dev\";\n  run;\n\n%if &sigma0 ne or &var0 ne %then %do;\nproc print data=_vartest label noobs;\n  var chisq pvalue;\n  format pvalue 6.4;\n  title  \"Hypothesis test for\";\n  title2;\n %if &var0 ne %then %do;\n  title3 \"H0: Variance(%upcase(&var)) = &var0\";\n  title4 \"Ha: Variance(%upcase(&var)) &gt; &var0\";\n %end;\n %if &sigma0 ne %then %do;\n  title3 \"H0: StdDev(%upcase(&var)) = &sigma0\";\n  title4 \"Ha: StdDev(%upcase(&var)) &gt; &sigma0\";\n %end;\nrun;\n%end;\n\n%exit:\ntitle;\noptions &opts;\n%mend;\nWith this Marcro, we can implement it to construct the hypothesis testing or confidence interval for population variance \\(\\sigma^2\\).\n\n/* Create a sample data set */\nDATA DAT;\n    INPUT X @@;\n    DATALINES;\n6.2 1.9 4.4 4.9 3.5\n4.6 4.2 1.1 1.3 4.8\n4.1 3.7 2.5 3.7 4.2\n1.4 3.2 2.6 1.5 3.9\n;\nRUN;\n\n/* Display the data */\nPROC PRINT DATA = DAT;\nRUN;\nThis dataset includes 20 measurements of the length of a product from a manufacturing process. Again, our goal is to conduct statistical inference on the population variance of the product length. Specifically, we want to test whether the population variance is greater than 2.25.\nDATA DAT;\n    INPUT X @@;\n    DATALINES;\n6.2 1.9 4.4 4.9 3.5\n4.6 4.2 1.1 1.3 4.8\n4.1 3.7 2.5 3.7 4.2\n1.4 3.2 2.6 1.5 3.9\n;\nRUN;\n\nPROC PRINT DATA = DAT;\nRUN;\n\n9.7.1 Using the VARTST Macro in SAS\nOnce the dataset has been created in SAS and the macro has been saved in a file with a known path, we can use the following code to conduct the one-sample variance test and construct the corresponding confidence intervals.\nUse the %INCLUDE statement to load the macro into your SAS session:\n\n/* When using SAS Windows */ \n%INCLUDE \n\"C:/Users/cyeh/Desktop/Teaching/GSU/STAT8678-SAS/code/L10/vartest.sas\";\n\n/* in SAS studio */ \n%INCLUDE\n\"/home/u64378616/chikuang/Part2/L10_Marco/vartest.sas\";\n\n%VARTST(\n    DATA  = DAT,\n    VAR   = X,\n    ALPHA = 0.05,\n    VAR0  = 2.25\n);\n\n\n\n\nQuestion for you:\n\nHow would you modify the macro call if you want to test whether the population standard deviation is greater than 1.5 instead of testing the population variance?\n\n\n\n\n\n\n\nNoteAnswer with Explanation\n\n\n\nTo calculate the p-value for \\(H_1:\\sigma^2 &lt; \\sigma_0^2\\) and \\(H_1:\\sigma^2\\ne \\sigma_0^2\\) from the p-value for \\(H_1: \\sigma^2 &gt; \\sigma_0^2\\). We can do the following:\nLet \\(p\\) be the p-value for the right-tailed test \\(H_1: \\sigma^2 &gt; \\sigma_0^2\\).\n\nFor the left-tailed test \\(H_1: \\sigma^2 &lt; \\sigma_0^2\\), the p-value is \\(1-p\\).\nFor the two-sided test \\(H_1: \\sigma^2 \\ne \\sigma_0^2\\), the p-value is \\(\\min\\{p, 1-p\\} \\times 2\\).",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>SAS Macro Program in One Sample Variance Problem</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html",
    "href": "part2_6_chi-square.html",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "",
    "text": "10.1 Motivation: What Is a Goodness-of-Fit Test?\nA goodness-of-fit test, in general, refers to measuring how well the observed data correspond to a fitted (assumed) model.\nWe will use this concept throughout the course as a way of checking model fit.\nSimilar to linear regression, in essence, a goodness-of-fit test compares:\nHypotheses for a Goodness-of-Fit Test\nA goodness-of-fit test examines the following hypotheses:\n\\[\nH_0 : \\text{the model } M_0 \\text{ fits}\n\\]\nversus\n\\[\nH_1 : \\text{the model } M_0 \\text{ does not fit}.\n\\]",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#motivation-what-is-a-goodness-of-fit-test",
    "href": "part2_6_chi-square.html#motivation-what-is-a-goodness-of-fit-test",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "",
    "text": "Observed values, and\n\nExpected (fitted or predicted) values under a specified model.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#example-categorical-data-dice-example",
    "href": "part2_6_chi-square.html#example-categorical-data-dice-example",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.2 Example: Categorical Data (Dice Example)",
    "text": "10.2 Example: Categorical Data (Dice Example)\nConsider the simplest example of a goodness-of-fit test with categorical data.\nSuppose we want to test whether a fair six-sided die has equal probability for each face.\nWe compare the observed frequencies to those expected under the assumed model:\n\\[\nX \\sim \\text{Multinomial}\\left(n = 30,\\; \\pi_0 = \\left(\\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}, \\tfrac{1}{6}\\right)\\right).\n\\]\nIn this setting, the goodness-of-fit test asks whether the probability in each cell is equal to the specified value \\(\\pi_0\\).",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#hypotheses-in-probability-vector-form",
    "href": "part2_6_chi-square.html#hypotheses-in-probability-vector-form",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.3 Hypotheses in Probability Vector Form",
    "text": "10.3 Hypotheses in Probability Vector Form\nThe hypotheses can be written as:\n\\[\nH_0 : \\boldsymbol{\\pi} = \\boldsymbol{\\pi}_0\n\\]\nversus\n\\[\nH_1 : \\boldsymbol{\\pi} \\neq \\boldsymbol{\\pi}_0.\n\\]",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#one-way-contingency-table",
    "href": "part2_6_chi-square.html#one-way-contingency-table",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.4 One-Way Contingency Table",
    "text": "10.4 One-Way Contingency Table\nIn such problems, the data can be summarized using a one-way contingency table:\n\n\n\n\n\n\n\n\n\n\n\n\nCategories\n1\n2\n3\n4\n5\n6\n\n\n\n\nNumber of Observations\n\\(O_1\\)\n\\(O_2\\)\n\\(O_3\\)\n\\(O_4\\)\n\\(O_5\\)\n\\(O_6\\)\n\n\n\nHere,\n\n\\(O_1, O_2, \\dots, O_6\\) are observed counts from your dataset.\nThe expected counts are computed from the assumed probability model.\n\nThese observed and expected counts form the basis of the \\(\\chi^2\\) goodness-of-fit test.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#one-way-contingency-table-and-model-fit",
    "href": "part2_6_chi-square.html#one-way-contingency-table-and-model-fit",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.5 One-Way Contingency Table and Model Fit",
    "text": "10.5 One-Way Contingency Table and Model Fit\n\n\n\n\n\n\n\n\n\n\n\n\nCategories\n1\n2\n3\n4\n5\n6\n\n\n\n\nNumber of Observations\n\\(O_1\\)\n\\(O_2\\)\n\\(O_3\\)\n\\(O_4\\)\n\\(O_5\\)\n\\(O_6\\)\n\n\nExpected Observations\n\\(n\\pi_1 = 5\\)\n\\(5\\)\n\\(5\\)\n\\(5\\)\n\\(5\\)\n\\(5\\)\n\n\n\nIn the setting of one-way contingency tables, we measure how well an observed variable \\(X\\) corresponds to a model specified by a vector of cell probabilities.\nIn other words, under the null hypothesis, we assume the data come from a particular distribution, and we test whether this assumed model fits the data well when compared with the saturated model (the model that fits the data perfectly).\nThe rationale behind model fitting is the assumption that a complex data-generating mechanism can be reasonably approximated by a simpler model. The goodness-of-fit test is applied to assess whether this assumption is supported by the data.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#test-statistics",
    "href": "part2_6_chi-square.html#test-statistics",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.6 Test Statistics",
    "text": "10.6 Test Statistics\nThere are two commonly used test statistics for the \\(\\chi^2\\) goodness-of-fit test.\nPearson Goodness-of-Fit Test Statistic\nThe Pearson goodness-of-fit statistic is defined as\n\\[\nX^2 = \\sum_{j=1}^{k} \\frac{(X_j - n\\pi_{0j})^2}{n\\pi_{0j}}.\n\\]\nAn easy way to remember this formula is\n\\[\nX^2 = \\sum_{j=1}^{k} \\frac{(O_j - E_j)^2}{E_j},\n\\]\nwhere\n\n\\(O_j = X_j\\) is the observed count in cell \\(j\\), and\n\n\\(E_j = \\mathbb{E}(X_j) = n\\pi_{0j}\\) is the expected count in cell \\(j\\) under the null hypothesis.\n\n\n10.6.1 1.2 Deviance Test Statistic\nThe deviance statistic is defined as\n\\[\nG^2\n= 2 \\sum_{j=1}^{k} X_j \\log\\!\\left(\\frac{X_j}{n\\pi_{0j}}\\right)\n= 2 \\sum_{j=1}^{k} O_j \\log\\!\\left(\\frac{O_j}{E_j}\\right).\n\\]\nIn some texts, \\(G^2\\) is also referred to as the likelihood ratio test (LRT) statistic, which compares the log-likelihoods of two models:\n\n\\(L_0\\): the reduced model under \\(H_0\\), and\n\n\\(L_1\\): the full (saturated) model under \\(H_A\\).\n\n\\[\nG^2\n=\n-2 \\log\\!\\left(\\frac{\\ell_0}{\\ell_1}\\right)\n=\n-2 \\left( L_0 - L_1 \\right)\n\\]\nNote that \\(X^2\\) and \\(G^2\\) are both functions of the observed data \\(X\\) and a vector of cell probabilities \\(\\pi_0\\). For this reason, we will sometimes write them as \\(X^2(x, \\pi_0)\\) and \\(G^2(x, \\pi_0)\\), respectively. When there is no ambiguity, we will simply use \\(X^2\\) and \\(G^2\\).\nWe will encounter these statistics repeatedly throughout the course, particularly in the analysis of two-way and \\(k\\)-way contingency tables, and when assessing the fit of log-linear and logistic regression models.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#how-do-they-work",
    "href": "part2_6_chi-square.html#how-do-they-work",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.7 2. How Do They Work?",
    "text": "10.7 2. How Do They Work?\nBoth \\(X^2\\) and \\(G^2\\) measure how closely the assumed model‚Äîhere, a multinomial model \\(\\text{Mult}(n, \\pi_0)\\)‚Äîfits the observed data.\nWhen the null hypothesis \\(H_0\\) is true, both statistics have an approximate chi-square distribution with \\(k - 1\\) degrees of freedom. This allows us to use the chi-square distribution to obtain critical values and \\(p\\)-values for hypothesis testing.\n\nIf the sample proportions \\(\\hat{\\pi}_j\\) (i.e., the saturated model) are exactly equal to the model probabilities \\(\\pi_{0j}\\) for all cells \\(j = 1, 2, \\ldots, k\\), then \\(O_j = E_j\\) for all \\(j\\), and both \\(X^2\\) and \\(G^2\\) are equal to zero. In this case, the model fits the data perfectly.\nIf the sample proportions \\(\\hat{\\pi}_j\\) deviate from the model probabilities \\(\\pi_{0j}\\), then both \\(X^2\\) and \\(G^2\\) are positive. Large values of \\(X^2\\) and \\(G^2\\) indicate that the data do not agree well with the assumed model \\(M_0\\).",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#dice-rolls-example",
    "href": "part2_6_chi-square.html#dice-rolls-example",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.8 Dice Rolls Example",
    "text": "10.8 Dice Rolls Example\n\nSuppose that we roll a die 30 times and observe the following table showing the number of times each face appears.\n\n\n\n\n\n\n\n\n\n\n\n\nCategories\n1\n2\n3\n4\n5\n6\n\n\n\n\nNumber of Observations\n\\(O_1 = 3\\)\n\\(O_2 = 7\\)\n\\(O_3 = 5\\)\n\\(O_4 = 10\\)\n\\(O_5 = 2\\)\n\\(O_6 = 3\\)\n\n\nExpected Observations\n\n\n\n\n\n\n\n\n\nWe want to test the null hypothesis that the die is fair.\nUnder this hypothesis, \\[\nX \\sim \\text{Mult}(n = 30, \\pi_0),\n\\] where \\[\n\\pi_{0j} = \\frac{1}{6}, \\quad j = 1, \\ldots, 6.\n\\]\nThis is our assumed model. Under \\(H_0\\), the expected counts are \\[\nE_j = \\frac{30}{6} = 5 \\quad \\text{for each cell}.\n\\]\nWe now have all the quantities needed to compute the goodness-of-fit statistics.\nWe now compute the two goodness-of-fit statistics explicitly.\n\n10.8.1 Pearson chi-square statistic\n\\[\n\\begin{aligned}\nX^2\n&= \\frac{(3-5)^2}{5}\n+ \\frac{(7-5)^2}{5}\n+ \\frac{(5-5)^2}{5}\n+ \\frac{(10-5)^2}{5}\n+ \\frac{(2-5)^2}{5}\n+ \\frac{(3-5)^2}{5} \\\\\n&= 9.2\n\\end{aligned}\n\\]\n\n\n10.8.2 Deviance (likelihood-ratio) statistic\n\\[\n\\begin{aligned}\nG^2\n&= 2\\Bigg(\n3 \\log\\frac{3}{5}\n+ 7 \\log\\frac{7}{5}\n+ 5 \\log\\frac{5}{5} \\\\\n&\\qquad\\quad\n+ 10 \\log\\frac{10}{5}\n+ 2 \\log\\frac{2}{5}\n+ 3 \\log\\frac{3}{5}\n\\Bigg) \\\\\n&= 8.8\n\\end{aligned}\n\\]\nNote that although \\(X^2\\) and \\(G^2\\) have the same approximate chi-square distribution, their realized numerical values need not be identical.\nThe corresponding \\(p\\)-values are\n\\[\nP(\\chi^2_5 \\ge 9.2) = 0.10,\n\\qquad\nP(\\chi^2_5 \\ge 8.8) = 0.12.\n\\]\nGiven a significance level of \\(\\alpha = 0.05\\), we fail to reject the null hypothesis.\nImportantly, failing to reject \\(H_0\\) does not mean that the null hypothesis is true. Rather, it means that we do not have sufficient evidence to conclude that it is false.\nIn this example, the fair-die model does not fit the data exactly, but the lack of fit is not large enough to conclude that the die is unfair at the 5% significance level.\nThe following SAS code implements the chi-square goodness-of-fit test for the dice example.\n\n\n10.8.3 SAS code: Chi-square goodness-of-fit test\nDATA DIE;\n    INPUT FACE $ COUNT;\n    DATALINES;\n1 3\n2 7\n3 5\n4 10\n5 2\n6 3\n;\nRUN;\n\nPROC FREQ DATA=DIE;\n    WEIGHT COUNT;\n    TABLES FACE / CHISQ;\n    /* \n    TABLES FACE / NOCUM ALL \n        CHISQ TESTP=(16.67 16.67 16.67 16.67 16.67 16.67);\n    */\nRUN;\n \nThe PROC FREQ step computes the Pearson chi-square statistic for testing whether the die is fair. The commented TESTP= option shows how expected probabilities can be specified explicitly.\nComputing residuals and goodness-of-fit statistics manually\nWe now compute Pearson residuals, deviance residuals, and the test statistics \\(X^2\\) and \\(G^2\\) directly from the data.\nDATA CAL;\n    SET DIE;\n\n    PI = 1/6;\n    ECOUNT = 30 * PI;\n\n    /* Pearson residual */\n    RES = (COUNT - ECOUNT) / SQRT(ECOUNT);\n\n    /* Deviance residual */\n    DEVRES = SQRT(ABS(2 * COUNT * LOG(COUNT / ECOUNT)))\n             * SIGN(COUNT - ECOUNT);\nRUN;\n\nPROC PRINT DATA=CAL;\nRUN;\n \nComputing test statistics and p-values\nPROC SQL;\n    SELECT \n        SUM((COUNT - ECOUNT)**2 / ECOUNT) AS X2,\n        1 - PROBCHI(CALCULATED X2, 5)     AS PVALUE_X2,\n        2 * SUM(COUNT * LOG(COUNT / ECOUNT)) AS G2,\n        1 - PROBCHI(CALCULATED G2, 5)     AS PVALUE_G2\n    FROM CAL;\nQUIT;\n\nNote that PROC FREQ automatically reports the Pearson chi-square statistic, but does not directly report the deviance statistic. The deviance statistic \\(G^2\\) must be computed manually as shown above.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_6_chi-square.html#tomato-phenotypes-example",
    "href": "part2_6_chi-square.html#tomato-phenotypes-example",
    "title": "10¬† \\(\\chi^2\\) Goodness of Fit Test",
    "section": "10.9 Tomato Phenotypes Example",
    "text": "10.9 Tomato Phenotypes Example\n\nTall cut-leaf tomatoes were crossed with dwarf potato-leaf tomatoes, and\n\\(n = 1611\\) offspring were classified by their phenotypes as summarized in the table below.\n\n\n\n\n\n\n\n\n\n\nCategories\ntall cut-leaf\ntall potato-leaf\ndwarf cut-leaf\ndwarf potato-leaf\n\n\n\n\nNumber of Observations\n\\(O_1 = 926\\)\n\\(O_2 = 288\\)\n\\(O_3 = 293\\)\n\\(O_4 = 104\\)\n\n\nExpected Observation\n\n\n\n\n\n\n\nGenetic theory predicts that the four phenotypes should occur with relative frequencies\n\\[\n9 : 3 : 3 : 1,\n\\] which implies that the phenotypes are not equally likely. In particular, the dwarf potato-leaf phenotype is expected to be the least frequent.\nWe would like to test whether the observed data are consistent with this genetic theory.\n\n10.9.1 Null hypothesis\nUnder the null hypothesis, the cell probabilities are\n\\[\n\\pi_1 = \\frac{9}{16}, \\quad\n\\pi_2 = \\pi_3 = \\frac{3}{16}, \\quad\n\\pi_4 = \\frac{1}{16}.\n\\]\n\n\n10.9.2 Expected frequencies\nThe expected counts under \\(H_0\\) are\n\\[\n\\begin{aligned}\nE_1 &= 1611 \\times \\frac{9}{16} = 906.2, \\\\\nE_2 &= E_3 = 1611 \\times \\frac{3}{16} = 302.1, \\\\\nE_4 &= 1611 \\times \\frac{1}{16} = 100.7.\n\\end{aligned}\n\\]\n\n\n10.9.3 Goodness-of-fit results\nUsing these expected counts, we compute the goodness-of-fit statistics and obtain\n\\[\nX^2 = 1.47, \\qquad G^2 = 1.48.\n\\]\nThe two statistics are nearly identical. Under the chi-square distribution with\n\\(k - 1 = 3\\) degrees of freedom, the corresponding \\(p\\)-values are approximately\n\\[\np \\approx 0.69.\n\\]\n\nSince the \\(p\\)-value is much larger than the significance level \\(\\alpha = 0.05\\), we fail to reject the null hypothesis. The observed data are therefore consistent with the genetic theory predicting a \\(9:3:3:1\\) ratio.\n\n\n10.9.3.1 SAS Code: Goodness-of-Fit Test\nStep 1: Enter the observed counts\nDATA LEAF;\n    INPUT TYPE $ COUNT;\n    DATALINES;\ntallc   926\ntallp   288\ndwarfc  293\ndwarfp  104\n;\nRUN;\n\nStep 2: Pearson chi-square goodness-of-fit test\nPROC FREQ DATA=LEAF ORDER=DATA;\n    WEIGHT COUNT;\n    TABLES TYPE / ALL CHISQ\n        TESTP=(56.25 18.75 18.75 6.25);\nRUN;\nThe proportions (56.25, 18.75, 18.75, 6.25) correspond to the theoretical probabilities \\((9,3,3,1)/16\\).\nManual Computation of Residuals, \\(X^2\\), and \\(G^2\\)\n\nStep 3: Define theoretical probabilities\nDATA PI;\n    INPUT PI;\n    PI = PI / 16;\nCARDS;\n9\n3\n3\n1\n;\nRUN;\n\nStep 4: Compute expected counts and residuals\nDATA CAL;\n    MERGE LEAF PI;\n    ECOUNT = 1611 * PI;\n\n    /* Pearson residual */\n    RES = (COUNT - ECOUNT) / SQRT(ECOUNT);\n\n    /* Deviance residual */\n    DEVRES = SQRT(ABS(2 * COUNT * LOG(COUNT / ECOUNT)))\n             * SIGN(COUNT - ECOUNT);\nRUN;\n\nPROC PRINT DATA=CAL;\nRUN;\n\nStep 5: Compute \\(X^2\\) and \\(G^2\\) explicitly\nPROC SQL;\n    SELECT\n        SUM((COUNT - ECOUNT)**2 / ECOUNT) AS X2,\n        1 - PROBCHI(CALCULATED X2, 3)     AS PVAL1,\n        2 * SUM(COUNT * LOG(COUNT / ECOUNT)) AS G2,\n        1 - PROBCHI(CALCULATED G2, 3)     AS PVAL2\n    FROM CAL;\nQUIT;\n\nVisualization: Observed vs Expected Counts\nGOPTIONS RESET=ALL;\nSYMBOL1 V=CIRCLE C=BLUE I=NONE;\nSYMBOL2 V=CIRCLE C=RED  I=NONE;\n\nLEGEND1 LABEL=NONE\n        VALUE=('Observed Count' 'Expected Count')\n        ACROSS=1\n        POSITION=(TOP RIGHT INSIDE)\n        MODE=PROTECT;\n\nTITLE \"Tomato Phenotypes\";\n\nPROC GPLOT DATA=CAL;\n    PLOT COUNT*TYPE ECOUNT*TYPE / OVERLAY LEGEND=LEGEND1;\nRUN;\nQUIT;\n\nTITLE;\n\nInterpretation\n\nSmall values of \\(X^2\\) and \\(G^2\\) indicate good agreement with the model\nLarge \\(p\\)-values confirm no strong evidence against the \\(9:3:3:1\\) ratio\nBoth Pearson and deviance statistics lead to the same conclusion\n\nThis example illustrates how goodness-of-fit testing connects theory, computation, and visualization in categorical data analysis.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>$\\chi^2$ Goodness of Fit Test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html",
    "href": "part2_7_power.html",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "",
    "text": "11.1 1 Introduction\nA power analysis is a calculation that helps you determine the minimum sample size required for a study to reliably detect an effect of interest.\nA power analysis is built around four main components.\nIf you know (or can reasonably estimate) any three of them, you can solve for the fourth.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#introduction",
    "href": "part2_7_power.html#introduction",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "",
    "text": "11.1.1 Key Components of Power Analysis\n\nStatistical power\nThe probability that a statistical test will correctly detect an effect of a certain size when the effect truly exists.\nPower is commonly set at 80% (0.8) or higher.\nSample size\nThe minimum number of observations needed to detect an effect of a given size with a specified power level.\nSignificance level (\\(\\alpha\\))\nThe maximum probability of rejecting a true null hypothesis (Type I error).\nTypically set at \\(\\alpha = 0.05\\).\nExpected effect size\nA standardized measure that quantifies the magnitude of the expected effect.\nThis is often based on:\n\nprior studies,\npilot data, or\nsubject-matter considerations.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#motivation-examples",
    "href": "part2_7_power.html#motivation-examples",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "11.2 Motivation Examples",
    "text": "11.2 Motivation Examples\n\nA company that manufactures light bulbs claims that a particular type of light bulb lasts 850 hours on average, with a standard deviation of 50 hours.\nA consumer protection group believes that the manufacturer has overestimated the average lifespan by about 40 hours.\nQuestion:\nHow many light bulbs does the consumer protection group need to test in order to demonstrate this discrepancy with reasonable confidence?\nThis is a classic sample size determination problem, where:\n\nthe effect size is the difference between the claimed mean and the suspected true mean,\nthe variability is known or estimated,\nand power and significance level must be specified.\n\n\n\nIt has been estimated that the average height of American white male adults is 70 inches.\nSuppose it is postulated that there is a positive correlation between height and intelligence.\nIf this is true, then the average height of white male graduate students on campus should be greater than 70 inches.\nYou plan to test this hypothesis by randomly sampling a group of white male graduate students.\nKey question:\nHow small can the sample be (or how few individuals do you need to measure) and still have enough power to detect a meaningful difference?\nThis motivates power analysis for a one-sided one-sample t-test, where:\n\nthe null hypothesis specifies a reference mean,\nthe alternative hypothesis specifies a directional effect,\nand the goal is to balance feasibility (small sample size) with statistical reliability (adequate power).\n\n\nIn the remainder of this lecture, we will formalize these ideas and use the one-sample \\(t\\)-test to illustrate how power, sample size, effect size, and significance level are mathematically connected.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#before-we-start-the-analysis",
    "href": "part2_7_power.html#before-we-start-the-analysis",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "11.3 Before We Start the Analysis",
    "text": "11.3 Before We Start the Analysis\nFor the power analysis in this lecture, we focus on the first example, which concerns testing the average lifespan of a light bulb. Our first goal is to determine the number of light bulbs that must be tested. That is, we want to find the sample size for a given significance level and power. Next, we will reverse the process and determine the power, given a sample size and significance level.\nWe know that the manufacturer claims the average lifespan of the light bulb is 850 hours, with a standard deviation of 50 hours, while the consumer protection group believes that the manufacturer has overestimated the lifespan by about 40 hours.\nIn terms of hypotheses:\n\nNull hypothesis: \\(H_0: \\mu = 850\\)\nAlternative hypothesis: \\(H_A: \\mu = 810\\)\n\nThe significance level is the probability of a Type I error, that is, rejecting \\(H_0\\) when it is actually true. We will set \\(\\alpha = 0.05.\\)\nThe power of the test against \\(H_A\\) is the probability that the test rejects \\(H_0\\) when \\(H_A\\) is true. We will set the desired power level to \\(\\text{Power} = 0.90.\\)\nBefore proceeding with the power analysis, let us briefly discuss the role of the standard deviation. Intuitively, the number of light bulbs required for testing depends on the variability of their lifespans:\n\nIf all light bulbs had exactly the same lifespan, then testing a single bulb would suffice.\nIn reality, lifespans vary substantially. For example, some bulbs may last 1000 hours, while others may last 500 hours.\n\nTherefore, the standard deviation of the lifespan distribution plays a critical role in determining the required sample size.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#power-analysis-in-sas",
    "href": "part2_7_power.html#power-analysis-in-sas",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "11.4 Power Analysis in SAS",
    "text": "11.4 Power Analysis in SAS\nIn SAS, it is straightforward to perform a power analysis for comparing means using PROC POWER.\nTo compute the required sample size:\n\nSpecify the mean under the null hypothesis.\nSpecify the mean under the alternative hypothesis.\nSpecify the population standard deviation.\nSet the significance level \\(\\alpha\\) (default is 0.05).\nSpecify the desired power level. Here we set \\(\\text{Power}=0.90\\).\nIndicate that the test is a one-sample t-test.\n\nThe following SAS code performs a power analysis for Example 1:\nPROC POWER;\n    ONESAMPLEMEANS TEST=t\n        NULLMEAN = 850\n        MEAN     = 810\n        STDDEV   = 50\n        POWER    = 0.9\n        NTOTAL   = .;\nRUN;\nThis code tells SAS to solve for the total sample size (ntotal) needed to achieve 90% power for a one-sample \\(t\\)-test at significance level (\\(\\alpha\\) = 0.05).\n\nThe result tells us that we need a sample size of at least 19 light bulbs in order to reject \\(H_0\\) under the alternative hypothesis \\(H_A\\) with a power of 0.9.\nNext, suppose we have a sample of size \\(n = 10\\). How much power do we have if we keep all other quantities the same? We can use the same SAS program to calculate the power.\nPROC POWER;\n    ONESAMPLEMEANS TEST=t\n        NULLMEAN = 850\n        MEAN     = 810\n        STDDEV   = 50\n        POWER    = .\n        NTOTAL   = 10;\nRUN;\n\nYou can see that the power is about 0.616 for a sample size of 10.\n\nThis means that if we only test 10 light bulbs, we have about a 61.6% chance of correctly rejecting the null hypothesis when the true mean is 810 hours. This is below the commonly desired power level of 0.8, indicating that a sample size of 10 may not be sufficient to reliably detect the effect size of interest.\n\nWhat if we have sample size of 15 or 20? We can use ta list of sample sizes as input to PROC POWER.\nPROC POWER;\n    ONESAMPLEMEANS TEST=t\n        NULLMEAN = 850\n        MEAN     = 810\n        STDDEV   = 50\n        POWER    = .\n        NTOTAL   = 10 to 45 by 5;\nRUN;\n\nWe can also expect that if we actually know the standard deviation is samller,\nPROC POWER;\n    ONESAMPLEMEANS TEST=t\n        NULLMEAN = 850\n        MEAN     = 810\n        STDDEV   = 30 to 100 by 10\n        POWER    = 0.8\n        NTOTAL   = . ;\nRUN;\n\nHere, varying `STDDEV to show how reduced variability lowers required sample size, while keeping the hypothesis, effect size, and power fixed.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#discussion",
    "href": "part2_7_power.html#discussion",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "11.5 Discussion",
    "text": "11.5 Discussion\nNormality assumption\nOne technical assumption underlying the power analysis: the normality assumption. If the variable of interest is not normally distributed, a small sample size will usually not achieve the power indicated by the theoretical results, because those results are derived under normality. In such cases, it may not even be appropriate to conduct a one-sample t-test with a very small sample size when the normality assumption is questionable.\nRelative difference\nThere is another important technical point. What truly matters for power analysis is not the individual values, but the difference between the two means, relative to the variability of the data. In fact, what really determines the power is the ratio\n\\[\n\\frac{\\text{difference in means}}{\\text{standard deviation}},\n\\]\nwhich is referred to as the effect size.\nFor example, we would obtain the same power if we subtracted 800 from both means, changing 850 to 50 and 810 to 10. The absolute scale of the data does not affect the power, only the standardized difference does.\nPROC POWER;\n    ONESAMPLEMEANS TEST=t\n        NULLMEAN = 50\n        MEAN     = 10\n        STDDEV   = 50\n        POWER    = 0.9\n        NTOTAL   = . ;\nRUN;\nIf we standardize our variable, we can calculate the means in terms of change in standard deviation.\nPROC POWER;\n    ONESAMPLEMEANS TEST=t\n        NULLMEAN = 1\n        MEAN     = 0.2\n        STDDEV   = 1\n        POWER    = 0.9\n        NTOTAL   = . ;\nRUN;\n\n\n\n\n\n\nNoteKey Takeaway\n\n\n\nIt is usually not an easy task to determine the ‚Äútrue‚Äù effect size. In practice, we make our best guess based on existing literature or evidence from a pilot study. A good estimate of the effect size is crucial for a successful power analysis.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#math-behind-the-power-analysis",
    "href": "part2_7_power.html#math-behind-the-power-analysis",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "11.6 Math behind the power analysis",
    "text": "11.6 Math behind the power analysis\n\nWe saw earlier that in tests of significance there are two types of errors.\nEach error has a given probability of occurring.\n\nThe significance level (Type I error) is\n\\[\n\\alpha = P(\\text{reject } H_0 \\mid H_0 \\text{ is true}).\n\\]\nThe probability of a Type II error is\n\\[\n\\beta = P(\\text{fail to reject } H_0 \\mid H_0 \\text{ is false}).\n\\]\n\nThe power of a test is related to the probability of a Type II error.\nIt is defined as \\[\n\\begin{aligned}\n1 - \\beta&= 1 - P(\\text{fail to reject } H_0 \\mid H_0 \\text{ is false})\\\\\n&= P(\\text{reject } H_0 \\mid H_0 \\text{ is false}).\n\\end{aligned}\n\\]\nTherefore, to define power, we must be specific about what\n‚Äúwhen \\(H_0\\) is false‚Äù means.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  },
  {
    "objectID": "part2_7_power.html#a-visualization-example",
    "href": "part2_7_power.html#a-visualization-example",
    "title": "11¬† Power Analysis with application in one sample \\(t\\)-test",
    "section": "11.7 A Visualization Example",
    "text": "11.7 A Visualization Example\nBottles of a popular cola drink are supposed to contain 300 ml of cola.\nThere is some variation from bottle to bottle because the filling machine is not perfectly precise.\nSuppose the distribution of the contents is normal with standard deviation\n\\[\n\\sigma = 3 \\text{ ml}.\n\\]\nWe want to carry out the following hypothesis test at significance level \\(\\alpha = 0.05\\),\nbased on a sample of six bottles of cola: \\[\nH_0 : \\mu = 300\n\\quad \\text{versus} \\quad\nH_1 : \\mu &lt; 300.\n\\]\nIf the power is calculated under the alternative hypothesis with \\[\n\\mu = 297,\n\\] then we can visualize \\(\\alpha\\) and \\(\\beta\\) as follows.\n\n\n\n\n\n\n\n\n\n\n11.7.1 Interpretation of the Power Visualization\nThe figure above illustrates the concepts of Type I error (\\(\\alpha\\)),\nType II error (\\(\\beta\\)), and power (\\(1-\\beta\\)) for a left-tailed one-sample \\(t\\)-test.\n\nThe red curve represents the sampling distribution of the test statistic under the null hypothesis\n\\[\nH_0 : \\mu = 300.\n\\]\nThe blue curve represents the sampling distribution under the alternative hypothesis\n\\[\nH_1 : \\mu = 297.\n\\]\nThe vertical cutoff line corresponds to the critical value determined by the significance level \\(\\alpha = 0.05\\).\n\nType I Error (\\(\\alpha\\))\n\nThe red shaded region to the left of the cutoff is \\[\n\\alpha = P(\\text{reject } H_0 \\mid H_0 \\text{ is true}).\n\\]\nThis is the probability of incorrectly rejecting the null hypothesis.\n\nType II Error (\\(\\beta\\))\n\nThe blue shaded region to the right of the cutoff is \\[\n\\beta = P(\\text{fail to reject } H_0 \\mid H_0 \\text{ is false}).\n\\]\nThis is the probability of failing to detect the true mean \\(\\mu = 297\\).\n\nPower (\\(1 - \\beta\\))\n\nPower increases when:\n\nthe true mean moves farther away from \\(300\\),\nthe sample size increases,\nor the variability (\\(\\sigma\\)) decreases.\n\n\nThis visualization highlights that power is not a single number, but depends on which alternative value of \\(\\mu\\) we consider when \\(H_0\\) is false.",
    "crumbs": [
      "Statistical Analysis",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Power Analysis with application in one sample $t$-test</span>"
    ]
  }
]