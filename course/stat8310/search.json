[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Description",
    "text": "Description\nThis course will cover the topics in the theory and practice of Bayesian statistical inference, ranging from a review of fundamentals to questions of current research interest. Motivation for the Bayesian approach. Bayesian computation, Monte Carlo methods, asymptotics. Model checking and comparison. A selection of examples and issues in modelling and data analysis. Discussion of advantages and difficulties of the Bayesian approach. This course will be computationally intensive through analysis of data sets using the R statistical computing language.\n\nPrerequisites\nMATH 4752/6752 ‚Äì Mathematical Statistics II or equivalent, and the ability to program in a high-level language.\n\n\nInstructor\nChi-Kuang Yeh, Assistant Professor in the Department of Mathematics and Statistics, Georgia State University.\n\nOffice: Suite 1407, 25 Park Place.\nEmail: cyeh@gsu.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#office-hour",
    "href": "index.html#office-hour",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Office Hour",
    "text": "Office Hour\n10:00‚Äì13:00 on Monday, or by appointment.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#grade-distribution",
    "href": "index.html#grade-distribution",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\nHomework ‚Äì 50%\nExam ‚Äì 30%\nFinal ‚Äì 20%",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assignment",
    "href": "index.html#assignment",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Assignment",
    "text": "Assignment\n\nA1, due on Jan 29, 2026\nA2, due on Feb 13, 2026\nA3, TBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#midterm",
    "href": "index.html#midterm",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Midterm",
    "text": "Midterm\n\nMarch 3, 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-and-corresponding-lectures",
    "href": "index.html#topics-and-corresponding-lectures",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Topics and Corresponding Lectures",
    "text": "Topics and Corresponding Lectures\nThose chapters are based on the lecture notes. This part will be updated frequently.\n\n\n\nStatus\nChapter\nTopic\nLecture\n\n\n\n\n‚úÖ\nCh. 1\nWelcome and Overview\n1\n\n\n‚úÖ\n‚Äî\nIntro to R Programming\n2\n\n\n‚úÖ\nCh. 2\nProbability and Exchangeability\n3‚Äì5\n\n\n‚úÖ\nCh. 3\nOne Parameter Models\n6‚Äì10\n\n\nüõ†Ô∏è\nCh. 4\nMonte Carlo\n11‚Äì",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-textbooks",
    "href": "index.html#recommended-textbooks",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Recommended Textbooks",
    "text": "Recommended Textbooks\n\nGelman, A., Carlin, J., Stern, H., Rubin, D., Dunson, D., and Vehtari, A. (2021). Bayesian Data Analysis, CRC Press, 3rd Ed.\nHoff, P.D. (2009). A First Course in Bayesian Statistical Methods, Springer.\nMcElreath, R. (2018). Statistical Rethinking: A Bayesian Course with Examples in R and Stan, CRC Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#side-readings",
    "href": "index.html#side-readings",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Side Readings",
    "text": "Side Readings\n\nTBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1¬† Quick Overview",
    "section": "",
    "text": "1.1 Why Bayesian?\nThe posterior distribution is obtained from the prior distribution and sampling model via Bayes‚Äô rule:\n\\[p(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta') p(\\theta') d \\theta'}.\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "01_intro.html#why-bayesian",
    "href": "01_intro.html#why-bayesian",
    "title": "1¬† Quick Overview",
    "section": "",
    "text": "Intuitive probability interpretation: Directly quantifies uncertainty about parameters as probability distributions\nIncorporates prior knowledge: Systematically combines domain expertise with data through the prior distribution\nPrincipled inference: Bayes‚Äô rule provides a coherent framework for updating beliefs based on evidence\nNatural handling of uncertainty: Posterior distributions capture full uncertainty, not just point estimates\nSequential analysis: Easily updates beliefs as new data arrives (posterior becomes new prior)\nSmall sample inference: Performs well with limited data by leveraging prior information\nPrediction with uncertainty: Generates predictive distributions that quantify uncertainty in future observations\nDecision-making: Naturally incorporates loss functions for optimal decision rules\nModel comparison: Bayes factors provide a principled approach to comparing competing models",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "01_intro.html#some-bayesian-topics-and-their-computational-focus",
    "href": "01_intro.html#some-bayesian-topics-and-their-computational-focus",
    "title": "1¬† Quick Overview",
    "section": "1.2 Some Bayesian Topics and their Computational Focus",
    "text": "1.2 Some Bayesian Topics and their Computational Focus\n\n\n\nTable¬†1.1: Some of the Bayesian Topics and its computational related focuses.\n\n\n\n\n\n\n\n\n\n\nTopics\nKey Concepts / Readings\nComputing Focus\n\n\n\n\nIntroduction to Bayesian Thinking\nBayesian vs.¬†Frequentist paradigms; Prior, likelihood, posterior\nReview of R basics and reproducible workflows\n\n\nBayesian Inference for Simple Models\nConjugate priors, Beta-Binomial, Normal-Normal, Poisson-Gamma\nSimulating posteriors, visualization\n\n\nPrior Elicitation and Sensitivity\nInformative vs.¬†noninformative priors, Jeffreys prior\nPrior sensitivity plots\n\n\nMonte Carlo Integration\nLaw of large numbers, sampling-based inference\nRandom sampling and Monte Carlo approximation\n\n\nMarkov Chain Monte Carlo (MCMC)\nMetropolis-Hastings, Gibbs sampler\nImplementing MCMC in R\n\n\nConvergence Diagnostics\nTrace plots, autocorrelation, Gelman‚ÄìRubin statistic\ncoda, rstan, and bayesplot packages\n\n\nHierarchical Bayesian Models\nPartial pooling, shrinkage, multilevel structures\nrstanarm / brms\n\n\nMidterm Project: Bayesian Linear Regression\nPosterior inference for regression, model selection\nbrms, rstanarm, custom Gibbs samplers\n\n\nBayesian Model Comparison\nBayes factors, BIC, DIC, WAIC, LOO\nPractical comparison via cross-validation\n\n\nModel Checking and Diagnostics\nPosterior predictive checks, residual analysis\npp_check in brms\n\n\nAdvanced Computation\nHamiltonian Monte Carlo (HMC), Variational Inference\nUsing Stan and CmdStanR\n\n\nBayesian Decision Theory\nUtility functions, decision rules, loss minimization\nSimple decision problems in R\n\n\nModern Bayesian Methods\nApproximate Bayesian computation (ABC), Bayesian neural networks\nExamples via rstan or tensorflow-probability\n\n\nStudent Project Presentations\nApplications and case studies\nFull workflow demonstration in R",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "01_intro.html#interesting-article",
    "href": "01_intro.html#interesting-article",
    "title": "1¬† Quick Overview",
    "section": "1.3 Interesting Article:",
    "text": "1.3 Interesting Article:\n\nGoligher, E.C., Harhay, M.O. (2023). What Is the Point of Bayesian Analysis?, American Journal of Respiratory and Critical Care Medicine, 209, 485‚Äì487.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "02_probability.html",
    "href": "02_probability.html",
    "title": "2¬† Belief function and Probability Review",
    "section": "",
    "text": "2.1 Belief functions\nProbability is a way to express rational beliefs.\nSome more notations:\nHow should we interpret these properties, and do they make sense?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#belief-functions",
    "href": "02_probability.html#belief-functions",
    "title": "2¬† Belief function and Probability Review",
    "section": "",
    "text": "A belief function \\(\\mathrm{Be}(\\cdot)\\) is a function that assigns number to statements such that the large the number, the higher the degree of belief.\n\n\nLet \\(F, G\\), and \\(H\\) be three possibly overlapping statements about the world.\nFor example:\n\nF = { a person owns a smartphone }\nG = { a person uses social media daily }\nH = { a person works remotely at least part of the time }\n\nor\n\nF = { a person has a graduate degree }\nG = { a person works in a STEM field }\nH = { a person is employed in the private sector }\n\nThe perference over bets involving these statements can be used to define a belief function\n\n\\(\\mathrm{Be}(F)&gt;\\mathrm{Be}(G)\\) means you prefer a bet \\(F\\) is true over that \\(G\\) is true.\n\nAlso, we want \\(\\mathrm{Be}(\\cdot)\\) to describe our beliefs under certain conditions\n\n\\(\\mathrm{Be}(F\\mid H) &gt; \\mathrm{Be}(G\\mid H)\\) means that if we knew that \\(H\\) were true, then we would perfer to bet that \\(F\\) is also true over \\(G\\) is also true.\n\\(\\mathrm{Be}(F\\mid G) &gt; \\mathrm{Be}(F\\mid H)\\) means that if we bet on \\(F\\), we would perfer to do it under the condition that \\(G\\) is true rather than \\(H\\) is true.\n\n\n\n\nLet \\(\\neg\\) denote negation. That is, \\(\\neg F\\) is the statement that \\(F\\) is not true.\nLet \\(F \\vee G\\) denote the disjunction (or) of statements \\(F\\) and \\(G\\), meaning that at least one of \\(F\\) or \\(G\\) is true.\nLet \\(F \\wedge G\\) denote the conjunction (and) of statements \\(F\\) and \\(G\\), meaning that both \\(F\\) and \\(G\\) are true.\n\n\nIt has been argued by many that any function that is to numerically represent our beliefs should have the following properties:\n\nB1: \\(\\mathrm{Be}(\\neg H\\mid H) \\le \\mathrm{Be}(F \\mid H) \\le \\mathrm{Be}(H \\mid H)\\)\nB2: \\(\\mathrm{Be}(F \\vee G\\mid H) \\ge  \\max\\{\\mathrm{Be}(F \\mid H), \\mathrm{Be}(G\\mid H)\\}\\)\nB3: \\(\\mathrm{Be}(F \\wedge G\\mid H)\\) can be derived from \\(\\mathrm{Be}(G\\mid H)\\) and \\(\\mathrm{Be}(F\\mid G \\wedge H)\\).\n\n\n\n\nB1 means that the number we assign to \\(\\mathrm{Be}(F \\mid H)\\), our conditional belief in \\(F\\) given \\(H\\), is bounded below and above by the numbers we assign to complete disbelief \\(\\mathrm{Be}(\\neg H \\mid H)\\) and complete belief \\(\\mathrm{Be}(H \\mid H)\\).\nB2 says that our belief that the truth lies in a given set of possibilities should not decrease as we add to the set of possibilities.\nB3 is a bit trickier. To see why it makes sense, imagine you have to decide whether or not \\(F\\) and \\(G\\) are true, knowing that \\(H\\) is true. You could do this by first deciding whether or not \\(G\\) is true given \\(H\\), and if so, then deciding whether or not \\(F\\) is true given \\(G\\) and \\(H\\).\n\n\nRecall the notation from (elementary) probability that, \\(F\\cup G\\) means F or G, and \\(F\\cap G\\) means F and G, and \\(\\emptyset\\) is the empty set.\n\nP1: \\[\n0=\\mathrm{Pr}(\\neg H \\mid H) \\leq \\mathrm{Pr}(F \\mid H) \\leq \\mathrm{Pr}(H \\mid H)=1\n\\]\nP2: \\[\\mathrm{Pr}(F \\cup G \\mid H)=\\mathrm{Pr}(F \\mid H)+\\mathrm{Pr}(G \\mid H),\\quad \\text{if}\\quad F \\cap G=\\emptyset\\]\nP3: \\[\\mathrm{Pr}(F \\cap G \\mid H)=\\mathrm{Pr}(G \\mid H) \\mathrm{Pr}(F \\mid G \\cap H)\\]\n\n\n\n2.1.1 Conclusion\nYou can see that, a probability function satisfy P1‚ÄìP3 also satisfies B1‚ÄìB3. Therefore, probability functions are a special case of belief functions, and we can use it to describe our belief.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#events-partitions-and-bayes-rule",
    "href": "02_probability.html#events-partitions-and-bayes-rule",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.2 Events, Partitions and Bayes‚Äô Rule",
    "text": "2.2 Events, Partitions and Bayes‚Äô Rule\n\nA collectiion of sets \\(\\{H_1,\\dots,H_K\\}\\) is a partition of another set \\(\\mathcal{H}\\) if\n\n\\(H_i \\cap H_j = \\emptyset\\) for all \\(i \\neq j\\) (mutually exclusive);\n\\(\\bigcup_{i=1}^K H_i = \\mathcal{H}\\) (collectively exhaustive).\n\n\nIn the context of indetifying which of several statements is true, if \\(\\mathcal{H}\\) is the set of all possible truths and \\(\\{H_1,\\dots,H_K\\}\\) is a partition of \\(\\mathcal{H}\\), then exactly one set \\(H_j\\) contains the truth.\n\nLet \\(\\mathcal{H}\\) be the status of a statistical model.\nValid partitions include:\n\n\\(\\{\\text{correctly specified}, \\text{misspecified}\\}\\)\n\\(\\{\\text{underfitting}, \\text{well-specified}, \\text{overfitting}\\}\\)\n\n\n\n2.2.1 Partition and Probability\nSuppose \\(\\{H_1,\\dots,H_K\\}\\) is a partition of \\(\\mathcal{H}\\),\\(\\mathrm{Pr}(\\mathcal{H})=1\\) and \\(E\\) is some specific event. Then, by the axioms of probability, we have\n\nLaw of total probability \\[\n\\sum_{k=1}^K \\mathrm{Pr}(H_k)=\\mathrm{Pr}\\left(\\bigcup_{k=1}^K H_k\\right)=\\mathrm{Pr}(\\mathcal{H})=1\n\\]\nLaw of marginal probability \\[\n\\mathrm{Pr}(E)=\\sum_{k=1}^K \\mathrm{Pr}(E \\cap H_k)=\\sum_{k=1}^K \\mathrm{Pr}(E \\mid H_k) \\mathrm{Pr}(H_k)\n\\]\nBayes‚Äô rule \\[\n\\mathrm{Pr}(H_j \\mid E)=\\frac{\\mathrm{Pr}(E \\mid H_j) \\mathrm{Pr}(H_j)}{\\mathrm{Pr}(E)}=\\frac{\\mathrm{Pr}(E \\mid H_j) \\mathrm{Pr}(H_j)}{\\sum_{k=1}^K \\mathrm{Pr}(E \\mid H_k) \\mathrm{Pr}(H_k)}\n\\]\n\n\nA subset of the 1996 General Social Survey includes data on the education level and income for a sample of males over 30 years of age. Let {H1,H2,H3,H4} be the events that a randomly selected person in this sample is in, respectively, the lower 25th percentile, the second 25th percentile, the third 25th percentile and the upper 25th percentile in terms of income. By definition,\n\\[\n\\left\\{\\operatorname{Pr}\\left(H_1\\right), \\operatorname{Pr}\\left(H_2\\right), \\operatorname{Pr}\\left(H_3\\right), \\operatorname{Pr}\\left(H_4\\right)\\right\\}=\\{.25, .25, .25, .25\\} .\n\\]\nNote that \\(\\{H1,H2,H3,H4\\}\\) is a partition and so these probabilities sum to 1. Let \\(E\\) be the event that a randomly sampled person from the survey has a college education. From the survey data, we have \\[\\{\\mathrm{Pr}(E\\mid H_1), \\mathrm{Pr}(E\\mid H_2),\\mathrm{Pr}(E\\mid H_3), \\mathrm{Pr}(E\\mid H_4)\\} = \\{.11, .19, .31, .53\\}.\\] These probabilities do not sum to 1 - they represent the proportions of people with college degrees in the four different income subpopulations \\(H_1, H_2, H_3\\) and \\(H_4\\). Now let‚Äôs consider the income distribution of the college-educated population. Using Bayes‚Äô rule we can obtain\n\\(\\{\\mathrm{Pr}(H_1\\mid E), \\mathrm{Pr}(H_2\\mid E), \\mathrm{Pr}(H_3\\mid E), \\mathrm{Pr}(H_4 \\mid E)\\} = \\{0.09, 0.17, 0.27, 0.47\\} ,\\) and we see that the income distribution for people in the college-educated population differs markedly from \\(\\{0.25, 0.25,0.25,0.25\\}\\), the distribution for the general population. Note that these probabilities do sum to 1 - they are the conditional probabilities of the events in the partition, given \\(E\\).\n\nIn Bayesian inference, \\({H_1, . . . ,H_K}\\) often refer to disjoint hypotheses or states of nature and \\(E\\) refers to the outcome of a survey, study or experiment. To compare hypotheses post-experimentally, we often calculate the following ratio: \\[\n\\begin{aligned}\n\\frac{\\operatorname{Pr}\\left(H_i \\mid E\\right)}{\\operatorname{Pr}\\left(H_j \\mid E\\right)} & =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right) \\operatorname{Pr}\\left(H_i\\right) / \\operatorname{Pr}(E)}{\\operatorname{Pr}\\left(E \\mid H_j\\right) \\operatorname{Pr}\\left(H_j\\right) / \\operatorname{Pr}(E)} \\\\\n& =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right) \\operatorname{Pr}\\left(H_i\\right)}{\\operatorname{Pr}\\left(E \\mid H_j\\right) \\operatorname{Pr}\\left(H_j\\right)} \\\\\n& =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right)}{\\operatorname{Pr}\\left(E \\mid H_j\\right)} \\times \\frac{\\operatorname{Pr}\\left(H_i\\right)}{\\operatorname{Pr}\\left(H_j\\right)} \\\\\n& =\\text { \"Bayes factor\" √ó \"prior beliefs\". }\n\\end{aligned}\n\\] This calculation reminds us that Bayes‚Äô rule does not determine what our beliefs should be after seeing the data, it only tells us how they should change after seeing the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#independence",
    "href": "02_probability.html#independence",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.3 Independence",
    "text": "2.3 Independence\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent, if given \\(H\\), we have \\(\\mathrm{Pr}(F \\cap G \\mid H) = \\mathrm{Pr}(F\\mid H)\\mathrm{Pr}(G\\mid H)\\).\n\nHow do we interpret conditional independence? By Axiom P3, the following is always true: \\[\n\\begin{array}{rlll}\n\\operatorname{Pr}(G \\mid H) \\operatorname{Pr}(F \\mid H \\cap G) \\stackrel{\\text { always }}{=} \\operatorname{Pr}(F \\cap G \\mid H) & \\stackrel{\\text { independence }}{=} \\operatorname{Pr}(F \\mid H) \\operatorname{Pr}(G \\mid H) \\\\\n\\operatorname{Pr}(G \\mid H) \\operatorname{Pr}(F \\mid H \\cap G) & = & \\operatorname{Pr}(F \\mid H) \\operatorname{Pr}(G \\mid H) \\\\\n\\operatorname{Pr}(F \\mid H \\cap G) & = & \\operatorname{Pr}(F \\mid H) .\n\\end{array}\n\\]\nThus, conditional independence implies that \\(\\mathrm{Pr}(F \\mid H \\cap G) = \\mathrm{Pr}(F\\mid H)\\). In other words, if we know \\(H\\) is true, and \\(F\\) and \\(G\\) are conditionally independent given \\(H\\), then knowing \\(G\\) does not change our belief about \\(F\\).\n\nLet‚Äôs consider the conditional dependence of \\(F\\) and \\(G\\) when \\(H\\) is assumed to be true in the following two situations:\nSiutation 1:\n\nF = { a hospital patient is a smoker }\nG = { a hospital patient has lung cancer }\nH = { smoking causes lung cancer}\n\nSituation 2:\n\nF = { a student studies regularly for an exam }\nG = { a student receives a high exam score }\nH = { studying improves exam performance }\n\nThink: In both of these situations, H being true implies a relationship between \\(F\\) and \\(G\\). What about when \\(H\\) is not true?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#random-variables",
    "href": "02_probability.html#random-variables",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.4 Random Variables",
    "text": "2.4 Random Variables\nIn Bayesian inference a random variable is defined as an unknown numerical quantity about which we make probability statements. For example, the quantitative outcome of a survey, experiment or study is a random variable before the study is performed. Additionally, a fixed but unknown population parameter is also a random variable\n\n2.4.1 Discrete Ramdon variables\nLet \\(Y\\) be a random variable and let \\(\\mathcal{Y}\\) be the set of all possible values that \\(Y\\) can take. If \\(\\mathcal{Y}\\) is countable, meaning that \\(\\mathcal{Y} = \\{y_1,y_2,\\dots\\}\\), then \\(Y\\) is a discrete random variable.\n\nThe event that the outcome \\(Y\\) of our survey has the value \\(Y\\) is expressed as \\(\\{Y=y\\}\\). For each \\(y\\in\\mathcal{Y}\\), the shorthand notation for \\(\\mathrm{Pr}(Y=y)\\) is \\(p(y)\\), and \\(p(\\cdot)\\) is called the probability mass function of \\(Y\\), and with two properties\n\n\\(0 \\leq p(y) \\leq 1\\) for all \\(y\\in\\mathcal{Y}\\),\n\\(\\sum_{y\\in\\mathcal{Y}} p(y) = 1\\).\n\n\nGeneral probability statements about \\(Y\\) can be derived from the pdf/pmf, for example, for any subset \\(A \\subseteq \\mathcal{Y}\\), we have \\(\\mathrm{Pr}(Y\\in A) = \\sum_{y\\in A} p(y)\\). When we have two disjoint subsets \\(A\\) and \\(B\\) of \\(\\mathcal{Y}\\), we have \\[\\mathrm{Pr}(Y\\in A \\cup B) = \\mathrm{Pr}(Y\\in A) + \\mathrm{Pr}(Y\\in B)=\\sum_{y\\in A} p(y) + \\sum_{y\\in B} p(y).\\]\n\nLet \\(Y\\) be the number of successes in \\(n\\) independent Bernoulli trials, each with probability of success \\(\\theta\\). Then, \\(Y\\) follows a Binomial distribution with parameters \\(n\\) and \\(\\theta\\), denoted as \\(Y \\sim \\mathrm{Binomial}(n,p)\\). The probability mass function of \\(Y\\) is given by \\[\np(y) = \\mathrm{Pr}(Y=y) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y}, \\quad y=0,1,2,\\dots,n.\n\\] If \\(\\theta=0.3\\) and \\(n=3\\), then the probability of observing exactly 2 successes is \\[\np(2) = \\mathrm{Pr}(Y=2 \\mid \\theta=0.3) = \\binom{3}{2\n} (0.3)^2 (0.7)^{1} = 3 \\cdot 0.09 \\cdot 0.7 = 0.189.\n\\]\n\n\n\n2.4.2 Continuous random variables\nIf \\(\\mathcal{Y}\\) is uncountable, for example, \\(\\mathcal{Y} = \\mathbb{R}\\) or \\(\\mathcal{Y} = (0,1)\\), then \\(Y\\) is a continuous random variable. In this case, we cannot list all possible values of \\(Y\\) and assign probabilities to each value. Instead, we use a probability distribution to describe the distribution of \\(Y\\). That is, the cummulative distribution function (cdf) defined as follows.\n\nThe cumulative distribution function (cdf) of a continuous random variable \\(Y\\) is defined as \\[\nF(y) = \\mathrm{Pr}(Y \\leq y), \\quad y \\in \\mathcal{Y}.\n\\]\n\nNote that, for the cdf \\(F(y)\\), we have the following properties:\n\n\\(0 \\leq F(y) \\leq 1\\) for all \\(y\\in\\mathcal{Y}\\),\n\\(F(y)\\) is non-decreasing, meaning that if \\(y_1 &lt; y_2\\), then \\(F(y_1) \\leq F(y_2)\\),\n\\(\\lim_{y \\to -\\infty} F(y) = 0\\)\n\\(\\lim_{y \\to \\infty} F(y) = 1\\).\n\nProbability of various events can be derived from the cdf. For example, for any interval \\(A = (a,b] \\subseteq \\mathcal{Y}\\), we have \\[\n\\mathrm{Pr}(Y \\in A) = \\mathrm{Pr}(a &lt; Y \\leq b) =\nF(b) - F(a).\n\\] Also, \\(\\mathrm{Pr}(Y \\leq a) = F(a)\\) and \\(\\mathrm{Pr}(Y &gt; a) = 1 - F(a)\\).\n\n\n2.4.3 Description of distributions through quantiles and moments\nIn this subsection, we discuss a few ways to describe probability distributions: quantiles and moments. They are used to describe the behaviour of the distribution compressing them into summary statistics.\n\nThe expectation or mean of a random variable \\(Y\\) can be thought as the centre of mass or the location of the distribution, which is defined as\n\nFor discrete random variable: \\[\nE(Y) = \\sum_{y\\in\\mathcal{Y}} y p(y).\n\\]\nFor continuous random variable: \\[\nE(Y) = \\int_{\\mathcal{Y}} y f(y) dy.\n\\]\n\n\n\n\n\n\n\n\nNoteDifference between mean, mode and median\n\n\n\n\nMean: the centre of mass of the distribution\nMode: The most probable value of \\(Y\\)\nMedian: The value of Y in the middle of the distribution.\n\n\n\nIn skewed distribution, the three will not equal to each other.\n\nlibrary(ggplot2)\n\n# -----------------------------\n# Theoretical reference lines\n# -----------------------------\nlines_normal &lt;- data.frame(\n  value = c(0, 0, 0),\n  Statistic = c(\"Mean\", \"Median\", \"Mode\")\n)\n\nlines_lognormal &lt;- data.frame(\n  value = c(exp(1/8), 1, exp(-1/4)),\n  Statistic = c(\"Mean\", \"Median\", \"Mode\")\n)\n\ncols &lt;- c(\"Mean\" = \"red\", \"Median\" = \"darkgreen\", \"Mode\" = \"purple\")\n\n# -----------------------------\n# Normal distribution\n# -----------------------------\np1 &lt;- ggplot() +\n  stat_function(fun = dnorm, size = 1, color = \"black\") +\n  geom_vline(\n    data = lines_normal,\n    aes(xintercept = value, color = Statistic),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = cols) +\n  scale_x_continuous(limits = c(-4, 4)) +\n  labs(\n    title = \"Non-skewed Distribution (Normal)\",\n    x = \"Value\", y = \"Density\", color = \"Statistic\"\n  ) +\n  theme_minimal()\n\n# -----------------------------\n# Log-normal distribution: LN(0, 0.5)\n# -----------------------------\np2 &lt;- ggplot() +\n  stat_function(\n    fun = function(x) dlnorm(x, meanlog = 0, sdlog = 0.5),\n    size = 1,\n    color = \"black\"\n  ) +\n  geom_vline(\n    data = lines_lognormal,\n    aes(xintercept = value, color = Statistic),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = cols) +\n  scale_x_continuous(limits = c(0, 8)) +\n  labs(\n    title = \"Skewed Distribution (Log-normal)\",\n    x = \"Value\", y = \"Density\", color = \"Statistic\"\n  ) +\n  theme_minimal()\n\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy use mean?\n\n\n\nThe mean is widely used in statistics and data analysis for several reasons:\n\nMathematical properties: The mean has desirable mathematical properties, such as linearity, which makes it easier to work with in various statistical analyses and models.\nSensitivity to all values: The mean takes into account all values in the dataset, providing a comprehensive measure of central tendency. It is also a scaled version of the total, which is often an interest\nFoundation for other statistical measures: The mean serves as the basis for many other statistical measures, such as variance and standard deviation, which are essential for understanding the spread and variability of data.\nMean minimizes the sum of squared deviations: The mean is the value that minimizes the sum of squared deviations (i.e., the expected penalty by choosing one value) from itself, making it a natural choice for summarizing data.\nMay contains full information: In some distributions (e.g., bernoulli distribution), the mean contains all the information about the distribution, making it a sufficient statistic for inference.\n\n\n\n\nThe variance of a random variable \\(Y\\) measures the spread or dispersion of the distribution, and is defined as \\[\n\\mathrm{Var}(Y) = E\\left[(Y - E(Y))^2\\right] = E[Y^2]- E^2[Y].\n\\] The standard deviation is the square root of the variance, denoted as \\(\\mathrm{SD}(Y) = \\sqrt{\\mathrm{Var}(Y)}\\).\n\n\nThe quantile of order \\(\\alpha\\) of a random variable \\(Y\\) is defined as the value \\(y_\\alpha\\) such that \\[\n\\mathrm{Pr}(Y \\leq y_\\alpha) = F(y_\\alpha) = \\alpha\n\\] for \\(0 &lt; \\alpha &lt; 1\\).\n\nFor example, the median is the quantile of order 0.5, denoted as \\(y_{0.5}\\), which satisfies \\(\\mathrm{Pr}(Y \\leq y_{0.5}) = 0.5\\). Also, \\((y_{0.025},y_{0.975})\\) and \\((y_{0.25},y_{0.75})\\) contains 95% and 50% of the mass of the distribution, respectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#joint-disitrubiton",
    "href": "02_probability.html#joint-disitrubiton",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.5 Joint Disitrubiton",
    "text": "2.5 Joint Disitrubiton\n\n2.5.1 Discrete random variables\nLet \\(Y_1\\) and \\(Y_2\\) be two random variables with possible values in \\(\\mathcal{Y}_1\\) and \\(\\mathcal{Y}_2\\), respectively. The joint distribution of \\(Y_1\\) and \\(Y_2\\) describes the probability of various combinations of values that \\((Y_1,Y_2)\\) can take.\nJoint beliefs about \\(Y_1\\) and \\(Y_2\\) can be represented with probabilities. For example, for subsets \\(A\\subset \\mathcal{Y}_1\\) and \\(B\\subset \\mathcal{Y}_2\\), \\(\\mathrm{Pr}(\\{Y_1\\in A\\} \\cap \\{Y_2 \\in B\\})\\) represents our belief that \\(Y_1\\) takes a value in \\(A\\) and \\(Y_2\\) takes a value in \\(B\\). The joint pdf or joint density of \\(Y_1\\) and \\(Y_2\\) is defined as\n\\[\np_{Y_1 Y_2}\\left(y_1, y_2\\right)=\\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right), \\text { for } y_1 \\in \\mathcal{Y}_1, y_2 \\in \\mathcal{Y}_2 .\n\\]\nThe marginal density of \\(Y_1\\) can be computed from the joint density: \\[\n\\begin{aligned}\np_{Y_1}\\left(y_1\\right) & \\equiv \\operatorname{Pr}\\left(Y_1=y_1\\right) \\\\\n& =\\sum_{y_2 \\in \\mathcal{Y}_2} \\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right) \\\\\n& \\equiv \\sum_{y_2 \\in \\mathcal{Y}_2} p_{Y_1 Y_2}\\left(y_1, y_2\\right)\n\\end{aligned}\n\\]\nThe conditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\) can be computed from the joint density and the marginal density: \\[\n\\begin{aligned}\np_{Y_2 \\mid Y_1}\\left(y_2 \\mid y_1\\right) & =\\frac{\\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right)}{\\operatorname{Pr}\\left(Y_1=y_1\\right)} \\\\\n& =\\frac{p_{Y_1 Y_2}\\left(y_1, y_2\\right)}{p_{Y_1}\\left(y_1\\right)} .\n\\end{aligned}\n\\]\nYou should be able to see that\n\n\\(\\left\\{p_{Y_1}, p_{Y_2 \\mid Y_1}\\right\\}\\) can be derived from \\(p_{Y_1 Y_2}\\),\n\\(\\left\\{p_{Y_2}, p_{Y_1 \\mid Y_2}\\right\\}\\) can be derived from \\(p_{Y_1 Y_2}\\)\n\\(p_{Y_1 Y_2}\\) can be derived from \\(\\left\\{p_{Y_1}, p_{Y_2 \\mid Y_1}\\right\\}\\)\n\\(p_{Y_1 Y_2}\\) can be derived from \\(\\left\\{p_{Y_2}, p_{Y_1 \\mid\nY_2}\\right\\}\\)\n\nBUT\n\n\\(p_{Y_1 Y_2}\\) cannot be derived from \\(\\left\\{p_{Y_1}, p_{Y_2}\\right\\}\\).\n\nThe subscripts of density functions are often dropped, in which case the type of density function is determined by the arguments. For example,\n\n\\(p(y_1,y_2)=p_{Y_1 Y_2}(y_1,y_2)\\) is the joint density of \\(Y_1\\) and \\(Y_2\\),\n\\(p(y_1)=p_{Y_1}(y_1)\\) is the marginal density of \\(Y_1\\)\n\\(p(y_2 \\mid y_1)=p_{Y_2\\mid Y_1}(y_2 \\mid y_1)\\) is the conditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\), and so on.\n\n\nSuppose a sociological study reports the following joint distribution of parents‚Äô education level and children‚Äôs income level in a population.\nJoint distribution of education and income Suppose a sociological study reports the following joint distribution of parents‚Äô education level and children‚Äôs income level in a population as shown in the Table below\n\n\n\nParent \\ Child\nLow Income\nMiddle Income\nHigh Income\n\n\n\n\nHigh School or Less\n0.18\n0.22\n0.10\n\n\nCollege\n0.08\n0.20\n0.12\n\n\nGraduate School\n0.04\n0.06\n0.10\n\n\n\nSuppose we randomly sample a parent‚Äìchild pair from this population.\nLet\n- \\(Y_1\\) be the parent‚Äôs education level\n- \\(Y_2\\) be the child‚Äôs income level\nWe are interested in the conditional probability that the child has high income, given that the parent has a college education.\nWe may answer this question using the conditional probability formula:\n\\[\n\\Pr(Y_2 = \\text{High Income} \\mid Y_1 = \\text{College})\n= \\frac{\\Pr(Y_2 = \\text{High Income} \\cap Y_1 = \\text{College})}\n{\\Pr(Y_1 = \\text{College})}\n\\]\nFrom the table,\n\\[\n\\Pr(Y_2 = \\text{High Income} \\cap Y_1 = \\text{College}) = 0.12\n\\]\n\\[\n\\Pr(Y_1 = \\text{College}) = 0.08 + 0.20 + 0.12 = 0.40\n\\]\nTherefore,\n\\[\n\\Pr(Y_2 = \\text{High Income} \\mid Y_1 = \\text{College})\n= \\frac{0.12}{0.40}\n= 0.30\n\\]\nThus, our conclusion from the table is, among children whose parents have a college education, 30% attain high income.\n\n\n\n2.5.2 Continuous random variables\nLet \\(Y_1\\) and \\(Y_2\\) be two continuous random variables with possible values in \\(\\mathcal{Y}_1\\) and \\(\\mathcal{Y}_2\\), respectively. The joint distribution of \\(Y_1\\) and \\(Y_2\\) describes the probability of various combinations of values that \\((Y_1,Y_2)\\) can take. We again work with the cumulative distribution function (cdf). The definition is given as follows.\n\nGiven a continuous joint cdf \\(F_{Y_1 Y_2}(y_1,y_2)\\), there is a function \\(p_{Y_1,Y_2}\\) such that \\[\nF_{Y_1,Y_2}(a,b) = \\int_{-\\infty}^a \\int_{-\\infty}^b p_{Y_1,Y_2}(y_1,y_2) dy_2 dy_1,\n\\] and \\(p_{Y_1,Y_2}(y_1,y_2)\\) is called the joint density function of \\(Y_1\\) and \\(Y_2\\).\n\nSimilar to the discrete case, we can derive marginal and conditional densities from the joint density as\n\nMarginal density of \\(Y_1\\): \\[\np_{Y_1}(y_1) = \\int_{\\mathcal{Y}_2} p_{Y_1,Y_2}(y_1,y_2) dy_2,\n\\]\nConditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\): \\[\np_{Y_2 \\mid Y_1}(y_2 \\mid y_1) = \\frac{p_{Y_1,Y_2}(y_1,y_2)}{p_{Y_1}(y_1)}.\n\\]\n\nThink about why \\(p_{Y_2 \\mid Y_1}(y_2 \\mid y_1)\\) is an actual pdf.\n\n\n2.5.3 Mixed continuous and discrete variables\nIt is possible to have joint distributions involving both discrete and continuous random variables. For example, let \\(Y_1\\) be a discrete random variable taking values in \\(\\mathcal{Y}_1\\) and \\(Y_2\\) be a continuous random variable taking values in \\(\\mathcal{Y}_2\\). The joint distribution of \\(Y_1\\) and \\(Y_2\\) can be described by the joint density function \\(p_{Y_1,Y_2}(y_1,y_2)\\), which gives the probability that \\(Y_1\\) takes the value \\(y_1\\) and \\(Y_2\\) takes a value in an infinitesimal interval around \\(y_2\\). One such as example is that \\(Y_1\\) is a binary variable indicating the presence or absence of a disease, and \\(Y_2\\) is a continuous variable representing the severity of symptoms. Suppose we define\n\nMarginal density \\(p_{Y_1}\\) from our belief \\(\\mathrm{Pr}(Y_1=y_1)\\)\na conditional density \\(p_{Y_2\\mid Y_1}\\) from \\(\\mathrm{Pr}(Y_2\\le y_2\\mid Y_1=y_1)\\doteq F_{Y_2\\mid Y_1}(y_2\\mid y_1)\\).\n\nThen, the joint density can be derived as \\[\np_{Y_1,Y_2}(y_1,y_2) = p_{Y_1}(y_1) p_{Y_2 \\mid Y_1}(y_2 \\mid y_1),\n\\] and the probability can be calculated as \\[\n\\mathrm{Pr}(Y_1\\in A,Y_2\\in B) = \\int_{y_2\\in B} \\left\\{\\sum_{y_1\\in A}p_{Y_1,Y_2}(y_1,y_2)\\right\\}dy_2.\n\\]\n\n\n2.5.4 Bayes‚Äô rule and parameter estimation\nLet\n\n\\(\\theta\\): proportion of people in a large population who have a certain charactersitic.\n\\(Y\\): number of people in a small random sample from the population who have the charactersitic\n\nThen, in this case, we may threat \\(\\theta\\) as continuous random variable taking values in \\(\\Theta = (0,1)\\), and \\(Y\\) as a discrete random variable taking values in \\(\\mathcal{Y}= \\{0,1,2,\\dots,n\\}\\), where \\(n\\) is the sample size. Bayesian estimation of the parameter \\(\\theta\\) derives from the calculate of \\(p(\\theta\\mid y)\\) where \\(y\\) is the observed value of \\(Y\\). In Bayesian, this calculation first requires that we have a joint density \\(p(y,\\theta)\\) representing our belief about \\(\\theta\\) and the survey outcome \\(Y\\). Often, it is natural to construct this joint density from\n\n\\(p(\\theta)\\): our prior belief about \\(\\theta\\) before seeing the data, and\n\\(p(y \\mid \\theta)\\): belief about \\(Y\\) given \\(\\theta\\), often called the likelihood function.\n\nOnce we observed \\(\\{Y=y\\}\\), we need to compute our updated belief about \\(\\theta\\), represented by the posterior density \\(p(\\theta \\mid y)\\) as \\[\np(\\theta \\mid y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta) p(\\theta) d\\theta}.\n\\]\nIf we have two values \\(\\theta_1\\) and \\(\\theta_2\\) in \\(\\Theta\\) that may be true, then the ratio of their posterior densities is given by\n\\[\n\\frac{p(\\theta_1 \\mid y)}{p(\\theta_2 \\mid y)} = \\frac{p(y \\mid \\theta_1) p(\\theta_1) / p(y)}{p(y \\mid \\theta_2) p(\\theta_2) / p(y)} =\n\\frac{p(y \\mid \\theta_1) p(\\theta_1)}{p(y \\mid \\theta_2) p(\\theta_2)}.\n\\]\n\n\n\n\n\n\nNoteNote\n\n\n\nFrom this calculation, we notice when we are calculating the relative posterior probability between two parameter values we do not need calculate \\(p(y)\\) out.\n\n\nAnother way to think about this is, for a function of \\(\\theta\\), \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta).\n\\]\n\n\n\n\n\n\nNoteNote\n\n\n\nWe will see that the numerator is the important part, while the denominator is just a normalizing constant to make sure the posterior density integrates to 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#independence-random-variables",
    "href": "02_probability.html#independence-random-variables",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.6 Independence Random Variables",
    "text": "2.6 Independence Random Variables\nLet \\(Y_1,\\dots,Y_n\\) be random variables with joint density \\(p(y_1,\\dots,y_n)\\), and \\(\\theta\\) is the parameter describe the conditions under which the random variables are generated. We say that \\(Y_1,\\dots,Y_n\\) are conditionally independent given \\(\\theta\\) if every collection of \\(n\\) sets \\(\\{A_1,\\dots,A_n\\}\\) satisfies \\[\n\\mathrm{Pr}(Y_1 \\in A_1,\\dots,Y_n \\in A_n \\mid \\theta) = \\prod_{i=1}^n \\mathrm{Pr}(Y_i \\in A_i \\mid \\theta).\n\\] If we have independence property, then \\[\n\\mathrm{Pr}(Y_i\\in A_i \\mid \\theta, Y_j\\in A_j) = \\mathrm{Pr}(Y_i \\in A_i \\mid \\theta),\n\\] so the conditional indpenddence can be interpreted as meaning that \\(Y_j\\) gives no additional information about \\(Y_i\\) once we know \\(\\theta\\). Also, under independence, the joint density can be factorized as \\[\np(y_1,\\dots,y_n \\mid \\theta) = \\prod_{i= 1}^n p_{Y_i}(y_i \\mid \\theta).\n\\]\nIf the samples are also identically distributed, meaning that each \\(Y_i\\) has the same marginal density \\(p_Y(y \\mid \\theta)\\), then the joint density can be further simplified as \\[\np(y_1,\\dots,y_n \\mid \\theta) = \\prod_{i= 1}^n p_Y(y_i \\mid \\theta).\n\\]\nIn this case , we say that \\(Y_1,\\dots,Y_n\\) are independent and identically distributed (i.i.d.) given \\(\\theta\\), with notation \\[  \nY_1,\\dots,Y_n\\mid \\theta \\stackrel{i.i.d.}{\\sim} p_Y(y \\mid \\theta).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#exchangeability",
    "href": "02_probability.html#exchangeability",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.7 Exchangeability",
    "text": "2.7 Exchangeability\n\nA sequence of random variables \\(Y_1,Y_2,\\dots,Y_n\\) is exchangeable if for any permutation \\(\\pi\\) of the indices \\(\\{1,2,\\dots ,n\\}\\), we have \\[\np(y_1,y_2,\\dots,y_n) = p(y_{\\pi(1 )},y_{\\pi(2)},\\dots,y_{\\pi(n)}).\n\\]\n\nIn other words, the joint density of an exchangeable sequence is invariant to the order of the random variables. That is, the labels contains no information about the outcome.\n\nSuppose a factory produces a large batch of items. Each item may be either defective or non-defective.\nLet \\[\nY_i =\n\\begin{cases}\n1, & \\text{if the } i\\text{th inspected item is defective}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nWe inspect \\(n = 10\\) items chosen at random from the batch and record\n\\(Y_1, Y_2, \\dots, Y_{10}.\\)\nConsider the following three observed sequences:\n\n\\(p(1,0,1,0,1,0,0,1,0,1)\\)\n\\(p(0,1,0,1,0,1,1,0,0,1)\\)\n\\(p(1,1,0,0,1,0,1,0,0,1)\\)\n\nEach sequence contains 5 defective items and 5 non-defective items.\nQuestion: Is there a reason to assign these three sequences different probabilities?\nIf the inspection order conveys no additional information about quality, then only the number of defective items matters, not their positions in the sequence. This motivates the concept of exchangeability.\n\n\n2.7.1 Independence versus dependence\nConsider the probability assignments\n\\[\n\\begin{cases}\n\\Pr(Y_{10} = 1) = a, \\\\[6pt]\n\\Pr(Y_{10} = 1 \\mid Y_1 = \\cdots = Y_9 = 1) = b.\n\\end{cases}\n\\]\nIf \\(a \\neq b\\), then \\(Y_{10}\\) is not independent of \\(Y_1, \\dots, Y_9\\).\nHowever, lack of independence does not imply lack of exchangeability.\nQuestion: should we have \\(a = b\\), \\(a &gt; b\\) or \\(a  &lt; b\\)?\n\n\n2.7.2 A latent-parameter model\nSuppose the defect rate \\(\\theta\\) of the factory is unknown.\nConditional on \\(\\theta\\), \\[\nY_1, \\dots, Y_{10} \\mid \\theta \\sim \\text{i.i.d. Bernoulli}(\\theta).\n\\]\nThen \\[\n\\Pr(Y_1 = y_1, \\dots, Y_{10} = y_{10} \\mid \\theta)\n= \\theta^{\\sum y_i}(1-\\theta)^{10-\\sum y_i}.\n\\]\nIf our uncertainty about \\(\\theta\\) is described by a prior distribution \\(p(\\theta)\\), the marginal joint distribution is\n\\[\np(y_1, \\dots, y_{10})\n= \\int \\theta^{\\sum y_i}(1-\\theta)^{10-\\sum y_i} p(\\theta)\\, d\\theta.\n\\]\nThis probability depends only on the number of defective items, not their order.\nThus, we have exchangeability, even though the \\(Y_i\\) are not independent under this model of belief.\n\nConditional i.i.d. given a latent parameter implies marginal exchangeability. That is, if \\(\\theta \\sim p(\\theta)\\) and \\(Y_1,\\dots,Y_n\\) are conditionally i.i.d. given \\(\\theta\\), then \\(Y_1,\\dots,Y_n\\) (i.e., unconditional on \\(\\theta\\)) are exchangeable.\n\nFor the Proof, see page 28 in Hopf (2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_probability.html#de-finettis-theorem",
    "href": "02_probability.html#de-finettis-theorem",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.8 de Finetti‚Äôs Theorem",
    "text": "2.8 de Finetti‚Äôs Theorem\nAs of now, we have seen that conditional i.i.d. given a latent parameter implies marginal exchangeability. For example, \\[\n\\begin{cases}\nY_1,\\dots,Y_n \\mid \\theta \\stackrel{i.i.d.}{\\sim}  \\\\\n\\theta \\sim p(\\theta) \\end{cases} \\implies Y_1,\\dots,Y_n \\text{ are exchangeable}.\n\\]\nThe converse is also true, as stated in de Finetti‚Äôs theorem.\n\nLet \\(Y_i\\in\\mathcal{Y}\\) for all \\(i \\in\\{1,2,\\dots,n\\}\\) be an exchangeable sequence of random variables. Then, there exists a parameter space \\(\\Theta\\) and a prior distribution \\(p(\\theta)\\) on \\(\\Theta\\) such that the joint distribution of \\(Y_1,\\dots,Y_n\\) can be represented as \\[\np(y_1,\\dots,y_n) = \\int_{\\Theta} \\left\\{\\prod_{i=1}^n p_Y(y_i \\mid \\theta)\\right\\} p(\\theta) d\\theta,\n\\] where \\(p_Y(y \\mid \\theta)\\) is a probability density function on \\(\\mathcal{Y}\\) for each \\(\\theta \\in \\Theta\\). The prior and sampling model depend on the form of the belief model \\(p(y_1,\\dots,y_n)\\).\n\nThe probability distribution \\(p(\\theta)\\) represents our belief about the outcomes \\(\\{Y_1,Y_2,\\dots,Y_n\\}\\), induced by our belief model \\(p(y_1,\\dots,y_n)\\). That is,\n\n\\(p(\\theta)\\) represents our belief about \\(\\lim_{n\\to\\infty} \\sum Y_i/n\\) in the binary sense\n\\(p(\\theta)\\) represents our belief about \\(\\lim_{n\\to\\infty} \\sum (Y_i\\le c)/n\\) for each \\(c\\) in the general case.\n\nThe main idea of this and the previous section is as follows \\[\n\\begin{aligned}\nY_1, \\ldots, Y_n \\mid \\theta &\\stackrel{\\text{i.i.d.}}{\\sim} p(\\cdot \\mid \\theta), \\\\\n\\theta &\\sim p(\\theta)\n\\end{aligned}\n\\quad \\Longleftrightarrow \\quad\nY_1, \\ldots, Y_n \\text{ are exchangeable for all } n .\n\\]\nQuestion: When is the condition of ‚Äúexchangeability for all \\(n\\)‚Äù reasonable?\n\nHave exchaneability and repeatability\n\nExchangeability holds if the labels convey no information\nrepeatability hold includes the follows\n\n\\(Y_1,\\dots,Y_n\\) are outcomes of a repeartable experiment\n\\(Y_1,\\dots,Y_n\\) are sampled from a finite population with replacement\n\\(Y_1,\\dots,Y_n\\) are sampled from an infinite population without replacement.\n\n\n\n\n\n\n\n\n\nNoteIn large finite population\n\n\n\nNote, if \\(Y_1,\\dots,Y_n\\) are exchangeable and sampled from a finite population of size \\(N\\) that is way bigger than \\(n\\) without replacement, then they can be modelled as approximate being conditional i.i.d.\n\n\n\nThis Chapter follows closely with Chapter 2 in Hoff (2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html",
    "href": "03_bi-1par.html",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "",
    "text": "3.1 Three basic ingredients of Bayesian inference\nRecall the important ingredients of Bayesian inference:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#three-basic-ingredients-of-bayesian-inference",
    "href": "03_bi-1par.html#three-basic-ingredients-of-bayesian-inference",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "",
    "text": "3.1.1 Prior\nThe prior distribution encodes our beliefs about the parameter \\(\\theta\\) before conduct any experiments.\n\n\n\n\n\n\nNotePrior and Data are independent\n\n\n\nNote that, the prior distribution is independent of the data. It represents our knowledge or beliefs about the parameter before seeing the data.\n\n\nHow do we choose a prior?\n\nInformative priors: Based on previous studies or expert knowledge\nWeakly informative priors: Provide some regularization without dominating the data\nNon-informative priors: Attempt to be ‚Äúobjective‚Äù (e.g., uniform, Jeffreys prior)\n\n\n\n3.1.2 Likelihood\nThe likelihood function represents the probability of observing the data given the parameter \\(\\theta\\). It can be derived from the assumed statistical model for the data or experiment, i.e., \\(y \\sim p(y \\mid \\theta)\\), or we can estimate this non-parametrically (i.e., without assuming the underlying distribution is the one we know.).\n\n\n\n\n\n\nNoteLikelihood is NOT a probability distribution for \\(\\theta\\)\n\n\n\nNote that, the likelihood function is not a probability distribution for \\(\\theta\\) itself. It is a function of \\(\\theta\\) for fixed data \\(y\\).\n\n\n\n\n3.1.3 Posterior\nThe posterior distribution combines the prior and likelihood to update our beliefs about \\(\\theta\\) after observing the data. It is given by Bayes‚Äô theorem: \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) \\pi(\\theta)}{p(y)},\n\\] where \\(p(y) = \\int p(y \\mid \\theta) \\pi(\\theta) d\\theta\\) is the marginal likelihood or evidence.\n\n\n3.1.4 An simple example\nExamples:\n\nBeta prior + Binomial likelihood ‚Üí Beta posterior\nNormal prior + Normal likelihood (known variance) ‚Üí Normal posterior\nGamma prior + Poisson likelihood ‚Üí Gamma posterior\n\nAdvantages: - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\nLimitations:\n\nMay not reflect true prior beliefs\nModern computing makes non-conjugate priors feasible\n\n\nLet‚Äôs look a simple example to illustrate the convenience of conjugate priors. Consider a Binomial model with unknown success probability \\(\\theta\\) and known number of trials \\(n\\). We can use a Beta prior for \\(\\theta\\).\nSuppose we have a Binomial model with known number of trials \\(n\\) and unknown success probability \\(\\theta\\). We can use a Beta prior for \\(\\theta\\).\n\nPrior: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\nLikelihood: \\(y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)\\)\n\nThe derivation of the posterior is as follows:\n\\[\n\\begin{aligned}\np(y \\mid \\theta) & = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\\\\n\\pi(\\theta) & = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)},\n\\end{aligned}\n\\] where \\(B(\\alpha, \\beta)\\) is the Beta function. Then the posterior is proportional to: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\pi(\\theta) \\propto \\theta^{y + \\alpha - 1} (1 -  \\theta)^{n - y + \\beta - 1}.\n\\] This is the kernel of a Beta distribution with parameters \\((\\alpha + y, \\beta + n - y)\\). Thus, the posterior distribution is: \\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta\n+ n - y).\n\\]\nThus, the Posterior is \\(\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#happiness-data-the-first-example-of-bayesian-inference-procedure",
    "href": "03_bi-1par.html#happiness-data-the-first-example-of-bayesian-inference-procedure",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.2 Happiness Data ‚Äì the first example of Bayesian inference procedure",
    "text": "3.2 Happiness Data ‚Äì the first example of Bayesian inference procedure\nWe study Bayesian inference for a binomial proportion \\(\\theta\\) when the sample size \\(n\\) is fixed. In this example, we want to see what is the procedure of doing Bayesian inference\n\nIn the 1998 General Social Survey, each female respondent aged 65 or over was asked whether she was generally happy.\nDefine the response variable \\[\nY_i =\n\\begin{cases}\n1, & \\text{if respondent } i \\text{ reports being generally happy},\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad i = 1,\\ldots,n,\n\\] where \\(n = 129\\).\nBecause we lack information that distinguishes individuals, it is reasonable to treat the responses as exchangeable.\nThat is, before observing the data, the labels or ordering of respondents carry no information.\nSince the sample size \\(n\\) is small relative to the population size \\(N\\) of senior women, results from the previous chapter justify the following modeling approximation.\nModeling Assumptions: Our beliefs about \\((Y_1,\\ldots,Y_{129})\\) are described by:\n\nAn unknown population proportion \\[\n\\theta = \\frac{1}{N}\\sum_{i=1}^N Y_i,\n\\] where \\(\\theta\\) represents the proportion of generally happy individuals in the population.\nA sampling model given \\(\\theta\\)\nConditional on \\(\\theta\\), the responses \\(Y_1,\\ldots,Y_{129}\\) are independent and identically distributed Bernoulli random variables with \\[\n\\Pr(Y_i = 1 \\mid \\theta) = \\theta.\n\\]\n\n\nGiven the population proportion \\(\\theta\\), each respondent independently reports being happy with probability \\(\\theta\\).\n\nLikelihood: Under this model, the probability of observing data \\(\\{y_1,\\ldots,y_{129}\\}\\) given \\(\\theta\\) is \\[\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{\\sum_{i=1}^{129} y_i}\n(1-\\theta)^{129-\\sum_{i=1}^{129} y_i}.\n\\]\nThis expression depends on the data only through the sufficient statistic \\[\nS = \\sum_{i=1}^{129} Y_i,\n\\] the total number of respondents who report being generally happy.\nFor the happiness data, \\[\nS = 118,\n\\] so the likelihood simplifies to \\[\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{118}(1-\\theta)^{11}.\n\\]\nQ: Which prior to be used?\nA prior distribution is conjugate to a likelihood if the posterior distribution belongs to the same family as the prior. For the binomial likelihood, the Beta distribution is conjugate. But we have another choice of prior, to use non-informative prior.\nA Uniform Prior Distribution: Suppose our prior information about \\(\\theta\\) is very weak, in the sense that all subintervals of \\([0,1]\\) with equal length are equally plausible.\nSymbolically, for any \\(0 \\le a &lt; b &lt; b+c \\le 1\\), \\[\n\\Pr(a \\le \\theta \\le b)\n=\n\\Pr(a+c \\le \\theta \\le b+c).\n\\]\nThis implies a uniform prior: \\[\n\\pi(\\theta) = 1, \\qquad 0 \\le \\theta \\le 1.\n\\]\nPosterior Distribution: Bayes‚Äô rule gives \\[\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{p(y_1,\\ldots,y_{129} \\mid \\theta)\\,\\pi(\\theta)}\n     {p(y_1,\\ldots,y_{129})}.\n\\]\nWith a uniform prior, this reduces to \\[\np(\\theta \\mid y_1,\\ldots,y_{129})\n\\propto\n\\theta^{118}(1-\\theta)^{11}.\n\\]\n\nKey idea: with a uniform prior, the posterior has the same shape as the likelihood.\n\nTo obtain a proper probability distribution, we must normalize.\nNormalizing Constant and the Beta Distribution: Using the identity \\[\n\\int_0^1 \\theta^{a-1}(1-\\theta)^{b-1}\\,d\\theta\n=\n\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n\\] we find \\[\np(y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(119)\\Gamma(12)}{\\Gamma(131)}.\n\\]\nTherefore, the posterior density is \\[\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(131)}{\\Gamma(119)\\Gamma(12)}\n\\theta^{119-1}(1-\\theta)^{12-1}.\n\\]\nThat is, \\[\n\\theta \\mid y \\sim \\mathrm{Beta}(119,\\,12).\n\\]\nRecall that, a random variable \\(\\theta \\sim \\mathrm{Beta}(a,b)\\) distribution if \\[\n\\pi(\\theta)\n=\n\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n\\theta^{a-1}(1-\\theta)^{b-1}.\n\\]\nFor \\(\\theta \\sim \\mathrm{Beta}(a,b)\\), the expectation (i.e., mean or the first moment) is \\(\\mathbb{E}(\\theta) = \\frac{a}{a+b}\\), and the variance is \\(\\mathrm{Var}(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}.\\)\nIn our example, the happiness data, the posterior distribution is \\[\n\\theta \\mid y \\sim \\mathrm{Beta}(119,12).\n\\]\nThus, the posterior mean is \\(\\mathbb{E}(\\theta \\mid y) = 0.915\\), and the posterior standard deviation is \\(\\mathrm{sd}(\\theta \\mid y) = 0.025\\).\nThese summaries quantify both our best estimate of the population proportion and our remaining uncertainty after observing the data.\n\n\n3.2.1 Inference about exchangeable binary data\nPosterior Inference under a Uniform Prior\nSuppose \\(Y_1, \\ldots, Y_n \\mid \\theta \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(\\theta)\\), and we place a uniform prior on \\(\\theta\\). The posterior distribution of \\(\\theta\\) given the observed data \\(y_1, \\ldots, y_n\\) is proportional to \\[\n\\begin{aligned}\np(\\theta \\mid y_1, \\ldots, y_n)\n&= \\frac{p(y_1, \\ldots, y_n \\mid \\theta) \\pi(\\theta)}{p(y_1, \\ldots, y_n)} \\\\\n&= \\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i} \\times \\frac{\\pi(\\theta)}{p(y_1, \\ldots, y_n)}\\\\\n&\\propto\n\\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i}.\n\\end{aligned}\n\\]\nConsider two parameter values \\(\\theta_a\\) and \\(\\theta_b\\). The ratio of their posterior densities is \\[\n\\begin{aligned}\n\\frac{p(\\theta_a \\mid y_1, \\ldots, y_n)}\n     {p(\\theta_b \\mid y_1, \\ldots, y_n)}\n&=\\frac{\\theta_a^{\\sum y_i}\\left(1-\\theta_a\\right)^{n-\\sum y_i} \\times p\\left(\\theta_a\\right) / p\\left(y_1, \\ldots, y_n\\right)}{\\theta_b^{\\sum y_i}\\left(1-\\theta_b\\right)^{n-\\sum y_i} \\times p\\left(\\theta_b\\right) / p\\left(y_1, \\ldots, y_n\\right)} \\\\\n&=\n\\left(\\frac{\\theta_a}{\\theta_b}\\right)^{\\sum_i y_i}\n\\left(\\frac{1 - \\theta_a}{1 - \\theta_b}\\right)^{n - \\sum_i y_i}\n\\frac{p(\\theta_a)}{p(\\theta_b)}.\n\\end{aligned}\n\\]\nThis expression shows that the data affect the posterior distribution only through the sum of the data \\(\\sum_{i=1}^n y_i\\) based on the relative probability density at \\(\\theta_a\\) to \\(\\theta_b\\).\nAs a result, for any set \\(A\\), one can show that \\[\n\\Pr(\\theta \\in A \\mid Y_1 = y_1, \\ldots, Y_n = y_n)\n=\n\\Pr\\left(\\theta \\in A \\mid \\sum_{i=1}^n Y_i = \\sum_{i=1}^n y_i\\right).\n\\]\nThis means that \\(\\sum_{i=1}^n Y_i\\) contains all the information in the data relevant for inference about \\(\\theta\\). We therefore say that \\(Y = \\sum_{i=1}^n Y_i\\) is a sufficient statistic for \\(\\theta\\). The term sufficient is used because knowing \\(\\sum_{i=1}^n Y_i\\) is sufficient to carry out inference about \\(\\theta\\); no additional information from the individual observations \\(Y_1, \\ldots, Y_n\\) is required.\nIn the case where \\(Y_1, \\ldots, Y_n \\mid \\theta\\) are i.i.d. Bernoulli\\((\\theta)\\) random variables, the sufficient statistic \\(Y = \\sum_{i=1}^n Y_i\\) follows a binomial distribution with parameters \\((n, \\theta)\\).\nThe Binomial Model\nBecause each \\(Y_i\\) is Bernoulli\\((\\theta)\\) and the observations are independent, the sufficient statistic \\(Y = \\sum_{i=1}^n Y_i\\) follows a binomial distribution with parameters \\((n, \\theta)\\).\nThat is, \\(\\Pr(Y = y \\mid \\theta)=\\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}\\), \\(y = 0, 1, \\ldots, n\\). For a binomial\\((n, \\theta)\\) random variable \\(Y\\),\n\n\\(\\mathbb{E}[Y \\mid \\theta] = n\\theta\\),\n\\(\\mathrm{Var}(Y \\mid \\theta) = n\\theta(1 - \\theta).\\)\n\n\n\n\n\n\n\n\n\n\nPosterior inference under a uniform prior distribution\nHaving observed \\(Y = y\\) our task is to obtain the posterior distribution of \\(\\theta\\). By Bayes‚Äô theorem, \\[\np(\\theta \\mid y)\n= \\frac{p(y \\mid \\theta),\\pi(\\theta)}{p(y)}.\n\\]\nFor a binomial model with \\(Y \\sim \\text{Binomial}(n,\\theta)\\), the likelihood is \\[\np(y \\mid \\theta) = \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}.\n\\]\nTherefore, \\[\np(\\theta \\mid y)\n= \\frac{\\binom{n}{y} \\theta^y(1-\\theta)^{n-y}\\pi(\\theta)}{p(y)}\n= c(y) \\theta^y(1-\\theta)^{n-y}\\pi(\\theta),\n\\] where \\(c(y)\\) is a normalizing constant that depends only on \\(y\\), not on \\(\\theta\\). When using the uniform distribution, \\(\\pi(\\theta)\\), we can calculate \\(c(y)\\) easily as \\[\n\\begin{aligned}\n1&=\\int_0^1 c(y) \\theta^y(1-\\theta)^{n-y} d \\theta \\\\\n&=c(y) \\int_0^1 \\theta^y(1-\\theta)^{n-y} d \\theta \\\\\n&=c(y) \\frac{\\Gamma(y+1) \\Gamma(n-y+1)}{\\Gamma(n+2)}\n\\end{aligned}.\n\\] Hence, \\(c(y)=\\Gamma(n+2)/\\{\\Gamma(y+1) \\Gamma(n-y+1)\\}\\), and the posterior distribution is \\[\n\\begin{aligned}\np(\\theta \\mid y) & =\\frac{\\Gamma(n+2)}{\\Gamma(y+1) \\Gamma(n-y+1)} \\theta^y(1-\\theta)^{n-y} \\\\\n& =\\frac{\\Gamma(n+2)}{\\Gamma(y+1) \\Gamma(n-y+1)} \\theta^{(y+1)-1}(1-\\theta)^{(n-y+1)-1},\n\\end{aligned}\n\\] Which is exactly the \\(\\operatorname{beta}(y+1, n-y+1)\\). In the happiness example, we have \\(n=129\\) and \\(Y=\\sum Y_i=118\\), so the posterior distribution is \\(\\operatorname{beta}(119,12)\\), written as \\[\nn=129, Y \\equiv \\sum Y_i=118 \\quad \\Rightarrow \\quad \\theta \\mid\\{Y=118\\} \\sim \\operatorname{beta}(119,12) .\n\\]\nThis confirms the sufficiency result for this model and prior distribution, by showing that if \\(\\sum y_i = y = 118\\), \\(p(\\theta\\mid  y_1,\\dots y_n) = p(\\theta\\mid y) = \\mathrm{beta}(119, 12)\\). That is, the information contained in \\(\\{Y_1 = y_1, \\dots, Y_n = y_n\\}\\) is the same as the information contained in \\(\\{Y = y\\}\\), where \\(Y = \\sum Y_i\\) and \\(y = \\sum y_i\\). This show the posterior when we use uniform prior. One may ask, what if we use a different prior?\nPosterior distributions under beta prior distributions\nThe uniform prior distribution has \\(\\pi(\\theta) = 1\\) for all \\(\\theta\\in [0,1]\\). This distribution can be thought of as a beta prior distribution with parameters \\(a = 1, b = 1\\) \\[\n\\pi(\\theta)=\\frac{\\Gamma(2)}{\\Gamma(1) \\Gamma(1)} \\theta^{1-1}(1-\\theta)^{1-1}=\\frac{1}{1 \\times 1} 1 \\times 1=1\n\\] for all \\(\\theta \\in[0,1]\\).\n\nThe gamma function is defined as \\[\n\\Gamma(x)=\\int_0^{\\infty} t^{x-1} e^{-t} d t, \\quad x&gt;0.\n\\]\nIt satisfies the following properties:\n\n\\(\\Gamma(n)=(n-1)!\\) for any positive integer \\(n\\).\n\\(\\Gamma(x+1)=x \\Gamma(x)\\) for any \\(x&gt;0\\).\n\\(\\Gamma(1 / 2)=\\sqrt{\\pi}\\).\n\\(\\Gamma(1)=1\\) by convention.\n\n\nNow, from the previous part, recall that we have, \\[\n\\text { if }\\left\\{\\begin{array}{c}\n\\theta \\sim \\operatorname{beta}(1,1) \\text { (uniform) } \\\\\nY \\sim \\operatorname{binomial}(n, \\theta)\n\\end{array}\\right\\}, \\text { then }\\{\\theta \\mid Y=y\\} \\sim \\operatorname{beta}(1+y, 1+n-y).\n\\]\nTo get the posterior distribution under a general beta prior distribution, we just need to add the number of 1‚Äôs to the \\(\\alpha\\) parameter and the number of 0‚Äôs to the \\(\\beta\\) parameter. To see this, assume \\(\\theta\\sim \\operatorname{beta}(\\alpha, \\beta)\\), and \\(Y\\mid \\theta \\sim \\operatorname{binomial}(n, \\theta)\\). Then, once we observed \\(\\{Y=y\\}\\), by Bayes‚Äô theorem, the posterior distribution is \\[\n\\begin{aligned}\np(\\theta \\mid y) & =\\frac{\\pi(\\theta) p(y \\mid \\theta)}{p(y)} \\\\\n& =\\frac{1}{p(y)} \\times \\frac{\\Gamma(a+b)}{\\Gamma(a) \\Gamma(b)} \\theta^{a-1}(1-\\theta)^{b-1} \\times\\binom{ n}{y} \\theta^y(1-\\theta)^{n-y} \\\\\n& =c(n, y, a, b) \\times \\theta^{a+y-1}(1-\\theta)^{b+n-y-1} \\\\\n&\\propto \\beta(a+y, b+n-y) .\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNoteOne-to-one correspondence between the distribution\n\n\n\nNote that, there is a one-to-one correspondence between the prior distribution parameters and the posterior distribution parameters. Two distributions are said to be the same if\n\nTheir CDFs are the same.\nTheir PDFs are the same.\nAll of their moments are the same. This implies that they are equal if and only if the moment generating function or the probability generating functions are the same.\n\n\n\nWe have seen the beta-binomial example twice, which is an example of conjugate prior, let‚Äôs definite this formally,\n\nA class \\(\\mathcal{P}\\) of prior distribution for \\(\\theta\\) is said conjugate for the likelihood function \\(p(y \\mid \\theta)\\) if for every prior distribution \\(\\pi(\\theta) \\in \\mathcal{P}\\), the corresponding posterior distribution \\(p(\\theta \\mid y)\\) is also in \\(\\mathcal{P}\\), that is \\[\n\\pi(\\theta) \\in \\mathcal{P} \\Rightarrow p(\\theta \\mid y) \\in \\mathcal{P}.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nConjugate priors simplify posterior calculations, but they may not accurately reflect genuine prior beliefs. Still, mixtures of conjugate priors offer substantially greater flexibility while remaining computationally tractable.\n\n\nIf the likelihood \\(\\theta \\mid \\{Y = y\\} \\sim beta(a + y, b + n ‚àí y)\\), recall that\n\n\\(\\mathrm{E}[\\theta \\mid y]=\\frac{a+y}{a+b+n}\\)\n\\(\\operatorname{mode}[\\theta \\mid y]=\\frac{a+y-1}{a+b+n-2}\\)\n\\(\\operatorname{Var}[\\theta \\mid y]=\\frac{\\mathrm{E}[\\theta \\mid y] \\mathrm{E}[1-\\theta \\mid y]}{a+b+n+1}\\)\n\nThe posterior mean can be expressed as a weighted average of the prior mean and the maximum likelihood estimate (MLE) of \\(\\theta\\): \\[\n\\begin{aligned}\n\\mathrm{E}[\\theta \\mid y] & =\\frac{a+y}{a+b+n} \\\\\n& =\\frac{a+b}{a+b+n} \\times\\frac{a}{a+b}+\\frac{n}{a+b+n}\\times \\frac{y}{n} \\\\\n& =\\frac{a+b}{a+b+n} \\times \\text { prior expectation }+\\frac{n}{a+b+n} \\times \\text { data mean }\n\\end{aligned}\n\\] For this model and prior distribution, the posterior expectation (also known as the posterior mean) can be expressed as a weighted average of the prior expectation and the sample mean. The weights are proportional to the prior sample size a + b and the observed sample size n, respectively. This representation leads to a natural interpretation of the Beta prior parameters as prior data:\n\n\\(a \\approx \\text{``prior \\# of 1‚Äôs,''}\\)\n\\(b \\approx \\text{``prior \\# of 0‚Äôs,''}\\)\n\\(a + b \\approx \\text{``prior sample size.''}\\)\n\nWhen \\(n \\gg a+b\\), it is reasonable to expect that most of the information about \\(\\theta\\) should come from the data rather than from the prior distribution. This intuition is confirmed mathematically. In particular, when \\(n \\gg a + b\\),\n\n\\(\\frac{a + b}{a + b + n} \\approx 0\\),\n\\(\\mathbb{E}[\\theta \\mid y] \\approx \\frac{y}{n}\\),\n\\(\\mathrm{Var}(\\theta \\mid y) \\approx \\frac{1}{n}\\,\\frac{y}{n}\\left(1 - \\frac{y}{n}\\right)\\).\n\nThus, in large samples, the posterior mean approaches the sample proportion and the posterior variance shrinks at rate \\(1/n\\), reflecting increasing information from the data.\n\n\n\n\n\n\n\n\n\nPrediction\nAn important feature of Bayesian inference is the existence of a predictive distribution for new observations.\n\nThe posterior predictive distribution for a new observation \\(Y_{\\text{new}}\\) given the observed data \\(y\\) is obtained by integrating over the posterior distribution of \\(\\theta\\).\n\nReturning to our notation for binary data, let \\(y_1, \\ldots, y_n\\) be the observed outcomes from a sample of \\(n\\) binary rvs, and let \\(\\tilde Y \\in \\{0,1\\}\\) denote a future observation from the same population that has not yet been observed. The predictive distribution of \\(\\tilde Y\\) is defined as the conditional distribution of \\(\\tilde Y\\) given the observed data \\(\\{Y_1=y_1,\\ldots,Y_n=y_n\\}\\). For conditionally i.i.d. binary observations, the predictive distribution can be derived by integrating out the unknown parameter \\(\\theta\\): \\[\n\\begin{aligned}\n\\operatorname{Pr}\\left(\\tilde{Y}=1 \\mid y_1, \\ldots, y_n\\right) & =\\int \\operatorname{Pr}\\left(\\tilde{Y}=1, \\theta \\mid y_1, \\ldots, y_n\\right) d \\theta \\\\\n& =\\int \\operatorname{Pr}\\left(\\tilde{Y}=1 \\mid \\theta, y_1, \\ldots, y_n\\right) p\\left(\\theta \\mid y_1, \\ldots, y_n\\right) d \\theta \\\\\n& =\\int  p\\left(\\theta \\mid y_1, \\ldots, y_n\\right) \\theta d \\theta \\\\\n& =\\mathrm{E}\\left[\\theta \\mid y_1, \\ldots, y_n\\right]\\\\\n&=\\frac{a+\\sum_{i=1}^n y_i}{a+b+n}.\n\\end{aligned}\n\\]\nHence, we also have, \\[\n\\operatorname{Pr}\\left(\\tilde{Y}=0 \\mid y_1, \\ldots, y_n\\right)  =1-\\mathrm{E}\\left[\\theta \\mid y_1, \\ldots, y_n\\right]=\\frac{b+\\sum_{i=1}^n\\left(1-y_i\\right)}{a+b+n} .\n\\]\n\n\n\n\n\n\nNoteProperties of the predictive distribution\n\n\n\n\nIt does not depend on any unknown quantities. If it did, it could not be used to make predictions.\nIt depends on the observed data. In particular, \\(\\tilde Y\\) is not independent of \\(Y_1,\\ldots,Y_n\\), because the observed data provide information about \\(\\theta\\), which in turn influences \\(\\tilde Y\\). If \\(\\tilde Y\\) were independent of the observed data, learning from data would be impossible.\n\n\n\n\nThe uniform prior distribution on [0,1], also known as the \\(\\text{Beta}(1,1)\\) prior, can be interpreted as containing the same information as a hypothetical prior dataset consisting of one success (‚Äú1‚Äù) and one failure (‚Äú0‚Äù).\nUnder this prior, the posterior predictive probability of a future success is \\[\n\\Pr(\\tilde Y = 1 \\mid Y = y)\n= \\mathbb{E}[\\theta \\mid Y = y]\n= \\frac{2}{2+n}\\cdot\\frac{1}{2}\n    + \\frac{n}{2+n}\\cdot\\frac{y}{n}.\n\\]\nThis expression highlights that the predictive probability is a weighted average of:\n\nthe prior mean \\(1/2\\), and\nthe sample proportion \\(y/n\\),\n\nwith weights proportional to the prior sample size 2 and the observed sample size n, respectively.\nThe posterior mode under this prior is \\[\n\\text{mode}(\\theta \\mid Y = y) = \\frac{y}{n},\n\\] where \\[\nY = \\sum_{i=1}^n Y_i.\n\\]\nAt first glance, the discrepancy between these two posterior summaries may seem surprising. However, it reflects the fact that different summaries capture different features of the posterior distribution.\nTo see this clearly, consider the case \\(Y = 0\\). In this case, \\[\n\\text{mode}(\\theta \\mid Y = 0) = 0,\n\\] but the predictive probability remains \\[\n\\Pr(\\tilde Y = 1 \\mid Y = 0) = \\frac{1}{2+n}.\n\\]\nThus, even when no successes have been observed, the Bayesian predictive distribution assigns a positive probability to a future success due to the prior information. This illustrates how Bayesian prediction naturally balances prior beliefs with observed data.\n\n\n\n3.2.2 Confidence Regions: Bayesian v.s. Frequentist\nIf it often desirable to identify the regions of the parameter space that are likely to contain the true value of the parameter. To do this, after observing the data \\(Y=y\\), we can construct an interval \\([\\ell(y),u(y)]\\) that is likely to contain the true value of \\(\\theta\\), i.e., the probability that \\(\\ell(y)&lt;\\theta&lt;u(y)\\) is large. There are two different ways to interpret this probability, leading to the concepts of Bayesian coverage and frequentist coverage.\n\nAn interval \\([\\ell(y), u(y)]\\), based on the observed data \\(Y = y\\), has 100(1-\\(\\alpha\\))% Bayesian coverage for \\(\\theta\\) if \\[\n\\Pr(\\ell(y) &lt; \\theta &lt; u(y)\\mid Y = y) = 1-\\alpha.\n\\]\n\n\nA random interval \\([\\ell(Y ), u(Y )]\\) has 100(1-\\(\\alpha\\))% frequentist coverage for \\(\\theta\\) if, before the data are gathered, \\[\n\\Pr(\\ell(Y ) &lt; \\theta &lt; u(Y )\\mid\\theta) = 1-\\alpha.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nIn a sense, the frequentist and Bayesian notions of coverage describe pre experimental and post experimental perspectives, respectively.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#frequentist-vs-bayesian-coverage",
    "href": "03_bi-1par.html#frequentist-vs-bayesian-coverage",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.3 Frequentist vs Bayesian Coverage",
    "text": "3.3 Frequentist vs Bayesian Coverage\nYou may recall an important point often emphasized in introductory statistics courses. Suppose we observe data \\(Y=y\\) and compute a frequentist confidence interval \\[\n[l(y),\\,u(y)].\n\\] Once the data are observed, the parameter \\(\\theta\\) is treated as fixed, not random.\nTherefore, \\[\n\\Pr\\!\\bigl(l(y) &lt; \\theta &lt; u(y) \\mid \\theta\\bigr)\n=\n\\begin{cases}\n1, & \\text{if } \\theta \\in [l(y),u(y)],\\\\\n0, & \\text{if } \\theta \\notin [l(y),u(y)].\n\\end{cases}\n\\]\nThis highlights a key limitation of frequentist confidence intervals:\n\nThey do not admit a post-experimental probability interpretation.\n\nAfter observing the data, it is not meaningful, from a frequentist perspective, to say that there is a 95% probability that \\(\\theta\\) lies in the computed interval.\nWhat Frequentist Coverage Means\nAlthough this interpretation may feel unsatisfying, frequentist coverage is still useful in many situations. Imagine repeatedly running many independent experiments and constructing a confidence interval for each one.\nIf each interval procedure has 95% frequentist coverage, then:\n\nAbout 95% of the intervals will contain the true parameter value.\n\nThis is a long-run, repeated-sampling interpretation, not a statement about any single observed interval.\nCan Bayesian and Frequentist Coverage Agree?\nA natural question is whether a confidence interval can simultaneously have:\n\na Bayesian interpretation, i.e., a 100(1-\\(\\alpha\\))% posterior probability that \\(\\theta\\) lies in the interval, and\napproximately 100(1-\\(\\alpha\\))% frequentist coverage.\n\nHartigan (1966) showed that, for the types of intervals considered in Hopf (2009), an interval that has 95% Bayesian coverage additionally has the property that \\[\n\\Pr\\!\\bigl(l(Y) &lt; \\theta &lt; u(Y) \\mid \\theta\\bigr)\n=\n0.95 + \\varepsilon_n,\n\\] where the error term satisfies \\(|\\varepsilon_n| &lt; a/n\\) for some constant \\(a\\). This result implies that, an interval with 95% Bayesian coverage, will also have approximately 95% frequentist coverage, at least asymptotically, as the sample size \\(n\\) grows.\nIn other words, under suitable conditions, Bayesian credible intervals and frequentist confidence intervals can agree in large samples, even though their interpretations are fundamentally different. Keep in mind that most non-Bayesian methods of constructing 100(1-\\(\\alpha\\))% confidence intervals also only achieve their nominal coverage probability asymptotically.\n\n\n\n\n\n\nNoteReminder\n\n\n\nThis reconciliation is important, but it should not obscure the conceptual distinction:\n\nfrequentist coverage is a pre-experimental property of a procedure,\nBayesian coverage is a post-experimental probability statement about \\(\\theta\\) given the data.\n\n\n\nFor further discussion of the similarities between Bayesian and frequentist intervals, see Severini (1991) and Sweeting (2001).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#posterior-quantile-intervals",
    "href": "03_bi-1par.html#posterior-quantile-intervals",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.4 Posterior Quantile Intervals",
    "text": "3.4 Posterior Quantile Intervals\nOne of the simplest ways to construct a Bayesian credible interval is to use posterior quantiles. To form a \\(100(1-\\alpha)\\%\\) credible interval for \\(\\theta\\), find numbers \\(\\theta_{\\alpha/2} &lt; \\theta_{1-\\alpha/2}\\) such that\n\n\\(\\Pr(\\theta &lt; \\theta_{\\alpha/2} \\mid Y=y) = \\alpha/2,\\)\n\\(\\Pr(\\theta &gt; \\theta_{1-\\alpha/2} \\mid Y=y) = \\alpha/2,\\)\n\nwhere \\(\\theta_{\\alpha/2}\\) and \\(\\theta_{1-\\alpha/2}\\) are the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) posterior quantiles of \\(\\theta\\). By construction, \\[\n\\begin{aligned}\n\\operatorname{Pr}\\left(\\theta \\in\\left[\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\right] \\mid Y=y\\right) & =1-\\operatorname{Pr}\\left(\\theta \\notin\\left[\\theta_{\\alpha / 2}, \\theta_{1-\\alpha / 2}\\right] \\mid Y=y\\right) \\\\\n& =1-\\left[\\operatorname{Pr}\\left(\\theta&lt;\\theta_{\\alpha / 2} \\mid Y=y\\right)+\\operatorname{Pr}\\left(\\theta&gt;\\theta_{1-\\alpha / 2} \\mid Y=y\\right)\\right] \\\\\n& =1-\\alpha .\n\\end{aligned}\n\\]\n\nSuppose we observe \\(n = 10\\) conditionally independent Bernoulli trials and obtain \\(Y = 2\\) successes. Using a uniform prior for \\(\\theta\\), \\(\\theta \\sim \\mathrm{Beta}(1,1),\\) the posterior distribution is \\[\n\\theta \\mid \\{Y=2\\} \\sim \\mathrm{Beta}(1+2,\\;1+8) = \\mathrm{Beta}(3,9).\n\\]\nA 95% posterior confidence interval can be obtained from by 2.5% and 97.5% quantiles of this Beta distribution \\([\\theta_{0.025}, \\theta_{0.975}]\\). In this case, \\[\n\\theta_{0.025} \\approx 0.06,\n\\qquad\n\\theta_{0.975} \\approx 0.52,\n\\] so \\[\n\\Pr(0.06 \\le \\theta \\le 0.52 \\mid Y=2) = 0.95.\n\\]\nThis interval has a direct probabilistic interpretation: given the observed data, there is a 95% posterior probability that \\(\\theta\\) lies in this range.\n\nb &lt;- a &lt;- 1 # prior parameter\nn &lt;- 10 ; y &lt;- 2 # data\nqbeta(c(0.025, 0.975), a + y, b + n - y)\n\n[1] 0.06021773 0.51775585\n\na_post &lt;- a + y\nb_post &lt;- b + (n - y)\n\n# 95% quantile-based credible interval\nci &lt;- qbeta(c(0.025, 0.975), a_post, b_post)\nci_low &lt;- ci[1]\nci_high &lt;- ci[2]\n\n# Grid for plotting posterior density\ntheta &lt;- seq(0, 1, length.out = 2000)\ndf &lt;- data.frame(theta = theta, density = dbeta(theta, a_post, b_post))\n\n# Plot: posterior density curve + two vertical CI bars\nggplot(df, aes(x = theta, y = density)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = ci_low, linewidth = 0.8) +\n  geom_vline(xintercept = ci_high, linewidth = 0.8) +\n  labs(\n    title = \"Beta Posterior with 95% Quantile-Based Credible Interval\",\n    subtitle = sprintf(\n      \"Data: n=%d, y=%d | Prior: Beta(%d,%d) | Posterior: Beta(%d,%d) | 95%% CI: [%.3f, %.3f]\",\n      n, y, a, b, a_post, b_post, ci_low, ci_high\n    ),\n    x = expression(theta),\n    y = expression(p(theta * \"|\" * y))\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nHighest posterior density (HPD) region\nThe Figure above illustrates the posterior distribution of \\(\\theta\\) for the binomial example with a uniform prior, together with a 95% quantile-based credible interval. Notice an important feature of the plot:\n\nThere exist values of \\(\\theta\\) outside the quantile-based interval that have higher posterior density than some values inside the interval.\n\nThis observation suggests that the quantile-based interval may not be the most efficient way to summarize posterior uncertainty. In particular, it motivates a more restrictive type of credible region that concentrates on the most plausible parameter values.\n\nA \\(100(1-\\alpha)\\%\\) HPD region is a subset of the sample space, \\(s(y) \\subset \\Theta\\) such that:\n\n\\(\\Pr(\\theta \\in s(y) \\mid Y = y) = 1 - \\alpha\\), and\nIf \\(\\theta_a \\in s(y)\\) and \\(\\theta_b \\notin s(y)\\), then \\(p(\\theta_a \\mid Y = y) \\ge p(\\theta_b \\mid Y = y).\\)\n\nIn words, an HPD region contains the parameter values with the largest posterior density, subject to containing probability mass \\(1-\\alpha\\).\n\nObserved that, all points inside an HPD region are at least as plausible as any point outside the region, according to the posterior distribution. This property distinguishes HPD regions from quantile-based intervals, which are defined purely by cumulative probability and may include low-density values while excluding higher-density ones.\n\n\n\n\n\n\n\n\n\nAn HPD region can be constructed conceptually as follows:\n\n\n\n\n\n\nNoteAlgorithm to construct an HPD region\n\n\n\n\nBegin with a horizontal line above the posterior density curve.\nGradually lower the line.\nAt each height, include all values of \\(\\theta\\) whose posterior density exceeds the line.\nStop lowering the line once the total posterior probability of the included region reaches \\(1-\\alpha\\).\n\n\n\nThis procedure guarantees that the retained region consists of the most probable values of \\(\\theta\\).\nHPD Regions and Multimodality\nIf the posterior density is unimodal, the HPD region will typically be a single interval. However, if the posterior density is multimodal (having multiple peaks), the HPD region need not be an interval; it may consist of several disjoint subsets of the parameter space.\n\nIn the binomial example with \\(n=10\\), \\(Y=2\\), and a uniform prior, the posterior distribution is \\(\\mathrm{Beta}(3,9)\\).\nFor this posterior:\n\nThe 95% quantile-based credible interval is approximately \\([0.06,\\,0.52]\\).\nThe 95% HPD region is approximately \\([0.04,\\,0.48]\\).\n\nThe HPD region is narrower, and therefore more precise, than the quantile-based interval, while still containing 95% of the posterior probability.\nBoth intervals are valid Bayesian credible intervals, but they summarize posterior uncertainty in different ways.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#the-poisson-model",
    "href": "03_bi-1par.html#the-poisson-model",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.5 The Poisson Model",
    "text": "3.5 The Poisson Model\nAnother commonly used distribution is the Poisson, in this case, the measurement are the integer numbers. Some examples include number of coin tosses, the number of friends they have, or the number of birthday celebrations have a person have. In these situations, the sample space is \\(\\mathcal{Y}=\\{0,1,2,\\ldots\\}.\\) There are other possible models for those situation, but perhaps the simplest probability model on \\(\\mathcal{Y}\\) is the Poisson model.\nPoisson distribution\nRecall that a random variable \\(Y\\) has a Poisson distribution with mean \\(\\theta\\) if \\[\n\\Pr(Y=y\\mid \\theta)=\\mathrm{dpois}(y,\\theta)=\\frac{\\theta^y e^{-\\theta}}{y!},\n\\qquad y\\in\\{0,1,2,\\ldots\\}.\n\\]\nFor such a random variable, we have,\n\n\\(\\mathbb{E}(Y)=\\theta\\)\n\\(\\mathrm{Var}(Y)=\\theta.\\)\n\nPeople sometimes use the Poisson distribution to model count data because of its simplicity and its ability to model events that occur independently over a fixed interval of time or space. The Poisson distribution is particularly useful when the events being counted are rare or infrequent, and when the average rate of occurrence is known. Note that, in this model, the mean and the variance are the same, which is a property that can be useful in certain applications; One may call this property as ‚Äúmean-variance relationship‚Äù.\n\n\n\n\n\n\n\n\n\n\n3.5.1 Inference on the Posterior for Poisson Model\nSuppose we observe data \\(Y_1, \\ldots, Y_n\\) and model them as conditionally independent Poisson random variables with common mean \\(\\theta\\), i.e., \\[\nY_i \\mid \\theta \\sim \\text{Poisson}(\\theta), \\qquad i = 1,\\ldots,n.\n\\]\nThe joint probability mass function of the data, given \\(\\theta\\), is \\[\n\\Pr(Y_1 = y_1, \\ldots, Y_n = y_n \\mid \\theta)\n= \\prod_{i=1}^n p(y_i \\mid \\theta).\n\\]\nUsing the Poisson pmf, \\[\np(y_i \\mid \\theta) = \\frac{\\theta^{y_i} e^{-\\theta}}{y_i!},\n\\] we obtain \\[\n\\begin{aligned}\n\\Pr(Y_1 = y_1, \\ldots, Y_n = y_n \\mid \\theta)\n&= \\prod_{i=1}^n \\frac{\\theta^{y_i} e^{-\\theta}}{y_i!}\\\\\n&= c(y_1,\\ldots,y_n)\\,\\theta^{\\sum_{i=1}^n y_i} e^{-n\\theta},\n\\end{aligned}\n\\] where \\[\nc(y_1,\\ldots,y_n) = \\prod_{i=1}^n \\frac{1}{y_i!},\n\\] which does not depend on \\(\\theta\\). This expression shows that the likelihood depends on the data only through the statistic \\(S = \\sum_{i=1}^n Y_i.\\)\nAs in the binomial model, the statistic \\(S = \\sum_{i=1}^n Y_i\\) contains all information in the data about \\(\\theta\\).\nIndeed, \\[\n\\sum_{i=1}^n Y_i \\mid \\theta \\sim \\text{Poisson}(n\\theta),\n\\] and we therefore say that \\(S\\) is a sufficient statistic for \\(\\theta\\).\n\n\n3.5.2 Comparing posterior beliefs\nTo compare two values \\(\\theta_a\\) and \\(\\theta_b\\) a posteriori, consider the posterior odds: \\[\n\\frac{p(\\theta_a \\mid y_1,\\ldots,y_n)}\n     {p(\\theta_b \\mid y_1,\\ldots,y_n)}.\n\\]\nBy Bayes‚Äô rule, \\[\np(\\theta \\mid y_1,\\ldots,y_n)\n\\propto \\pi(\\theta)\\,p(y_1,\\ldots,y_n \\mid \\theta)\n\\propto \\pi(\\theta)\\,\\theta^{\\sum_{i=1}^n y_i} e^{-n\\theta}.\n\\]\nTherefore, \\[\n\\frac{p(\\theta_a \\mid y)}\n     {p(\\theta_b \\mid y)}\n=\n\\frac{\\theta_a^{\\sum y_i} e^{-n\\theta_a} p(\\theta_a)}\n     {\\theta_b^{\\sum y_i} e^{-n\\theta_b} p(\\theta_b)}.\n\\]\nThis expression highlights how posterior beliefs balance prior information with evidence from the data.\nConjugate prior for the Poisson model\nWe now seek a prior distribution for \\(\\theta\\) that leads to a posterior distribution of the same functional form. From the likelihood, \\[\np(\\theta \\mid y)\n\\propto p(\\theta)\\,\\theta^{\\sum y_i} e^{-n\\theta},\n\\] we see that a conjugate prior must involve terms of the form \\[\n\\theta^{c_1} e^{-c_2 \\theta}\n\\] for some constants \\(c_1\\) and \\(c_2\\). The simplest family of distributions with this structure is the family of Gamma distributions, also called Gamma family.\n\n\n3.5.3 Gamma distribution\nA positive random variable \\(\\theta\\) has a Gamma\\((a,b)\\) distribution if \\[\n\\pi(\\theta)\n=\n\\frac{b^a}{\\Gamma(a)} \\theta^{a-1} e^{-b\\theta},\n\\qquad \\theta &gt; 0,\n\\] where \\(a &gt; 0\\) is the shape parameter and \\(b &gt; 0\\) is the rate parameter.\nFor a Gamma\\((a,b)\\) random variable,\n\n\\(\\mathbb{E}(\\theta) = a/b,\\)\n\\(\\mathrm{Var}(\\theta) = a/b^2.\\)\n\\(\\operatorname{mode}[\\theta]=\\left\\{\\begin{array}{cc}\n(a-1) / b & \\text { if } a&gt;1 \\\\\n0 & \\text { if } a \\leq 1\n\\end{array} .\\right.\\)\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.4 Posterior distribution of \\(\\theta\\)\nIf the prior is \\(\\theta \\sim \\text{Gamma}(a,b),\\) then combining the prior with the Poisson likelihood yields \\[\n\\begin{aligned}\np\\left(\\theta \\mid y_1, \\ldots, y_n\\right) & =\\pi(\\theta) \\times p\\left(y_1, \\ldots, y_n \\mid \\theta\\right) / p\\left(y_1, \\ldots, y_n\\right) \\\\\n& =\\left\\{\\theta^{a-1} e^{-b \\theta}\\right\\} \\times\\left\\{\\theta^{\\sum y_i} e^{-n \\theta}\\right\\} \\times c\\left(y_1, \\ldots, y_n, a, b\\right) \\\\\n& =\\left\\{\\theta^{a+\\sum y_i-1} e^{-(b+n) \\theta}\\right\\} \\times c\\left(y_1, \\ldots, y_n, a, b\\right)\\\\\n&\\propto\n\\theta^{a+\\sum y_i-1} e^{-(b+n)\\theta}.\n\\end{aligned}\n\\]\nThus, by the uniqueness theorem (of the density), the posterior distribution is \\[\n\\theta \\mid y_1,\\ldots,y_n\n\\sim \\text{Gamma}\\big(a + \\sum_{i=1}^n y_i,\\; b + n\\big).\n\\]\nThis shows that the Gamma distribution is conjugate to the Poisson likelihood.\nInterpretation\nPosterior inference for the Poisson model is therefore straightforward:\n\nThe data enter only through the sufficient statistic \\(\\sum Y_i\\);\nThe posterior mean is \\[\n\\begin{aligned}\n\\mathrm{E}\\left[\\theta \\mid y_1, \\ldots, y_n\\right] & =\\frac{a+\\sum y_i}{b+n} \\\\\n& =\\frac{b}{b+n} \\frac{a}{b}+\\frac{n}{b+n} \\frac{\\sum y_i}{n}\n\\end{aligned}\n\\]\nThis decomposition shows that it is a convex combination of the prior mean \\(a/b\\) and the sample mean \\(\\bar{y}\\), and gives a useful information\n\\(b\\) acts like the number of prior observations;\n\\(a\\) acts like the total count from those \\(b\\) observations;\n\\(a/b\\) is the prior mean.\nIncreasing the sample size \\(n\\) reduces posterior uncertainty, because the ifnromation from the data dominates the prior belief. To see this, we have, for \\(n\\gg b\\), we have\n\\(\\mathrm{E}[\\theta \\mid y] \\approx \\bar{y}\\),\n\\(\\mathrm{Var}(\\theta \\mid y) \\approx \\bar{y}/n\\).\n\nThis conjugate structure makes the Poisson‚ÄìGamma model a convenient and interpretable starting point for Bayesian analysis of count data.\n\n\n3.5.5 Posterior predictive distribution for Poisson Model\nWe have seen that, the Bayesian prediction for a future observation \\(\\tilde{y}\\) is based on the posterior predictive distribution. In Gamma-Poisson model, we have \\[\np(\\tilde{y} \\mid y_1,\\ldots,y_n)\n=\n\\int_0^\\infty\np(\\tilde{y} \\mid \\theta, y)\\,\np(\\theta \\mid y_1,\\ldots,y_n)\n\\, d\\theta.\n\\]\nFor the Poisson model, \\[\np(\\tilde{y} \\mid \\theta) = \\text{Poisson}(\\theta),\n\\qquad\np(\\theta \\mid y) = \\text{Gamma}\\!\\left(a + \\sum y_i,\\; b + n\\right).\n\\]\nSubstituting, \\[\np(\\tilde{y} \\mid y)\n=\n\\int_0^\\infty\n\\text{dpois}(\\tilde{y},\\theta)\\,\n\\text{dgamma}\\!\\left(\\theta,\\; a + \\sum y_i,\\; b + n\\right)\n\\, d\\theta.\n\\]\nWriting this integral explicitly, \\[\np(\\tilde{y} \\mid y)\n=\n\\int_0^\\infty\n\\frac{\\theta^{\\tilde{y}} e^{-\\theta}}{\\tilde{y}!}\n\\cdot\n\\frac{(b+n)^{a+\\sum y_i}}{\\Gamma(a+\\sum y_i)}\n\\theta^{a+\\sum y_i-1} e^{-(b+n)\\theta}\n\\, d\\theta.\n\\]\nCombining terms, we have \\[\np(\\tilde{y} \\mid y)\n=\n\\frac{(b+n)^{a+\\sum y_i}}{\\tilde{y}!\\,\\Gamma(a+\\sum y_i)}\n\\int_0^\\infty\n\\theta^{a+\\sum y_i+\\tilde{y}-1}\ne^{-(b+n+1)\\theta}\n\\, d\\theta.\n\\]\nThe integral term seems difficult to evaluate in a glance, but there is actually a clear ‚Äútrick‚Äù to help use. Recall the density of a Gamma distribution: \\[\n1 = \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta\\theta}\\, d\\theta, \\quad, \\alpha,\\beta&gt;0,\n\\] which implies \\[\n\\int_0^\\infty \\theta^{\\alpha-1} e^{-\\beta\\theta}\\, d\\theta\n=\n\\frac{\\Gamma(\\alpha)}{\\beta^\\alpha},\n\\qquad \\alpha,\\beta&gt;0.\n\\]\nApplying this with \\[\n\\alpha = a + \\sum y_i + \\tilde{y},\n\\qquad\n\\beta = b + n + 1,\n\\] we obtain \\[\n\\int_0^\\infty\n\\theta^{a+\\sum y_i+\\tilde{y}-1}\ne^{-(b+n+1)\\theta}\n\\, d\\theta\n=\n\\frac{\\Gamma(a+\\sum y_i+\\tilde{y})}{(b+n+1)^{a+\\sum y_i+\\tilde{y}}}.\n\\]\nSubstituting back and simplifying, \\[\np(\\tilde{y} \\mid y_1,\\ldots,y_n)\n=\n\\frac{\\Gamma(a+\\sum y_i+\\tilde{y})}{\\Gamma(a+\\sum y_i)\\,\\tilde{y}!}\n\\left(\\frac{b+n}{b+n+1}\\right)^{a+\\sum y_i}\n\\left(\\frac{1}{b+n+1}\\right)^{\\tilde{y}},\n\\] for \\(\\tilde{y} \\in \\{0,1,2,\\ldots\\}\\). We relieazed that it is a negative binomial distribution with parameters \\((a + \\sum y_i, b + n)\\). That is, \\(\\tilde{Y} \\mid y_1,\\ldots,y_n \\sim \\text{NB}(a + \\sum y_i, b + n)\\). As a result, we have the mean and variance of the posterior predictive distribution\n\\[\\mathbb{E}(\\tilde{Y} \\mid y) = \\frac{a + \\sum y_i}{b + n}=\\mathbb{E}[\\theta \\mid y], \\]\nand\n\\[\n\\begin{aligned}\n\\mathrm{Var}\\left[\\tilde{Y} \\mid y_1, \\ldots, y_n\\right]\n& =\\frac{a+\\sum y_i}{b+n} \\frac{b+n+1}{b+n}\\\\\n&=\\operatorname{Var}\\left[\\theta \\mid y_1, \\ldots y_n\\right] \\times(b+n+1) \\\\\n& =\\mathbb{E}\\left[\\theta \\mid y_1, \\ldots, y_n\\right] \\times \\frac{b+n+1}{b+n} \\\\\n& = \\frac{a + \\sum y_i}{b + n} \\times \\frac{b + n + 1}{b + n}\n\\end{aligned},\n\\] respectively\nInterpretation and Take away\nRecall that the predictive variance is to some extent a measure of our posterior uncertainty about a new observation \\(\\tilde{Y}\\). It reflects two sources of uncertainty:\n\nSampling variability (Sampling) For a Poisson model, the variance of \\(Y\\) given \\(\\theta\\) is equal to \\(\\theta\\).\nParameter uncertainty (Population) When \\(\\theta\\) is unknown, uncertainty about \\(\\theta\\) inflates the variance of future observations.\n\nFor large \\(n\\), the data dominate the prior: \\[\n\\frac{b + n + 1}{b + n} \\approx 1,\n\\] so predictive uncertainty is driven primarily by sampling variability. In this case, the uncertainy abbout \\(\\theta\\) is small which for the Possion model is equal to \\(theta\\)\nFor small \\(n\\), posterior uncertainty about \\(\\theta\\) is substantial, and \\[\n\\frac{b + n + 1}{b + n} &gt; 1,\n\\] leading to larger predictive variance than under a fixed-\\(\\theta\\) Poisson model (i.e., larger than just the sampling variability).\n\nBayesian prediction naturally incorporates both sampling variability and parameter uncertainty.\n\nThis completes posterior inference and prediction for the Gamma-Poisson model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#example-birth-rates",
    "href": "03_bi-1par.html#example-birth-rates",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.6 Example: Birth rates",
    "text": "3.6 Example: Birth rates\n\nOver the course of the 1990s, the General Social Survey collected data on the educational attainment and number of children of women who were 40 years old at the time of participation in the survey. These women were in their 20s during the 1970s, a period of historically low fertility rates in the United States.\nIn this example, we compare women with a bachelor‚Äôs degree to those without a bachelor‚Äôs degree in terms of their numbers of children.\n\n\n\n\n\n\n\n\n\nSampling models\nLet\n\n\\(Y_{i,1}\\) denote the number of children for woman \\(i\\) without a bachelor‚Äôs degree, \\(i=1,\\dots,n_1\\),\n\\(Y_{i,2}\\) denote the number of children for woman \\(i\\) with a bachelor‚Äôs degree, \\(i=1,\\dots,n_2\\).\n\nThen, since it is a coint data, we assume the following Poisson sampling models:\n\\[\nY_{1,1}, \\ldots, Y_{n_1,1} \\mid \\theta_1 \\sim \\text{i.i.d. Poisson}(\\theta_1),\n\\]\n\\[\nY_{1,2}, \\ldots, Y_{n_2,2} \\mid \\theta_2 \\sim \\text{i.i.d. Poisson}(\\theta_2).\n\\]\nHere, \\(\\theta_1\\) and \\(\\theta_2\\) represent the population mean birth rates for the two groups.\nData summaries\nThe empirical summaries of the data are:\n\nNo college degree: \\[\nn_1 = 111, \\qquad \\sum_{i=1}^{n_1} Y_{i,1} = 217, \\qquad \\bar{Y}_1 = 1.95\n\\]\nBachelor‚Äôs degree or higher: \\[\nn_2 = 44, \\qquad \\sum_{i=1}^{n_2} Y_{i,2} = 66, \\qquad \\bar{Y}_2 = 1.50\n\\]\n\nPrior distributions\nWe place independent Gamma priors on the two population means:\n\\[\n\\theta_1, \\theta_2 \\sim \\text{i.i.d. Gamma}(a=2, b=1),\n\\]\nwhere the Gamma distribution is parameterized by shape \\(a\\) and rate \\(b\\).\nThe prior mean is \\(a/b = 2\\), representing weak prior information corresponding to roughly one prior observation with an average count of two. (Why?)\nPosterior distributions\nUnder the Poisson‚ÄìGamma conjugate model, the posterior distributions are\n\\[\n\\theta_1 \\mid y \\sim \\text{Gamma}(a + \\sum Y_{i,1}, \\, b + n_1)\n= \\text{Gamma}(219, 112),\n\\]\n\\[\n\\theta_2 \\mid y \\sim \\text{Gamma}(a + \\sum Y_{i,2}, \\, b + n_2)\n= \\text{Gamma}(68, 45).\n\\]\nThese posteriors summarize our updated beliefs about the two population mean birth rates after observing the data. From the Gamma posterior distributions, we can compute posterior means, modes, and 95% quantile-based credible intervals.\n\nPosterior means: \\[\n\\mathbb{E}(\\theta_1 \\mid y) = \\frac{219}{112} \\approx 1.96,\n\\qquad\n\\mathbb{E}(\\theta_2 \\mid y) = \\frac{68}{45} \\approx 1.51\n\\]\nPosterior modes: \\[\n\\text{mode}(\\theta_1 \\mid y) = \\frac{218}{112} \\approx 1.95,\n\\qquad\n\\text{mode}(\\theta_2 \\mid y) = \\frac{67}{45} \\approx 1.49\n\\]\n\n\n# ============================================================\n# Poisson‚ÄìGamma model: two-group posterior summaries\n# Prior:  theta ~ Gamma(a, b)  with shape = a, rate = b\n# Data:   Yi | theta ~ i.i.d. Poisson(theta)\n# Posterior: theta | y ~ Gamma(a + sum(y), b + n)\n# ============================================================\n\n# ----------------------------\n# 1) Inputs: prior + data\n# ----------------------------\na &lt;- 2\nb &lt;- 1\n\ngroup &lt;- data.frame(\n  group = c(\"Less than bachelor's\", \"Bachelor's or higher\"),\n  n     = c(111, 44),\n  sum_y = c(217, 66)\n)\n\n# ----------------------------\n# 2) Posterior functions\n# ----------------------------\npost_shape &lt;- function(a, sum_y) a + sum_y\npost_rate  &lt;- function(b, n)     b + n\n\npost_mean &lt;- function(shape, rate) shape / rate\npost_mode &lt;- function(shape, rate) ifelse(shape &gt; 1, (shape - 1) / rate, 0)\n\npost_ci &lt;- function(shape, rate, level = 0.95) {\n  alpha &lt;- (1 - level) / 2\n  qgamma(c(alpha, 1 - alpha), shape = shape, rate = rate)\n}\n\n# ----------------------------\n# 3) Compute summaries\n# ----------------------------\nout &lt;- within(group, {\n  shape_post &lt;- post_shape(a, sum_y)\n  rate_post  &lt;- post_rate(b, n)\n\n  mean_post  &lt;- post_mean(shape_post, rate_post)\n  mode_post  &lt;- post_mode(shape_post, rate_post)\n\n  ci_post    &lt;- t(mapply(post_ci, shape_post, rate_post))\n  ci_lower   &lt;- ci_post[, 1]\n  ci_upper   &lt;- ci_post[, 2]\n})\n\n# ----------------------------\n# 4) Print results clearly\n# ----------------------------\ncat(\"Prior: theta ~ Gamma(shape = \", a, \", rate = \", b, \")\\n\\n\", sep = \"\")\n\nPrior: theta ~ Gamma(shape = 2, rate = 1)\n\nfor (i in seq_len(nrow(out))) {\n  cat(\"------------------------------------------------------------\\n\")\n  cat(\"Group: \", out$group[i], \"\\n\", sep = \"\")\n  cat(\"n = \", out$n[i], \",  sum(y) = \", out$sum_y[i], \"\\n\", sep = \"\")\n  cat(\"Posterior: theta | y ~ Gamma(shape = \", out$shape_post[i],\n      \", rate = \", out$rate_post[i], \")\\n\", sep = \"\")\n  cat(sprintf(\"Posterior mean = %.6f\\n\", out$mean_post[i]))\n  cat(sprintf(\"Posterior mode = %.6f\\n\", out$mode_post[i]))\n  cat(sprintf(\"Posterior 95%% CI = [%.6f, %.6f]\\n\", out$ci_lower[i], out$ci_upper[i]))\n}\n\n------------------------------------------------------------\nGroup: Less than bachelor's\nn = 111,  sum(y) = 217\nPosterior: theta | y ~ Gamma(shape = 219, rate = 112)\nPosterior mean = 1.955357\nPosterior mode = 1.946429\nPosterior 95% CI = [1.704943, 2.222679]\n------------------------------------------------------------\nGroup: Bachelor's or higher\nn = 44,  sum(y) = 66\nPosterior: theta | y ~ Gamma(shape = 68, rate = 45)\nPosterior mean = 1.511111\nPosterior mode = 1.488889\nPosterior 95% CI = [1.173437, 1.890836]\n\nsummary_tbl &lt;- out[, c(\"group\", \"n\", \"sum_y\", \"mean_post\", \"mode_post\", \"ci_lower\", \"ci_upper\")]\nprint(summary_tbl, row.names = FALSE)\n\n                group   n sum_y mean_post mode_post ci_lower ci_upper\n Less than bachelor's 111   217  1.955357  1.946429 1.704943 2.222679\n Bachelor's or higher  44    66  1.511111  1.488889 1.173437 1.890836\n\n\nThe posterior distributions indicate strong evidence that the mean birth rate is higher for women without a bachelor‚Äôs degree (i.e., \\(\\theta_1&gt;\\theta_2\\)). For example,\n\\[\n\\Pr(\\theta_1 &gt; \\theta_2 \\mid y) \\approx 0.97.\n\\]\nPosterior predictive distributions\nNow consider two randomly sampled future individuals:\n\n\\(\\tilde{Y}_1\\): a woman without a bachelor‚Äôs degree,\n\\(\\tilde{Y}_2\\): a woman with a bachelor‚Äôs degree.\n\n\nQuestion: To what extent do we expect the one without the college degree to have more children than the other?\n\nThe posterior predictive distributions integrate over uncertainty in \\(\\theta\\):\n\\[\np(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta.\n\\]\nUnder the Poisson‚ÄìGamma model, the posterior predictive distributions are Negative Binomial:\n\\[\n\\tilde{Y}_1 \\mid y \\sim \\text{NegBin}(a + \\sum Y_{i,1}, \\, b + n_1),\n\\]\n\\[\n\\tilde{Y}_2 \\mid y \\sim \\text{NegBin}(a + \\sum Y_{i,2}, \\, b + n_2).\n\\]\n\n\n# A tibble: 2 √ó 9\n  group                    n sum_y shape  rate post_mean post_mode  q025  q975\n  &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Less than bachelor's   111   217   219   112      1.96      1.95  1.70  2.22\n2 Bachelor's or higher    44    66    68    45      1.51      1.49  1.17  1.89\n\n\n\n\n\n\n\n\nFigure¬†3.1\n\n\n\n\n\n\n\ny &lt;- 0:10\n\n\nGroup 1: Less than bachelor's\n\n\nsize = 219   mu = 1.955357 \n\n\n [1] 1.427473e-01 2.766518e-01 2.693071e-01 1.755660e-01 8.622930e-02\n [6] 3.403387e-02 1.124423e-02 3.198421e-03 7.996053e-04 1.784763e-04\n[11] 3.601115e-05\n\n\n\nGroup 2: Bachelor's or higher\n\n\nsize = 68   mu = 1.511111 \n\n\n [1] 2.243460e-01 3.316420e-01 2.487315e-01 1.261681e-01 4.868444e-02\n [6] 1.524035e-02 4.030961e-03 9.263700e-04 1.887982e-04 3.465861e-05\n[11] 5.801551e-06\n\n\n    y Less.than.bachelor.s Bachelor.s.or.higher\n1   0         1.427473e-01         2.243460e-01\n2   1         2.766518e-01         3.316420e-01\n3   2         2.693071e-01         2.487315e-01\n4   3         1.755660e-01         1.261681e-01\n5   4         8.622930e-02         4.868444e-02\n6   5         3.403387e-02         1.524035e-02\n7   6         1.124423e-02         4.030961e-03\n8   7         3.198421e-03         9.263700e-04\n9   8         7.996053e-04         1.887982e-04\n10  9         1.784763e-04         3.465861e-05\n11 10         3.601115e-05         5.801551e-06\n\n\nInterpretation and Conclusion\nAlthough the posterior distributions of \\(\\theta_1\\) and \\(\\theta_2\\) are clearly separated, the posterior predictive distributions for \\(\\tilde{Y}_1\\) and \\(\\tilde{Y}_2\\) exhibit substantial overlap.\nFor example,\n\\[\n\\Pr(\\tilde{Y}_1 &gt; \\tilde{Y}_2 \\mid y) \\approx 0.48,\n\\qquad\n\\Pr(\\tilde{Y}_1 = \\tilde{Y}_2 \\mid y) \\approx 0.22.\n\\]\nThis illustrates an important distinction:\n\nStrong evidence that population means differ does not imply that the difference is large or easily detectable at the individual level.\n\n\n\n\n\n\n\n\nNote\n\n\n\nPopulation-level effects and individual-level variability are fundamentally different sources of uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#exponential-family",
    "href": "03_bi-1par.html#exponential-family",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.7 Exponential Family",
    "text": "3.7 Exponential Family\nMany common sampling models belong to the exponential family of distributions, including the binomial and Poisson distribution we saw in this chapter. A one-parameter exponential family has probability density (or mass) function of the form \\[\n\\begin{aligned}\np(y \\mid \\phi)\n&= h(y) \\exp\\{ \\phi t(y) - A(\\phi) \\} \\\\\n&= h(y) c(\\phi) \\exp\\{\\phi t(y)\\},\n\\end{aligned}\n\\] where \\(\\phi\\) is the unknown parameter, \\(t(y)\\) is a sufficient statistic, and \\(h(y)\\), \\(A(\\phi)\\), and \\(c(\\phi)\\) are known functions. Diaconis and Ylvisaker (1979) studied conjugate priors for exponential family models and showed that they have the general form \\[\np(\\phi \\mid n_0, t_0) = \\kappa (n_0,t_0) c(\\phi)^{n_0} \\exp \\{n_0t_0 \\phi\\},\n\\] where \\(n_0 &gt; 0\\) and \\(t_0\\) are hyperparameters.\nWith this result, and suppose the data consist of \\(n\\) independent observations \\(y_1,\\ldots,y_n\\) are sampled from \\(Y_1,\\dots,Y_n \\stackrel{iid}{\\sim} p(y\\mid \\theta)\\). Combining the prior with the likelihood gives the posterior distribution \\[\np(\\phi \\mid y_1, \\ldots, y_n)\n\\;\\propto\\;\np(\\phi)\\, p(y_1, \\ldots, y_n \\mid \\phi).\n\\]\nSubstituting the exponential-family form, \\[\n\\begin{aligned}\np(\\phi \\mid y_1, \\ldots, y_n)\n&\\propto\nc(\\phi)^{\\,n_0+n}\n\\exp\\left\\{\n\\phi \\left[\nn_0 t_0 + \\sum_{i=1}^n t(y_i)\n\\right]\n\\right\\} \\\\\n&\\propto\np\\!\\left(\\phi \\mid n_0 + n,\\;\nn_0 t_0 + n \\,\\bar t(y)\\right),\n\\end{aligned}\n\\] where \\[\n\\bar t(y) = \\frac{1}{n}\\sum_{i=1}^n t(y_i).\n\\]\nThus, the posterior distribution has the same functional form as the prior. This is why such priors are called conjugate.\n\n\n3.7.1 Interpretation of \\(n_0\\) and \\(t_0\\)\nThe similarity between the prior and posterior distributions suggests an interpretation of the hyperparameters:\n\n\\(n_0\\) can be interpreted as a prior sample size,\n\\(t_0\\) can be interpreted as a prior guess of \\(t(Y)\\).\n\nThis interpretation can be made more precise. Diaconis and Ylvisaker (1979) show that \\[\n\\mathbb{E}[t(Y)]\n=\n\\mathbb{E}\\!\\left[\\,\\mathbb{E}[t(Y)\\mid \\phi]\\,\\right]\n=\n\\mathbb{E}\\!\\left[-\\frac{c'(\\phi)}{c(\\phi)}\\right]\n=\nt_0.\n\\] (See also Exercise 3.6 in Hopf, 2009)\nThus, \\(t_0\\) represents the prior expected value of the sufficient statistic \\(t(Y)\\).\nThe parameter \\(n_0\\) measures how informative the prior is. One way to see this is to note that, as a function of \\(\\phi\\), \\(p(\\phi \\mid n_0, t_0)\\) has the same shape as a likelihood \\(p(\\tilde y_1, \\ldots, \\tilde y_{n_0} \\mid \\phi)\\) based on \\(n_0\\) hypothetical ‚Äúprior observations‚Äù \\(\\tilde y_1, \\ldots, \\tilde y_{n_0}\\) satisfying \\[\n\\frac{1}{n_0}\\sum_{i=1}^{n_0} t(\\tilde y_i) = t_0.\n\\]\nIn this sense, the prior distribution \\(p(\\phi \\mid n_0, t_0)\\) contains the same amount of information as would be obtained from \\(n_0\\) independent observations from the population.\n\nThe exponential family representation of the binomial\\((\\theta)\\) model can be obtained from the density of a single Bernoulli random variable:\n\\[\np(y \\mid \\theta)\n=\n\\theta^y (1-\\theta)^{1-y}.\n\\]\nRewrite this as\n\\[\n\\begin{aligned}\np(y \\mid \\theta)\n&=\n\\left( \\frac{\\theta}{1-\\theta} \\right)^y (1-\\theta) \\\\\n&=\n\\exp\\left\\{\ny \\log\\left( \\frac{\\theta}{1-\\theta} \\right)\n\\right\\}\n(1-\\theta).\n\\end{aligned}\n\\]\nLet\n\\[\n\\phi = \\log\\left( \\frac{\\theta}{1-\\theta} \\right),\n\\]\nthe log-odds. Then\n\\[\n\\theta = \\frac{e^\\phi}{1+e^\\phi},\n\\qquad\n1-\\theta = \\frac{1}{1+e^\\phi}.\n\\]\nSubstituting gives\n\\[\np(y \\mid \\phi)\n=\ne^{\\phi y} (1+e^\\phi)^{-1}.\n\\]\nThus the Bernoulli model is an exponential family model with\n\n\\(t(y) = y\\),\n\\(c(\\phi) = (1+e^\\phi)^{-1}\\),\n\\(h(y) = 1\\).\n\nConjugate prior\nThe conjugate prior for \\(\\phi\\) has the form\n\\[\np(\\phi \\mid n_0, t_0)\n\\propto\n(1+e^\\phi)^{-n_0}\n\\exp\\{ n_0 t_0 \\phi \\}.\n\\]\nSince \\(t(y)=y\\), the parameter \\(t_0\\) represents the prior expectation of \\(Y\\), or equivalently,\n\\[\nt_0 = \\mathbb{E}(\\theta).\n\\]\nUsing a change of variables back to \\(\\theta\\), the prior becomes\n\\[\np(\\theta \\mid n_0, t_0)\n\\propto\n\\theta^{n_0 t_0 - 1}\n(1-\\theta)^{n_0 (1-t_0) - 1},\n\\]\nwhich is a Beta distribution:\n\\[\n\\theta \\sim \\text{Beta}(n_0 t_0,\\; n_0(1-t_0)).\n\\]\nA weakly informative prior can be obtained by setting\n\n\\(t_0\\) equal to our prior expectation,\n\\(n_0 = 1\\).\n\nFor example, if our prior expectation is \\(1/2\\), this gives\n\\[\n\\theta \\sim \\text{Beta}(1/2, 1/2),\n\\]\nwhich is Jeffreys‚Äô prior for the binomial model.\nUnder this prior, the posterior distribution is\n\\[\n\\theta \\mid y_1, \\ldots, y_n\n\\sim\n\\text{Beta}\n\\left(\nt_0 + \\sum y_i,\n\\;\n(1-t_0) + \\sum (1-y_i)\n\\right).\n\\]\n\n\nThe Poisson\\((\\theta)\\) model can also be written in exponential family form.\nThe Poisson pmf is\n\\[\np(y \\mid \\theta)\n=\n\\frac{\\theta^y e^{-\\theta}}{y!}.\n\\]\nRewrite as\n\\[\np(y \\mid \\theta)\n=\n\\frac{1}{y!}\n\\exp\\{ y \\log\\theta - \\theta \\}.\n\\]\nThus it is an exponential family with\n\n\\(t(y) = y\\),\n\\(\\phi = \\log\\theta\\),\n\\(c(\\phi) = \\exp(-e^\\phi)\\),\n\\(h(y) = 1/y!\\).\n\nConjugate prior\nThe conjugate prior for \\(\\phi\\) has the form\n\\[\np(\\phi \\mid n_0, t_0)\n\\propto\n\\exp\\{ n_0 t_0 \\phi - n_0 e^\\phi \\}.\n\\]\nTransforming back to \\(\\theta = e^\\phi\\), this becomes\n\\[\np(\\theta \\mid n_0, t_0)\n\\propto\n\\theta^{n_0 t_0 - 1}\ne^{-n_0 \\theta},\n\\]\nwhich is a Gamma distribution:\n\\[\n\\theta \\sim \\text{Gamma}(n_0 t_0,\\; n_0).\n\\]\nA weakly informative prior can be obtained by setting\n\n\\(t_0\\) equal to the prior expectation of \\(\\theta\\),\n\\(n_0 = 1\\),\n\ngiving\n\\[\n\\theta \\sim \\text{Gamma}(t_0, 1).\n\\]\nUnder this prior, the posterior distribution is\n\\[\n\\theta \\mid y_1, \\ldots, y_n\n\\sim\n\\text{Gamma}\n\\left(\nt_0 + \\sum y_i,\n\\;\nn_0 + n\n\\right).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "03_bi-1par.html#discussion",
    "href": "03_bi-1par.html#discussion",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.8 Discussion",
    "text": "3.8 Discussion\nThe notion of conjugacy for classes of prior distributions was developed in Raiffa and Schlaifer (1961). Important results on conjugacy for exponential families appear in Diaconis and Ylvisaker (1979) and Diaconis and Ylvisaker (1985). They show that any prior distribution may be approximated by a mixture of conjugate priors.\nMost authors refer to intervals of high posterior probability as credible intervals, as opposed to confidence intervals. However, credible intervals do not necessarily have frequentist coverage properties. In many common models they are numerically similar to classical intervals, but this is not guaranteed.\nSome authors argue that accurate frequentist coverage can guide the construction of prior distributions. See Kass and Wasserman (1996) for a review of formal methods for selecting prior distributions.\n\nThis Chapter follows closely with Chapter 3 in Hoff (2009).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "04_MC.html",
    "href": "04_MC.html",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "",
    "text": "4.1 Background and Motivation\nWhat we have seen in the last chapter up to now, is to use the conjugate prior to obtain closed form expressions for the posterior distribution. However, in many cases, conjugate priors are not available or not desirable. In such cases, we need to resort to numerical methods to approximate the posterior distribution.\nThere are two broad classes of approaches:\nWe will be focusing on the Monte Carlo (MC) methods and its variation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#background-and-motivation",
    "href": "04_MC.html#background-and-motivation",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "",
    "text": "Question.\nHow can we perform Bayesian inference when conjugate priors are not available and the posterior has no closed-form expression?\n\n\n\nSimulation-based methods:\naccept‚Äìreject sampling, Markov chain Monte Carlo (MCMC), particle filters, and related algorithms.\nDeterministic approximation methods:\nLaplace approximations (including INLA), variational Bayes, expectation propagation, and related techniques.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#overview",
    "href": "04_MC.html#overview",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.2 Overview",
    "text": "4.2 Overview\n\n4.2.1 Monte Carlo (MC)\nMonte Carlo methods approximate expectations or probabilities using random sampling.\nIf samples can be drawn directly from the target distribution, Monte Carlo methods provide simple and effective estimators.\nTypical use: - Numerical integration - Bootstrap methods - Simulation-based probability estimation\n\n\n4.2.2 Markov Chain Monte Carlo (MCMC)\nMCMC methods are used when direct sampling is infeasible.\nThey construct a Markov chain whose stationary distribution is the target distribution, and use dependent samples from the chain after burn-in.\nTypical use: - Bayesian posterior sampling - High-dimensional or unnormalized distributions\n\n\n4.2.3 Gibbs Sampler\nThe Gibbs sampler is a special case of MCMC that samples sequentially from full conditional distributions.\nBecause proposals are drawn exactly from conditionals, all updates are automatically accepted.\nTypical use: - Bayesian hierarchical models - Models with conjugate full conditionals\n\n\n4.2.4 Relationship Between Monte Carlo, MCMC, and Gibbs Sampling\nThese three concepts are not competing methods, but rather form a nested hierarchy of ideas used for approximating expectations and probability distributions using randomness.\n\n\n\n4.2.5 Summary Table\n\n\n\n\n\n\n\n\n\n\nMethod\nIndependent Samples\nUses Markov Chain\nAccept‚ÄìReject Step\nTypical Application\n\n\n\n\nMonte Carlo (MC)\nYes\nNo\nNo\nDirect simulation, integration\n\n\nMCMC\nNo\nYes\nUsually\nBayesian posterior sampling\n\n\nGibbs Sampler\nNo\nYes\nNo (always accept)\nBayesian models with tractable conditionals\n\n\n\n\n\n\n\n\n\nNoteKey summary\n\n\n\n\nMonte Carlo is the general idea of using randomness for approximation.\nMCMC is Monte Carlo with dependent samples generated by a Markov chain.\nGibbs sampling is a specific MCMC algorithm based on full conditional distributions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#monte-carlo-methods",
    "href": "04_MC.html#monte-carlo-methods",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.3 Monte Carlo Methods",
    "text": "4.3 Monte Carlo Methods\nMotivation: why Monte Carlo?\nIn Bayesian inference we repeatedly encounter integrals such as\n\\[\n\\mathbb{E}[g(\\theta)\\mid y]\n=\\int g(\\theta)\\,p(\\theta\\mid y)\\,d\\theta,\n\\qquad\n\\Pr(\\theta\\in A\\mid y)=\\int_A p(\\theta\\mid y)\\,d\\theta,\n\\]\nthat are not available in closed form. Monte Carlo replaces these integrals by averages of random draws.\n\n\n\n\n\n\nNote\n\n\n\nKey idea: replace an intractable integral by an empirical mean.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#the-monte-carlo-method",
    "href": "04_MC.html#the-monte-carlo-method",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.4 The Monte Carlo Method",
    "text": "4.4 The Monte Carlo Method\nIn the previous chapter, we obtained the following posterior distributions for the birth rates of women without and with bachelor‚Äôs degrees:\n\\[\n\\theta_1 \\mid \\sum_{i=1}^{111} Y_{i,1} = 217\n\\sim \\text{Gamma}(219, 112),\n\\]\n\\[\n\\theta_2 \\mid \\sum_{i=1}^{44} Y_{i,2} = 66\n\\sim \\text{Gamma}(68, 45).\n\\]\nIt was claimed that\n\n\\[\n\\Pr(\\theta_1 &gt; \\theta_2 \\mid \\text{data}) = 0.97.\n\\]\n\nHow do we compute such a probability? From Chapter 2, since \\(\\theta_1\\) and \\(\\theta_2\\) are conditionally independent given the data \\(y\\), we have \\[\n\\Pr(\\theta_1 &gt; \\theta_2 \\mid y)\n=\n\\int_0^\\infty \\int_0^{\\theta_1}\np(\\theta_1 \\mid y)\np(\\theta_2 \\mid y)\n\\, d\\theta_2 \\, d\\theta_1.\n\\]\nSubstituting the gamma densities gives\n\\[\n\\int_0^\\infty \\int_0^{\\theta_1}\n\\text{dgamma}(\\theta_1; 219, 112)\n\\,\n\\text{dgamma}(\\theta_2; 68, 45)\n\\, d\\theta_2 \\, d\\theta_1.\n\\]\nThis integral can be evaluated numerically. However, in realistic Bayesian models, such integrals quickly become high-dimensional and analytically intractable. This motivates Monte Carlo (MC) methods.\n\n4.4.1 MC Approximation\nSuppose we wish to compute\n\\[\n\\mathbb{E}[g(\\theta) \\mid y]\n=\n\\int g(\\theta) \\, p(\\theta \\mid y) \\, d\\theta.\n\\]\nIf we can generate independent samples\n\\[\n\\theta^{(1)}, \\ldots, \\theta^{(S)}\n\\sim p(\\theta \\mid y),\n\\]\nthen we approximate the expectation by\n\\[\n\\frac{1}{S}\n\\sum_{s=1}^S g(\\theta^{(s)}).\n\\]\nThis is called a Monte Carlo approximation. By the Law of Large Numbers,\n\\[\n\\frac{1}{S}\n\\sum_{s=1}^S g(\\theta^{(s)})\n\\;\\longrightarrow\\;\n\\mathbb{E}[g(\\theta) \\mid y] = \\int g(\\theta) p(\\theta \\mid y) d\\theta\n\\quad \\text{as } S \\to \\infty.\n\\] With the property above, we can calculate many quantities of interest about the posterior distribution. For example, suppose \\(\\bar{\\theta}\\) is the average of \\(\\{\\theta^{(1)}, \\dots, \\theta^{(S)}\\}\\), then as \\(S \\to \\infty\\):\n\n\\(\\bar{\\theta} \\to \\mathbb{E}[\\theta \\mid y]\\),\n\\(\\frac{1}{S-1}\\sum_{s=1}^S (\\theta^{(s)} - \\bar{\\theta})^2\\to \\mathrm{Var}(\\theta \\mid y)\\).\n\\(\\frac{1}{S}\\sum_{s=1}^S \\mathbf{1}\\{\\theta^{(s)} \\in A\\}\\to \\Pr(\\theta \\in A \\mid y)\\).\nthe empirical distribution of \\(\\{\\theta^{(1)}, \\dots, \\theta^{(S)}\\}\\) converges to \\(p(\\theta \\mid y)\\).\nthe sample median converges to the posterior median \\(\\theta_{1/2}\\).\nthe sample \\(\\alpha\\)-quantile converges to \\(\\theta_\\alpha\\).\n\n\n\n4.4.2 Convergence Properties\nLet \\(\\theta^{(1)}, \\dots, \\theta^{(S)} \\sim p(\\theta \\mid y)\\).\nAs \\(S \\to \\infty\\):\n\n\\(\\displaystyle \\frac{\\#\\{\\theta^{(s)} \\le c\\}}{S}\n\\;\\longrightarrow\\;\n\\Pr(\\theta \\le c \\mid y)\\)\nThe empirical distribution of \\(\\{\\theta^{(1)},\\dots,\\theta^{(S)}\\}\\) converges to \\(p(\\theta \\mid y)\\)\nThe sample median converges to the posterior median \\(\\theta_{1/2}\\)\nThe sample \\(\\alpha\\)-quantile converges to \\(\\theta_\\alpha\\)\n\nKey message:\nAlmost any aspect of a posterior distribution can be approximated arbitrarily well using a sufficiently large Monte Carlo sample.\nThus Monte Carlo sampling allows us to approximate:\n\nposterior means,\nposterior variances,\nposterior probabilities,\ncredible intervals,\nmany more\n\n\nTo approximate \\[\n\\Pr(\\theta_1 &gt; \\theta_2 \\mid y),\n\\]\nwe can:\n\nChoose a (large) number of samples \\(S\\) (e.g., \\(S=10,000\\)).\nDraw \\(\\theta_1^{(s)} \\sim \\text{Gamma}(219, 112)\\).\nDraw \\(\\theta_2^{(s)} \\sim \\text{Gamma}(68, 45)\\).\nCompute the indicator \\[\nI^{(s)} = \\mathbf{1}\\{\\theta_1^{(s)} &gt; \\theta_2^{(s)}\\}.\n\\]\n\nThen\n\\[\n\\Pr(\\theta_1 &gt; \\theta_2 \\mid y)\n\\approx\n\\frac{1}{S}\n\\sum_{s=1}^S I^{(s)}.\n\\]\nThis avoids evaluating any double integrals.\n\n\nWhy Monte Carlo?\nWorks in high dimensions. Requires only the ability to simulate. Avoids symbolic integration. Scales to complex hierarchical models.\n\nThis is the foundation of modern Bayesian computation.\n\n# Figure 4.1 ‚Äî Monte Carlo Approximation (Gamma(68,45))\n# Histograms + KDEs for S = 10, 100, 1000; true density in gray\n\nset.seed(8670)\nlibrary(ggplot2)\n\n# Posterior: Gamma(shape=68, rate=45)\nshape_post &lt;- 68\nrate_post  &lt;- 45\n\n# MC samples\nS_list &lt;- c(10, 100, 1000)\nmc_df &lt;- do.call(rbind, lapply(S_list, function(S) {\n  data.frame(theta = rgamma(S, shape = shape_post, rate = rate_post),\n             S = factor(S, levels = S_list))\n}))\n\n# Grid for true density (choose a sensible range around the mass)\nxgrid &lt;- seq(\n  qgamma(0.001, shape = shape_post, rate = rate_post),\n  qgamma(0.999, shape = shape_post, rate = rate_post),\n  length.out = 600\n)\n\ntrue_df &lt;- data.frame(\n  theta = xgrid,\n  dens  = dgamma(xgrid, shape = shape_post, rate = rate_post)\n)\n\n# Plot\nggplot(mc_df, aes(x = theta)) +\n  # histogram (density scale so it overlays with densities)\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 18, color = \"black\", fill = \"white\") +\n  # KDE from MC samples\n  geom_density(linewidth = 1.1) +\n  # True density (gray)\n  geom_line(data = true_df, aes(x = theta, y = dens),\n            linewidth = 1.2, color = \"gray50\") +\n  facet_wrap(~ S, nrow = 1, scales = \"free_y\") +\n  labs(\n    title = \"Monte Carlo Approximation\",\n  subtitle = paste(\n    \"Histograms and KDEs for Monte Carlo samples\",\n    \"True Gamma(68, 45) density shown in gray\",\n    sep = \"\\n\"\n  ),\n    x = expression(theta),\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 13),\n    strip.text = element_text(size = 14, face = \"bold\")\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#numerical-evaluation",
    "href": "04_MC.html#numerical-evaluation",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.5 Numerical Evaluation",
    "text": "4.5 Numerical Evaluation\nWe now compare Monte Carlo approximations to quantities that can be computed analytically in this conjugate example.\nSuppose\n\\[\nY_1,\\dots,Y_n \\mid \\theta \\sim \\text{Poisson}(\\theta),\n\\quad\n\\theta \\sim \\text{Gamma}(a,b).\n\\]\nAfter observing \\(y_1,\\dots,y_n\\) with \\(\\sum y_i = sy\\) and sample size \\(n\\), the posterior distribution is\n\\[\n\\theta \\mid y \\sim \\text{Gamma}(a+sy,\\; b+n).\n\\]\n\nFor the birth-rate example:\n\n\\(a = 2\\)\n\\(b = 1\\)\n\\(sy = 66\\)\n\\(n = 44\\)\n\nPosterior:\n\\[\n\\theta \\mid y \\sim \\text{Gamma}(68,45).\n\\]\nPosterior mean:\n\\[\n\\mathbb{E}[\\theta \\mid y]\n=\n\\frac{a+sy}{b+n}\n=\n\\frac{68}{45}\n=\n1.51.\n\\]\n\n4.5.1 Monte Carlo in R\n\nset.seed(8670)\n\n## Posterior parameters\na  &lt;- 2\nb  &lt;- 1\nsy &lt;- 66\nn  &lt;- 44\n\nshape_post &lt;- a + sy\nrate_post  &lt;- b + n\n\n## Exact quantities\nmean_exact &lt;- shape_post / rate_post\np_exact    &lt;- pgamma(1.75, shape = shape_post, rate = rate_post)\nci_exact   &lt;- qgamma(c(0.025, 0.975),\n                     shape = shape_post,\n                     rate  = rate_post)\n\n## Monte Carlo samples\ntheta_mc10   &lt;- rgamma(10,   shape_post, rate_post)\ntheta_mc100  &lt;- rgamma(100,  shape_post, rate_post)\ntheta_mc1000 &lt;- rgamma(1000, shape_post, rate_post)\n\n## Function to summarize MC output\nmc_summary &lt;- function(theta_sample) {\n  c(\n    Mean        = mean(theta_sample),\n    Prob_less   = mean(theta_sample &lt; 1.75),\n    CI_lower    = quantile(theta_sample, 0.025),\n    CI_upper    = quantile(theta_sample, 0.975)\n  )\n}\n\n## Build comparison table\nresults &lt;- rbind(\n  Exact   = c(Mean      = mean_exact,\n              Prob_less = p_exact,\n              CI_lower  = ci_exact[1],\n              CI_upper  = ci_exact[2]),\n\n  MC_10   = mc_summary(theta_mc10),\n  MC_100  = mc_summary(theta_mc100),\n  MC_1000 = mc_summary(theta_mc1000)\n)\n\nround(results, 4)\n\n          Mean Prob_less CI_lower CI_upper\nExact   1.5111    0.8998   1.1734   1.8908\nMC_10   1.5077    1.0000   1.3628   1.6228\nMC_100  1.5033    0.8500   1.1536   1.9305\nMC_1000 1.5180    0.8860   1.1810   1.8937\n\n\n\nSmax &lt;- 1000\ntheta_seq &lt;- rgamma(Smax, shape = shape_post, rate = rate_post)\ncum_mean  &lt;- cumsum(theta_seq) / seq_along(theta_seq)\n\nplot(cum_mean, type=\"l\",\n     xlab=\"Number of draws (S)\",\n     ylab=\"Cumulative Monte Carlo mean\")\nabline(h = mean_exact, lty = 2, lwd = 2)\n\n\n\n\n\n\n\n\n\nxgrid &lt;- seq(0.5, 2.5, length.out = 400)\ntrue_pdf &lt;- dgamma(xgrid, shape = shape_post, rate = rate_post)\n\npar(mfrow=c(1,3))\n\nfor (S in c(10,100,1000)) {\n  x &lt;- get(paste0(\"theta_mc\", S))\n  hist(x, prob=TRUE,\n       main=paste0(\"S = \", S),\n       xlab=expression(theta),\n       border=\"black\")\n  lines(xgrid, true_pdf, lwd=2)\n}\n\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n\n\n\n4.5.2 There are much more about the MC method\n\nVariance reduction methods\nAntithetic variates\nControl variables\nImportance Sampling\nStratified Sampling\nStratified Importance Sampling\netc‚Ä¶\n\nYou may refer to my notes in Chapter 4 in the Computational Methods in Statistics Course.\n\n\n4.5.3 MC for predictive distribution and Sampling from it\nAs discussed earlier, the predictive distribution of a future random variable \\(\\tilde Y\\) is the probability distribution that reflects uncertainty about \\(\\tilde Y\\) after accounting for both:\n\nknown quantities (conditioned on observed data), and\nunknown quantities (integrated out).\n\n\n\n4.5.4 Sampling Model vs Predictive Model\nSuppose \\(\\tilde Y\\) denotes the number of children for a randomly selected woman aged 40 with a college degree.\nIf the true mean birthrate \\(\\theta\\) were known, uncertainty about \\(\\tilde Y\\) would be described by the sampling model \\[\n\\Pr(\\tilde Y = \\tilde y \\mid \\theta) = p(\\tilde y \\mid \\theta)\n= \\frac{\\theta^{\\tilde y} e^{-\\theta}}{\\tilde y!},\n\\] that is, \\[\n\\tilde Y \\mid \\theta \\sim \\text{Poisson}(\\theta).\n\\]\nIn practice, however, \\(\\theta\\) is unknown. Therefore, predictions must account for uncertainty in \\(\\theta\\).\n\n\n4.5.5 Prior Predictive Distribution\nIf no data have been observed, predictions are obtained by integrating out \\(\\theta\\) using the prior distribution: \\[\n\\Pr(\\tilde Y = \\tilde y)\n= \\int p(\\tilde y \\mid \\theta)\\, p(\\theta)\\, d\\theta.\n\\]\nThis is called the prior predictive distribution.\nFor a Poisson model with a Gamma prior, \\[\n\\theta \\sim \\text{Gamma}(a,b),\n\\] the prior predictive distribution of \\(\\tilde Y\\) is \\[\n\\tilde Y \\sim \\text{Negative Binomial}(a,b).\n\\]\n\n\n4.5.6 Posterior Predictive Distribution\nAfter observing data \\(Y_1 = y_1, \\ldots, Y_n = y_n\\), the relevant predictive distribution for a new observation is \\[\n\\Pr(\\tilde Y = \\tilde y \\mid Y_1=y_1,\\ldots,Y_n=y_n)\n= \\int p(\\tilde y \\mid \\theta)\\, p(\\theta \\mid y_1,\\ldots,y_n)\\, d\\theta.\n\\]\nThis distribution is called the posterior predictive distribution.\nFor the Poisson‚ÄìGamma model, the posterior distribution is \\[\n\\theta \\mid y_1,\\ldots,y_n \\sim \\text{Gamma}\\!\\left(a + \\sum_{i=1}^n y_i,\\; b+n\\right),\n\\] and the posterior predictive distribution is again Negative Binomial.\n\n\n\n4.5.7 Monte Carlo Sampling from the Posterior Predictive Distribution\nIn many models, the posterior predictive distribution cannot be evaluated analytically. However, it can often be sampled using Monte Carlo methods.\nThe idea is simple:\n\nDraw \\(\\theta^{(s)} \\sim p(\\theta \\mid y_1,\\ldots,y_n)\\)\n\nDraw \\(\\tilde Y^{(s)} \\sim p(\\tilde y \\mid \\theta^{(s)})\\)\n\nRepeat for \\(s = 1,\\ldots,S\\)\n\nThis produces samples \\[\n\\tilde Y^{(1)}, \\ldots, \\tilde Y^{(S)}\n\\sim p(\\tilde y \\mid y_1,\\ldots,y_n),\n\\] which approximate the posterior predictive distribution.\n\n\n\n4.5.8 Example: Comparing Two Groups (Poisson Model)\nSuppose we observe two independent groups with Poisson data:\n\nGroup 1: \\(\\sum Y_{i,1} = 217\\), \\(n_1 = 111\\)\n\nGroup 2: \\(\\sum Y_{i,2} = 66\\), \\(n_2 = 44\\)\n\nWith a common prior \\[\n\\theta_k \\sim \\text{Gamma}(a,b), \\quad k=1,2,\n\\] the posterior distributions are \\[\n\\theta_1 \\mid \\mathbf y_1 \\sim \\text{Gamma}(a+217,\\; b+111),\n\\] \\[\n\\theta_2 \\mid \\mathbf y_2 \\sim \\text{Gamma}(a+66,\\; b+44).\n\\]\nBecause \\(\\theta_1\\) and \\(\\theta_2\\) are posterior independent, posterior predictive sampling proceeds independently for each group:\n\\[\n\\theta_1^{(s)} \\sim p(\\theta_1 \\mid \\mathbf y_1), \\quad\n\\tilde Y_1^{(s)} \\sim \\text{Poisson}(\\theta_1^{(s)}),\n\\] \\[\n\\theta_2^{(s)} \\sim p(\\theta_2 \\mid \\mathbf y_2), \\quad\n\\tilde Y_2^{(s)} \\sim \\text{Poisson}(\\theta_2^{(s)}).\n\\]\n\n\n\n4.5.9 Monte Carlo Approximation of Predictive Quantities\nUsing Monte Carlo samples \\(\\{\\tilde Y_1^{(s)}, \\tilde Y_2^{(s)}\\}\\), we can approximate quantities such as \\[\n\\Pr(\\tilde Y_1 &gt; \\tilde Y_2 \\mid \\text{data})\n\\approx\n\\frac{1}{S} \\sum_{s=1}^S\n\\mathbb{I}\\bigl(\\tilde Y_1^{(s)} &gt; \\tilde Y_2^{(s)}\\bigr).\n\\]\nMore generally, Monte Carlo samples from the posterior predictive distribution allow us to approximate:\n\npredictive probabilities,\npredictive expectations,\nquantiles and credible intervals,\nfunctions of future observations.\n\nThis flexibility is one of the main strengths of Monte Carlo methods in Bayesian analysis.\n\n\n4.5.10 Checking\n4.4 Posterior Predictive Model Checking\nPosterior predictive model checking assesses whether a fitted Bayesian model can plausibly reproduce key features of the observed data.\nWe focus on women aged 40 without a college degree. The empirical distribution of the number of children for these women, together with the corresponding posterior predictive distribution, is shown in Figure 4.6.\nIn this sample of size \\[\nn = 111,\n\\] the number of women with exactly two children is \\[\ny_{\\text{obs}} = 38,\n\\] which is twice the number of women with exactly one child.\nIn contrast, the posterior predictive distribution (shown in gray) suggests that sampling a woman with two children is slightly less likely than sampling a woman with one child, with probabilities approximately \\[\n0.27 \\quad \\text{vs.} \\quad 0.28.\n\\]\nThese two distributions appear to be in conflict: if the observed data contain twice as many women with two children as with one child, why does the model predict otherwise?\nPossible Explanations\nOne explanation is sampling variability. The empirical distribution of a finite sample does not necessarily match the true population distribution, and with moderate sample sizes, random fluctuations can be substantial. A smooth population distribution can easily produce a bumpy empirical histogram.\nAn alternative explanation is model misspecification. In particular, the Poisson model cannot capture certain features of the data. There is no Poisson distribution with a sharp peak at \\(y = 2\\), whereas the empirical distribution shows exactly such behavior.\nThese explanations can be investigated systematically using Monte Carlo simulation.\nA Posterior Predictive Discrepancy Statistic\nDefine the discrepancy statistic \\[\nt(y) = \\frac{\\#{y_i = 2}}{\\#{y_i = 1}},\n\\] the ratio of the number of women with two children to the number with one child.\nFor the observed data, \\[\nt(y_{\\text{obs}}) = 2.\n\\]\nTo assess whether this value is surprising under the model, we examine the posterior predictive distribution of \\(t(\\tilde{Y})\\).\nPosterior Predictive Monte Carlo Procedure\nFor each Monte Carlo iteration \\(s = 1, \\dots, S\\): 1. Sample from the posterior \\[\n\\theta^{(s)} \\sim p(\\theta \\mid y_{\\text{obs}})\n\\] 2. Generate a posterior predictive dataset \\[\n\\tilde{Y}^{(s)} = (\\tilde{y}_1^{(s)}, \\dots, \\tilde{y}_n^{(s)}),\n\\quad\n\\tilde{y}_i^{(s)} \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Poisson}(\\theta^{(s)})\n\\] 3. Compute the discrepancy \\[\nt^{(s)} = t(\\tilde{Y}^{(s)})\n\\]\nThis yields samples \\[\n{ t^{(1)}, \\dots, t^{(S)} }\n\\] from the posterior predictive distribution of \\(t(\\tilde{Y})\\).\n\n## Prior parameters\na &lt;- 2\nb &lt;- 1\n\n## Data summary (no bachelor's degree group)\nn  &lt;- 111\nsy &lt;- 217   # sum(y_i)\n\n## Storage\nt_mc &lt;- numeric(10000)\n\nfor (s in 1:10000) {\n  ## Draw from posterior\n  theta &lt;- rgamma(1, shape = a + sy, rate = b + n)\n  \n  ## Posterior predictive sample\n  y_mc &lt;- rpois(n, theta)\n  \n  ## Discrepancy statistic\n  n1 &lt;- sum(y_mc == 1)\n  n2 &lt;- sum(y_mc == 2)\n  \n  ## Avoid division by zero\n  t_mc[s] &lt;- ifelse(n1 &gt; 0, n2 / n1, NA)\n}\n\n## Remove undefined values\nt_mc &lt;- t_mc[!is.na(t_mc)]\n\nInterpretation\nFigure 4.6 shows the posterior predictive distribution of \\(t(\\tilde{Y})\\), with the observed value \\(t(y_{\\text{obs}})\\) indicated by a vertical line.\nOut of 10,000 Monte Carlo samples, only about 0.5% produce values of \\[\nt(\\tilde{Y}) \\ge t(y_{\\text{obs}}).\n\\]\nThis indicates that the observed discrepancy is extremely unlikely under the fitted Poisson model.\nConclusion\nThe posterior predictive check suggests that the Poisson model is inadequate for these data. Although it matches the posterior mean reasonably well, it fails to reproduce important distributional features.\nThis does not imply that the model is useless for all inferential goals. However, if our goal is to accurately describe the distribution of family sizes, a more flexible model is needed.\nPosterior predictive checks provide a principled, simulation-based tool for diagnosing such failures and guiding model refinement.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#gibbs-sampler-1",
    "href": "04_MC.html#gibbs-sampler-1",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.6 Gibbs sampler",
    "text": "4.6 Gibbs sampler\nFor many multiparameter Bayesian models, the joint posterior distribution does not belong to a standard family and is therefore difficult to sample from directly. However, it is often the case that sampling from the full conditional distribution of each parameter is straightforward.\nIn such situations, posterior approximation can be carried out using the Gibbs sampler, an iterative Monte Carlo algorithm that constructs a dependent sequence of parameter values whose distribution converges to the target joint posterior distribution.\nIn this chapter, we introduce the Gibbs sampler in the context of the normal model with a semiconjugate prior, and study how well it approximates the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#a-semiconjugate-prior-distribution",
    "href": "04_MC.html#a-semiconjugate-prior-distribution",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.7 6.1 A Semiconjugate Prior Distribution",
    "text": "4.7 6.1 A Semiconjugate Prior Distribution\nFor normal distribution, it may be modeled our uncertainty about the population mean \\(\\theta\\) as depending on the sampling variance \\(\\sigma^2\\) via \\[\n\\theta \\mid \\sigma^2 \\sim\nN\\!\\left(\\mu_0,\\; \\frac{\\sigma^2}{\\kappa_0}\\right).\n\\]\nThis formulation ties the prior variance of \\(\\theta\\) to the sampling variability of the data, and \\(\\mu_0\\) can be interpreted as representing \\(\\kappa_0\\) prior observations from the population.\nIn some settings this dependence is reasonable, but in others we may wish to specify prior uncertainty about \\(\\theta\\) independently of \\(\\sigma^2\\), so that \\[\np(\\theta, \\sigma^2) = p(\\theta)\\,p(\\sigma^2).\n\\]\nOne such specification is the following semiconjugate prior distribution: \\[\n\\theta \\sim \\text{Normal}(\\mu_0, \\tau_0^2),\n\\qquad\n\\frac{1}{\\sigma^2} \\sim \\text{Gamma}\\!\\left(\\frac{\\nu_0}{2}, \\frac{\\nu_0 \\sigma_0^2}{2}\\right).\n\\]\nPosterior Distribution of \\(\\theta \\mid \\sigma^2\\)\nSuppose \\[\nY_1, \\dots, Y_n \\mid \\theta, \\sigma^2 \\;\\overset{i.i.d.}{\\sim}\\; (\\theta, \\sigma^2).\n\\]\nThen, conditional on \\(\\sigma^2\\) and the observed data \\(y_1, \\dots, y_n\\), the posterior distribution of \\(\\theta\\) is \\[\n\\theta \\mid \\sigma^2, y_1, \\dots, y_n\n\\;\\sim\\;\n\\text{Normal}(\\mu_n, \\tau_n^2),\n\\] where \\[\n\\mu_n\n=\n\\frac{\\mu_0 / \\tau_0^2 + n \\bar{y} / \\sigma^2}\n     {1 / \\tau_0^2 + n / \\sigma^2},\n\\qquad\n\\tau_n^2\n=\n\\left( \\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2} \\right)^{-1}.\n\\]\nThis conditional posterior distribution will form one step of the Gibbs sampler.\n\n4.7.1 Key Takeaway\n\nThe joint posterior of \\((\\theta, \\sigma^2)\\) is not available in closed form.\nThe full conditional distributions of \\(\\theta \\mid \\sigma^2, y\\) and \\(\\sigma^2 \\mid \\theta, y\\) are available in standard forms.\nThis structure makes the Gibbs sampler a natural and efficient tool for posterior approximation.\n\nIn the next section, we derive the full conditional distribution of \\(\\sigma^2\\) and combine the two conditional updates into a complete Gibbs sampling algorithm.\nIn the conjugate case where \\(\\tau_0^2\\) is proportional to \\(\\sigma^2\\), we showed that the marginal posterior distribution \\[\np(\\sigma^2 \\mid y_1,\\ldots,y_n)\n\\] is an inverse-gamma distribution. In this setting, Monte Carlo samples of \\((\\theta,\\sigma^2)\\) from the joint posterior distribution can be obtained by the following two-step procedure:\n\nSample \\[\n\\sigma^{2(s)} \\sim p(\\sigma^2 \\mid y_1,\\ldots,y_n),\n\\] which is an inverse-gamma distribution.\nSample \\[\n\\theta^{(s)} \\sim p(\\theta \\mid \\sigma^{2(s)}, y_1,\\ldots,y_n),\n\\] which is a normal distribution.\n\nThis approach works because both full conditional distributions are standard and easy to sample from.\nHowever, when \\(\\tau_0^2\\) is not proportional to \\(\\sigma^2\\), the marginal posterior distribution of the precision \\[\n\\frac{1}{\\sigma^2}\n\\] is not a gamma distribution, nor any other standard distribution from which we can easily sample. As a result, direct Monte Carlo sampling from the marginal posterior is no longer straightforward, motivating the need for alternative approximation methods.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#discrete-approximations",
    "href": "04_MC.html#discrete-approximations",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.8 6.2 Discrete Approximations",
    "text": "4.8 6.2 Discrete Approximations\nIn the conjugate case where \\(\\tau_0^2\\) was proportional to \\(\\sigma^2\\), we showed that \\[\np(\\sigma^2 \\mid y_1,\\dots,y_n)\n\\] was an inverse-gamma distribution, and Monte Carlo samples of \\((\\theta, \\sigma^2)\\) from the joint posterior distribution could be obtained by:\n\nsampling \\(\\sigma^{2(s)} \\sim p(\\sigma^2 \\mid y_1,\\dots,y_n)\\), an inverse-gamma distribution;\nsampling \\(\\theta^{(s)} \\sim p(\\theta \\mid \\sigma^{2(s)}, y_1,\\dots,y_n)\\), a normal distribution.\n\nHowever, in the semiconjugate case where \\(\\tau_0^2\\) is not proportional to \\(\\sigma^2\\), the marginal posterior distribution of \\(1/\\sigma^2\\) is not a gamma distribution, nor any other standard distribution from which we can easily sample directly.\n\n4.8.1 Posterior Density Ratios\nLet \\[\n\\tilde{\\sigma}^2 = \\frac{1}{\\sigma^2}\n\\] denote the precision. Recall that the posterior distribution of \\((\\theta, \\tilde{\\sigma}^2)\\) is equal to the joint distribution \\[\np(\\theta, \\tilde{\\sigma}^2, y_1,\\dots,y_n),\n\\] divided by \\(p(y_1,\\dots,y_n)\\), which does not depend on the parameters. Therefore, the relative posterior probabilities of two parameter values \\((\\theta_1, \\tilde{\\sigma}_1^2)\\) and \\((\\theta_2, \\tilde{\\sigma}_2^2)\\) are directly computable: \\[\n\\frac{p(\\theta_1, \\tilde{\\sigma}_1^2 \\mid y_1,\\dots,y_n)}\n     {p(\\theta_2, \\tilde{\\sigma}_2^2 \\mid y_1,\\dots,y_n)}\n=\n\\frac{p(\\theta_1, \\tilde{\\sigma}_1^2, y_1,\\dots,y_n)}\n     {p(\\theta_2, \\tilde{\\sigma}_2^2, y_1,\\dots,y_n)}.\n\\]\n\n\n4.8.2 Joint Distribution\nThe joint density can be written as \\[\n\\begin{aligned}\np(\\theta, \\tilde{\\sigma}^2, y_1,\\dots,y_n)\n&= p(\\theta, \\tilde{\\sigma}^2)\\, p(y_1,\\dots,y_n \\mid \\theta, \\tilde{\\sigma}^2) \\\\\n&= \\text{Normal}(\\theta \\mid \\mu_0, \\tau_0^2)\n   \\times \\text{Gamma}\\!\\left(\\tilde{\\sigma}^2 \\mid \\frac{\\nu_0}{2}, \\frac{\\nu_0 \\sigma_0^2}{2}\\right)\n   \\times \\prod_{i=1}^n \\text{Normal}\\!\\left(y_i \\mid \\theta, \\frac{1}{\\tilde{\\sigma}^2}\\right).\n\\end{aligned}\n\\]\nAll components of this joint density are standard distributions and therefore easy to evaluate numerically.\n\n\n4.8.3 Discrete Posterior Approximation\nA discrete approximation to the posterior distribution is obtained by constructing a grid over the parameter space and evaluating relative posterior probabilities on that grid.\nSpecifically:\n\nchoose grids \\(\\{\\theta_1,\\dots,\\theta_G\\}\\) and \\(\\{\\tilde{\\sigma}_1^2,\\dots,\\tilde{\\sigma}_H^2\\}\\) consisting of evenly spaced parameter values;\nevaluate \\(p(\\theta_g, \\tilde{\\sigma}_h^2, y_1,\\dots,y_n)\\) for each grid point \\((\\theta_g, \\tilde{\\sigma}_h^2)\\);\nassign posterior probabilities proportional to these values:\n\n\\[\np(\\theta_g, \\tilde{\\sigma}_h^2 \\mid y_1,\\dots,y_n)\n\\;\\propto\\;\np(\\theta_g, \\tilde{\\sigma}_h^2, y_1,\\dots,y_n).\n\\]\nThis discrete approximation can then be normalized and used to compute posterior summaries such as means, variances, and credible regions.\n\n\n4.8.4 Remarks\n\nDiscrete approximations are conceptually simple and transparent.\nThey are feasible only in low-dimensional parameter spaces.\nFor higher-dimensional models, simulation-based methods such as the Gibbs sampler become essential.\n\n\n\n4.8.5 Discrete approximations\nLet \\(\\{\\theta_1,\\ldots,\\theta_G\\}\\) and \\(\\{\\tilde\\sigma_1^2,\\ldots,\\tilde\\sigma_H^2\\}\\) be discrete grids for \\(\\theta\\) and \\(\\sigma^2\\), respectively.\nA discrete approximation to the joint posterior distribution is defined by\n\\[\np_D(\\theta_k,\\tilde\\sigma_l^2 \\mid y_1,\\ldots,y_n)\n=\n\\frac{p(\\theta_k,\\tilde\\sigma_l^2 \\mid y_1,\\ldots,y_n)}\n{\\sum_{g=1}^G \\sum_{h=1}^H p(\\theta_g,\\tilde\\sigma_h^2 \\mid y_1,\\ldots,y_n)}.\n\\]\nUsing Bayes‚Äô rule, this can be written equivalently as\n\\[\np_D(\\theta_k,\\tilde\\sigma_l^2 \\mid y_1,\\ldots,y_n)\n=\n\\frac{p(\\theta_k,\\tilde\\sigma_l^2,y_1,\\ldots,y_n)}\n{\\sum_{g=1}^G \\sum_{h=1}^H p(\\theta_g,\\tilde\\sigma_h^2,y_1,\\ldots,y_n)}.\n\\]\nThis defines a valid joint probability distribution over \\[\n\\theta \\in \\{\\theta_1,\\ldots,\\theta_G\\}, \\qquad\n\\sigma^2 \\in \\{\\tilde\\sigma_1^2,\\ldots,\\tilde\\sigma_H^2\\},\n\\] since the probabilities sum to one. In fact, if the joint prior distribution is discrete on this grid, then \\(p_D\\) is exactly the posterior distribution.\n\n4.8.5.1 Application: midge data\nWe now apply this approximation to the midge data from the previous chapter.\nRecall that the data consist of \\(n=9\\) observations with sample mean \\(\\bar y = 1.804\\) and sample variance \\(s^2 = 0.017\\).\nIn the conjugate model, the prior variance of \\(\\theta\\) is proportional to the sampling variance \\(\\sigma^2 / \\kappa_0\\). When the sampling variance is very small, this can undesirably force the prior uncertainty about \\(\\theta\\) to be small as well. The semiconjugate prior removes this constraint.\nRecall that we previously suggested a prior mean and standard deviation of \\[\n\\mu_0 = 1.9, \\qquad \\tau_0 = 0.95,\n\\] placing most of the prior mass on \\(\\theta &gt; 0\\).\nFor \\(\\sigma^2\\), we take prior parameters \\[\n\\nu_0 = 1, \\qquad \\sigma_0^2 = 0.01.\n\\]\n\n\n4.8.5.2 Grid-based approximation\nThe R implementation evaluates the joint posterior distribution \\[\np(\\theta,\\sigma^2 \\mid y_1,\\ldots,y_n)\n\\] on a \\(100 \\times 100\\) grid of evenly spaced parameter values, with \\[\n\\theta \\in \\{1.505, 1.510, \\ldots, 1.995, 2.00\\}\n\\] and \\[\n\\sigma^2 \\in \\{1.75, 3.5, \\ldots, 173.25, 175.0\\}.\n\\]\nThe first panel of Figure 6.1 shows the resulting discrete approximation to the joint posterior distribution of \\((\\theta,\\sigma^2)\\).\nMarginal and conditional posterior distributions can then be obtained by simple summation. For example, the marginal posterior distribution of \\(\\theta\\) is\n\\[\np_D(\\theta_k \\mid y_1,\\ldots,y_n)\n=\n\\sum_{h=1}^H p_D(\\theta_k,\\tilde\\sigma_h^2 \\mid y_1,\\ldots,y_n).\n\\]\nThe resulting discrete approximations to the marginal posterior distributions of \\(\\theta\\) and \\(\\sigma^2\\) are shown in the second and third panels of Figure 6.1.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(scales)\n\n## -----------------------------\n## Data and semiconjugate prior\n## -----------------------------\ny  &lt;- c(1.64,1.70,1.72,1.74,1.82,1.82,1.82,1.90,2.08)\nn  &lt;- length(y)\n\nmu0     &lt;- 1.9\ntau0_sq &lt;- 0.95^2\nnu0     &lt;- 1\ns20     &lt;- 0.01\n\n## -----------------------------\n## Grids: theta and precision  (tilde sigma^2 = 1/sigma^2)\n## -----------------------------\nG &lt;- 100\nH &lt;- 100\n\nmean.grid &lt;- seq(1.505, 2.00, length.out = G)\nprec.grid &lt;- seq(1.75, 175,  length.out = H)   # this IS \\tilde{\\sigma}^2\n\npost.grid &lt;- matrix(NA_real_, nrow = G, ncol = H)\n\n## -----------------------------\n## Discrete joint posterior on the grid\n## p(theta, prec | y) ‚àù p(theta) p(prec) ‚àè N(y_i | theta, 1/prec)\n## -----------------------------\nfor (g in 1:G) {\n  for (h in 1:H) {\n    post.grid[g, h] &lt;-\n      dnorm(mean.grid[g], mu0, sqrt(tau0_sq)) *\n      dgamma(prec.grid[h], shape = nu0/2, rate = s20*nu0/2) *\n      prod(dnorm(y, mean.grid[g], sd = 1 / sqrt(prec.grid[h])))\n  }\n}\npost.grid &lt;- post.grid / sum(post.grid)\n\n## -----------------------------\n## Data frame + marginals\n## -----------------------------\npost_df &lt;- expand.grid(theta = mean.grid, prec = prec.grid)\npost_df$prob &lt;- as.vector(post.grid)\n\ntheta_marg &lt;- post_df %&gt;%\n  group_by(theta) %&gt;%\n  summarise(prob = sum(prob), .groups = \"drop\")\n\nprec_marg &lt;- post_df %&gt;%\n  group_by(prec) %&gt;%\n  summarise(prob = sum(prob), .groups = \"drop\")\n\n## -----------------------------\n## Plots (Figure 6.1 style)\n## -----------------------------\np_joint &lt;- ggplot(post_df, aes(theta, prec, fill = prob)) +\n  geom_raster(interpolate = TRUE) +\n  scale_fill_gradient(low = \"white\", high = \"black\",\n                      trans = \"sqrt\", labels = label_number()) +\n  labs(x = expression(theta),\n       y = expression(tilde(sigma)^2)) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\np_theta &lt;- ggplot(theta_marg, aes(theta, prob)) +\n  geom_line(linewidth = 1) +\n  labs(x = expression(theta),\n       y = expression(p(theta~\"|\"~y[1:n]))) +\n  theme_classic()\n\np_prec &lt;- ggplot(prec_marg, aes(prec, prob)) +\n  geom_line(linewidth = 1) +\n  labs(x = expression(tilde(sigma)^2),\n       y = expression(p(tilde(sigma)^2~\"|\"~y[1:n]))) +\n  theme_classic()\n\np_joint + p_theta + p_prec",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "04_MC.html#markov-chain-monte-carlo-mcmc-1",
    "href": "04_MC.html#markov-chain-monte-carlo-mcmc-1",
    "title": "4¬† Monte Carlo Method and its variations",
    "section": "4.9 Markov Chain Monte Carlo (MCMC)",
    "text": "4.9 Markov Chain Monte Carlo (MCMC)\n\nThis Chapter borrows materials from Chapter 4 in Hoff (2009) and Chapter 4 in Computational Methods in Statistics Course",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Method and its variations</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "5¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html",
    "href": "App_A-intro-R.html",
    "title": "6¬† Appendix: Introduction to R",
    "section": "",
    "text": "6.1 R\nFor conducting analyses with data sets of hundreds to thousands of observations, calculating by hand is not feasible and you will need a statistical software. R is one of those. R can also be thought of as a high-level programming language. In fact, R is one of the top languages to be used by data analysts and data scientists. There are a lot of analysis packages in R that are currently developed and maintained by researchers around the world to deal with different data problems. Most importantly, R is free! In this section, we will learn how to use R to conduct basic statistical analyses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#ide",
    "href": "App_A-intro-R.html#ide",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.2 IDE",
    "text": "6.2 IDE\n\n6.2.1 Rstudio\nRStudio is an integrated development environment (IDE) designed specifically for working with the R programming language. It provides a user-friendly interface that includes a source editor, console, environment pane, and tools for plotting, debugging, version control, and package management. RStudio supports both R and Python and is widely used for data analysis, statistical modeling, and reproducible research. It also integrates seamlessly with tools like R Markdown, Shiny, and Quarto, making it popular among data scientists, statisticians, and educators.\n\n\n6.2.2 Visual Studio Code (VS Code)\nVS Code is a versatile code editor that supports multiple programming languages, including R. With the R extension for VS Code, users can write and execute R code, access R‚Äôs console, and utilize features like syntax highlighting, code completion, and debugging. While not as specialized as RStudio for R development, VS Code offers a lightweight alternative with extensive customization options and support for various programming tasks.\n\n\n6.2.3 Positron\nPositron IDE is the next-generation integrated development environment developed by Posit, the company behind RStudio. Designed to be a modern, extensible, and language-agnostic IDE, Positron builds on the strengths of RStudio while supporting a broader range of languages and workflows, including R, Python, and Quarto.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#rstudio-layout",
    "href": "App_A-intro-R.html#rstudio-layout",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.3 RStudio Layout",
    "text": "6.3 RStudio Layout\nRStudio consists of several panes: - Source: Where you write scripts and markdown documents. - Console: Where you type and execute R commands. - Environment/History: Shows your variables and command history. - Files/Plots/Packages/Help/Viewer: For file management, viewing plots, managing packages, accessing help, and viewing web content.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-scripts",
    "href": "App_A-intro-R.html#r-scripts",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.4 R Scripts",
    "text": "6.4 R Scripts\nR scripts are plain text files containing R code. You can create a new script in RStudio by clicking File &gt; New File &gt; R Script.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-help",
    "href": "App_A-intro-R.html#r-help",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.5 R Help",
    "text": "6.5 R Help\nUse ?function_name or help(function_name) to access help for any R function. For example:\n?mean\nhelp(mean)",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-packages",
    "href": "App_A-intro-R.html#r-packages",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.6 R Packages",
    "text": "6.6 R Packages\nPackages extend R‚Äôs functionality. There are thousands of packages available in R ecosystem. You may install them from different sources.\n\n6.6.1 With Comprehensive R Archive Network (CRAN)\nCRAN is the primary repository for R packages. It contains thousands of packages that can be easily installed and updated.\nInstall a package with:\ninstall.packages(\"package_name\")\n\n\n6.6.2 With Bioconductor\nBioconductor is a repository for bioinformatics packages in R. It provides tools for the analysis and comprehension of high-throughput genomic data.\nInstall Bioconductor packages using the BiocManager package:\nBiocManager::install(\"package_name\")\n\n\n6.6.3 From GitHub\nMany of the authors of R packages host their work on GitHub. You can install these packages using the devtools package:\ndevtools::install_github(\"username/package_name\")\n\n\n6.6.4 Load a package\nOnce a package is installed, you need to load it into your R session to use its functions:\nlibrary(package_name)\nAlternatively, you may use a function in the package with package_name::function_name() without loading the entire package.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-markdown",
    "href": "App_A-intro-R.html#r-markdown",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.7 R Markdown",
    "text": "6.7 R Markdown\nR Markdown allows you to combine text, code, and output in a single document. Create a new R Markdown file in RStudio via File &gt; New File &gt; R Markdown....\nRecently, the posit team has developed a new version of the R Markdown called quarto document, with the file extension .qmd. It is still under rapid development.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#vectors",
    "href": "App_A-intro-R.html#vectors",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.8 Vectors",
    "text": "6.8 Vectors\nVectors are the most basic data structure in R.\n\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\n\nYou can perform operations on vectors:\n\nx * 2\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#data-sets",
    "href": "App_A-intro-R.html#data-sets",
    "title": "6¬† Appendix: Introduction to R",
    "section": "6.9 Data Sets",
    "text": "6.9 Data Sets\nData frames are used for storing data tables. Create a data frame:\n\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Score = c(90, 85))\ndf\n\n   Name Score\n1 Alice    90\n2   Bob    85\n\n\nYou can import data from files using read.csv() or read.table().\n\nThis appendix is adapted from Why R?.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  }
]