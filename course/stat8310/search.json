[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Description",
    "text": "Description\nThis course will cover the topics in the theory and practice of Bayesian statistical inference, ranging from a review of fundamentals to questions of current research interest. Motivation for the Bayesian approach. Bayesian computation, Monte Carlo methods, asymptotics. Model checking and comparison. A selection of examples and issues in modelling and data analysis. Discussion of advantages and difficulties of the Bayesian approach. This course will be computationally intensive through analysis of data sets using the R statistical computing language.\n\nPrerequisites\nMATH 4752/6752 – Mathematical Statistics II or equivalent, and the ability to program in a high-level language.\n\n\nInstructor\nChi-Kuang Yeh, Assistant Professor in the Department of Mathematics and Statistics, Georgia State University.\n\nOffice: Suite 1407, 25 Park Place.\nEmail: cyeh@gsu.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#office-hour",
    "href": "index.html#office-hour",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Office Hour",
    "text": "Office Hour\n10:00–13:00 on Monday, or by appointment.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#grade-distribution",
    "href": "index.html#grade-distribution",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\nHomework – 50%\nExam – 30%\nFinal – 20%",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assignment",
    "href": "index.html#assignment",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Assignment",
    "text": "Assignment\n\nA1, due on Jan 29, 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#midterm",
    "href": "index.html#midterm",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Midterm",
    "text": "Midterm\n\nMarch 3, 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-and-corresponding-lectures",
    "href": "index.html#topics-and-corresponding-lectures",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Topics and Corresponding Lectures",
    "text": "Topics and Corresponding Lectures\nThose chapters are based on the lecture notes. This part will be updated frequently.\n\n\n\nStatus\nTopic\nLecture Covered\n\n\n\n\n✅\nWelcome and Overview\n1\n\n\n✅\nIntro to R Programming\n2\n\n\n️⏳\nProbability and Exchangeability\n3–",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-textbooks",
    "href": "index.html#recommended-textbooks",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Recommended Textbooks",
    "text": "Recommended Textbooks\n\nGelman, A., Carlin, J., Stern, H., Rubin, D., Dunson, D., and Vehtari, A. (2021). Bayesian Data Analysis, CRC Press, 3rd Ed.\nHoff, P.D. (2009). A First Course in Bayesian Statistical Methods, Springer.\nMcElreath, R. (2018). Statistical Rethinking: A Bayesian Course with Examples in R and Stan, CRC Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#side-readings",
    "href": "index.html#side-readings",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Side Readings",
    "text": "Side Readings\n\nTBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Quick Overview",
    "section": "",
    "text": "1.1 Why Bayesian?\nThe posterior distribution is obtained from the prior distribution and sampling model via Bayes’ rule:\n\\[p(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta') p(\\theta') d \\theta'}.\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#why-bayesian",
    "href": "intro.html#why-bayesian",
    "title": "1  Quick Overview",
    "section": "",
    "text": "Intuitive probability interpretation: Directly quantifies uncertainty about parameters as probability distributions\nIncorporates prior knowledge: Systematically combines domain expertise with data through the prior distribution\nPrincipled inference: Bayes’ rule provides a coherent framework for updating beliefs based on evidence\nNatural handling of uncertainty: Posterior distributions capture full uncertainty, not just point estimates\nSequential analysis: Easily updates beliefs as new data arrives (posterior becomes new prior)\nSmall sample inference: Performs well with limited data by leveraging prior information\nPrediction with uncertainty: Generates predictive distributions that quantify uncertainty in future observations\nDecision-making: Naturally incorporates loss functions for optimal decision rules\nModel comparison: Bayes factors provide a principled approach to comparing competing models",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#some-bayesian-topics-and-their-computational-focus",
    "href": "intro.html#some-bayesian-topics-and-their-computational-focus",
    "title": "1  Quick Overview",
    "section": "1.2 Some Bayesian Topics and their Computational Focus",
    "text": "1.2 Some Bayesian Topics and their Computational Focus\n\n\n\nTable 1.1: Some of the Bayesian Topics and its computational related focuses.\n\n\n\n\n\n\n\n\n\n\nTopics\nKey Concepts / Readings\nComputing Focus\n\n\n\n\nIntroduction to Bayesian Thinking\nBayesian vs. Frequentist paradigms; Prior, likelihood, posterior\nReview of R basics and reproducible workflows\n\n\nBayesian Inference for Simple Models\nConjugate priors, Beta-Binomial, Normal-Normal, Poisson-Gamma\nSimulating posteriors, visualization\n\n\nPrior Elicitation and Sensitivity\nInformative vs. noninformative priors, Jeffreys prior\nPrior sensitivity plots\n\n\nMonte Carlo Integration\nLaw of large numbers, sampling-based inference\nRandom sampling and Monte Carlo approximation\n\n\nMarkov Chain Monte Carlo (MCMC)\nMetropolis-Hastings, Gibbs sampler\nImplementing MCMC in R\n\n\nConvergence Diagnostics\nTrace plots, autocorrelation, Gelman–Rubin statistic\ncoda, rstan, and bayesplot packages\n\n\nHierarchical Bayesian Models\nPartial pooling, shrinkage, multilevel structures\nrstanarm / brms\n\n\nMidterm Project: Bayesian Linear Regression\nPosterior inference for regression, model selection\nbrms, rstanarm, custom Gibbs samplers\n\n\nBayesian Model Comparison\nBayes factors, BIC, DIC, WAIC, LOO\nPractical comparison via cross-validation\n\n\nModel Checking and Diagnostics\nPosterior predictive checks, residual analysis\npp_check in brms\n\n\nAdvanced Computation\nHamiltonian Monte Carlo (HMC), Variational Inference\nUsing Stan and CmdStanR\n\n\nBayesian Decision Theory\nUtility functions, decision rules, loss minimization\nSimple decision problems in R\n\n\nModern Bayesian Methods\nApproximate Bayesian computation (ABC), Bayesian neural networks\nExamples via rstan or tensorflow-probability\n\n\nStudent Project Presentations\nApplications and case studies\nFull workflow demonstration in R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#interesting-article",
    "href": "intro.html#interesting-article",
    "title": "1  Quick Overview",
    "section": "1.3 Interesting Article:",
    "text": "1.3 Interesting Article:\n\nGoligher, E.C., Harhay, M.O. (2023). What Is the Point of Bayesian Analysis?, American Journal of Respiratory and Critical Care Medicine, 209, 485–487.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "01_probability.html",
    "href": "01_probability.html",
    "title": "2  Belief function and Probability Review",
    "section": "",
    "text": "2.1 Belief functions\nProbability is a way to express rational beliefs.\nSome more notations:\nHow should we interpret these properties, and do they make sense?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#belief-functions",
    "href": "01_probability.html#belief-functions",
    "title": "2  Belief function and Probability Review",
    "section": "",
    "text": "A belief function \\(\\mathrm{Be}(\\cdot)\\) is a function that assigns number to statements such that the large the number, the higher the degree of belief.\n\n\nLet \\(F, G\\), and \\(H\\) be three possibly overlapping statements about the world.\nFor example:\n\nF = { a person owns a smartphone }\nG = { a person uses social media daily }\nH = { a person works remotely at least part of the time }\n\nor\n\nF = { a person has a graduate degree }\nG = { a person works in a STEM field }\nH = { a person is employed in the private sector }\n\nThe perference over bets involving these statements can be used to define a belief function\n\n\\(\\mathrm{Be}(F)&gt;\\mathrm{Be}(G)\\) means you prefer a bet \\(F\\) is true over that \\(G\\) is true.\n\nAlso, we want \\(\\mathrm{Be}(\\cdot)\\) to describe our beliefs under certain conditions\n\n\\(\\mathrm{Be}(F\\mid H) &gt; \\mathrm{Be}(G\\mid H)\\) means that if we knew that \\(H\\) were true, then we would perfer to bet that \\(F\\) is also true over \\(G\\) is also true.\n\\(\\mathrm{Be}(F\\mid G) &gt; \\mathrm{Be}(F\\mid H)\\) means that if we bet on \\(F\\), we would perfer to do it under the condition that \\(G\\) is true rather than \\(H\\) is true.\n\n\n\n\nLet \\(\\neg\\) denote negation. That is, \\(\\neg F\\) is the statement that \\(F\\) is not true.\nLet \\(F \\vee G\\) denote the disjunction (or) of statements \\(F\\) and \\(G\\), meaning that at least one of \\(F\\) or \\(G\\) is true.\nLet \\(F \\wedge G\\) denote the conjunction (and) of statements \\(F\\) and \\(G\\), meaning that both \\(F\\) and \\(G\\) are true.\n\n\nIt has been argued by many that any function that is to numerically represent our beliefs should have the following properties:\n\nB1: \\(\\mathrm{Be}(\\neg H\\mid H) \\le \\mathrm{Be}(F \\mid H) \\le \\mathrm{Be}(H \\mid H)\\)\nB2: \\(\\mathrm{Be}(F \\vee G\\mid H) \\ge  \\max\\{\\mathrm{Be}(F \\mid H), \\mathrm{Be}(G\\mid H)\\}\\)\nB3: \\(\\mathrm{Be}(F \\wedge G\\mid H)\\) can be derived from \\(\\mathrm{Be}(G\\mid H)\\) and \\(\\mathrm{Be}(F\\mid G \\wedge H)\\)\n\n\n\n\nB1 means that the number we assign to \\(\\mathrm{Be}(F \\mid H)\\), our conditional belief in \\(F\\) given \\(H\\), is bounded below and above by the numbers we assign to complete disbelief \\(\\mathrm{Be}(\\neg H \\mid H)\\) and complete belief \\(\\mathrm{Be}(H \\mid H)\\).\nB2 says that our belief that the truth lies in a given set of possibilities should not decrease as we add to the set of possibilities.\nB3 is a bit trickier. To see why it makes sense, imagine you have to decide whether or not \\(F\\) and \\(G\\) are true, knowing that \\(H\\) is true. You could do this by first deciding whether or not \\(G\\) is true given \\(H\\), and if so, then deciding whether or not \\(F\\) is true given \\(G\\) and \\(H\\).\n\n\nRecall the notation from (elementary) probability that, \\(F\\cup G\\) means F or G, and \\(F\\cap G\\) means F and G, and \\(\\emptyset\\) is the empty set.\n\nP1: \\[\n0=\\mathrm{Pr}(\\neg H \\mid H) \\leq \\mathrm{Pr}(F \\mid H) \\leq \\mathrm{Pr}(H \\mid H)=1\n\\]\nP2: \\[\\mathrm{Pr}(F \\cup G \\mid H)=\\mathrm{Pr}(F \\mid H)+\\mathrm{Pr}(G \\mid H),\\quad \\text{if}\\quad F \\cap G=\\emptyset\\]\nP3: \\[\\mathrm{Pr}(F \\cap G \\mid H)=\\mathrm{Pr}(G \\mid H) \\mathrm{Pr}(F \\mid G \\cap H)\\]\n\n\n\n2.1.1 Conclusion\nYou can see that, a probability function satisfy P1–P3 also satisfies B1–B3. Therefore, probability functions are a special case of belief functions, and we can use it to describe our belief.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#events-partitions-and-bayes-rule",
    "href": "01_probability.html#events-partitions-and-bayes-rule",
    "title": "2  Belief function and Probability Review",
    "section": "2.2 Events, Partitions and Bayes’ Rule",
    "text": "2.2 Events, Partitions and Bayes’ Rule\n\nA collectiion of sets \\(\\{H_1,\\dots,H_K\\}\\) is a partition of another set \\(\\mathcal{H}\\) if\n\n\\(H_i \\cap H_j = \\emptyset\\) for all \\(i \\neq j\\) (mutually exclusive);\n\\(\\bigcup_{i=1}^K H_i = \\mathcal{H}\\) (collectively exhaustive).\n\n\nIn the context of indetifying which of several statements is true, if \\(\\mathcal{H}\\) is the set of all possible truths and \\(\\{H_1,\\dots,H_K\\}\\) is a partition of \\(\\mathcal{H}\\), then exactly one set \\(H_j\\) contains the truth.\n\nLet \\(\\mathcal{H}\\) be the status of a statistical model.\nValid partitions include:\n\n\\(\\{\\text{correctly specified}, \\text{misspecified}\\}\\)\n\\(\\{\\text{underfitting}, \\text{well-specified}, \\text{overfitting}\\}\\)\n\n\n\n2.2.1 Partition and Probability\nSuppose \\(\\{H_1,\\dots,H_K\\}\\) is a partition of \\(\\mathcal{H}\\),\\(\\mathrm{Pr}(\\mathcal{H})=1\\) and \\(E\\) is some specific event. Then, by the axioms of probability, we have\n\nLaw of total probability \\[\n\\sum_{k=1}^K \\mathrm{Pr}(H_k)=\\mathrm{Pr}\\left(\\bigcup_{k=1}^K H_k\\right)=\\mathrm{Pr}(\\mathcal{H})=1\n\\]\nLaw of marginal probability \\[\n\\mathrm{Pr}(E)=\\sum_{k=1}^K \\mathrm{Pr}(E \\cap H_k)=\\sum_{k=1}^K \\mathrm{Pr}(E \\mid H_k) \\mathrm{Pr}(H_k)\n\\]\nBayes’ rule \\[\n\\mathrm{Pr}(H_j \\mid E)=\\frac{\\mathrm{Pr}(E \\mid H_j) \\mathrm{Pr}(H_j)}{\\mathrm{Pr}(E)}=\\frac{\\mathrm{Pr}(E \\mid H_j) \\mathrm{Pr}(H_j)}{\\sum_{k=1}^K \\mathrm{Pr}(E \\mid H_k) \\mathrm{Pr}(H_k)}\n\\]\n\n\nA subset of the 1996 General Social Survey includes data on the education level and income for a sample of males over 30 years of age. Let {H1,H2,H3,H4} be the events that a randomly selected person in this sample is in, respectively, the lower 25th percentile, the second 25th percentile, the third 25th percentile and the upper 25th percentile in terms of income. By definition,\n\\[\n\\left\\{\\operatorname{Pr}\\left(H_1\\right), \\operatorname{Pr}\\left(H_2\\right), \\operatorname{Pr}\\left(H_3\\right), \\operatorname{Pr}\\left(H_4\\right)\\right\\}=\\{.25, .25, .25, .25\\} .\n\\]\nNote that \\(\\{H1,H2,H3,H4\\}\\) is a partition and so these probabilities sum to 1. Let \\(E\\) be the event that a randomly sampled person from the survey has a college education. From the survey data, we have \\[\\{\\mathrm{Pr}(E\\mid H_1), \\mathrm{Pr}(E\\mid H_2),\\mathrm{Pr}(E\\mid H_3), \\mathrm{Pr}(E\\mid H_4)\\} = \\{.11, .19, .31, .53\\}.\\] These probabilities do not sum to 1 - they represent the proportions of people with college degrees in the four different income subpopulations \\(H_1, H_2, H_3\\) and \\(H_4\\). Now let’s consider the income distribution of the college-educated population. Using Bayes’ rule we can obtain\n\\(\\{\\mathrm{Pr}(H_1\\mid E), \\mathrm{Pr}(H_2\\mid E), \\mathrm{Pr}(H_3\\mid E), \\mathrm{Pr}(H_4 \\mid E)\\} = \\{0.09, 0.17, 0.27, 0.47\\} ,\\) and we see that the income distribution for people in the college-educated population differs markedly from \\(\\{0.25, 0.25,0.25,0.25\\}\\), the distribution for the general population. Note that these probabilities do sum to 1 - they are the conditional probabilities of the events in the partition, given \\(E\\).\n\nIn Bayesian inference, \\({H_1, . . . ,H_K}\\) often refer to disjoint hypotheses or states of nature and \\(E\\) refers to the outcome of a survey, study or experiment. To compare hypotheses post-experimentally, we often calculate the following ratio: \\[\n\\begin{aligned}\n\\frac{\\operatorname{Pr}\\left(H_i \\mid E\\right)}{\\operatorname{Pr}\\left(H_j \\mid E\\right)} & =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right) \\operatorname{Pr}\\left(H_i\\right) / \\operatorname{Pr}(E)}{\\operatorname{Pr}\\left(E \\mid H_j\\right) \\operatorname{Pr}\\left(H_j\\right) / \\operatorname{Pr}(E)} \\\\\n& =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right) \\operatorname{Pr}\\left(H_i\\right)}{\\operatorname{Pr}\\left(E \\mid H_j\\right) \\operatorname{Pr}\\left(H_j\\right)} \\\\\n& =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right)}{\\operatorname{Pr}\\left(E \\mid H_j\\right)} \\times \\frac{\\operatorname{Pr}\\left(H_i\\right)}{\\operatorname{Pr}\\left(H_j\\right)} \\\\\n& =\\text { \"Bayes factor\" × \"prior beliefs\". }\n\\end{aligned}\n\\] This calculation reminds us that Bayes’ rule does not determine what our beliefs should be after seeing the data, it only tells us how they should change after seeing the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#independence",
    "href": "01_probability.html#independence",
    "title": "2  Belief function and Probability Review",
    "section": "2.3 Independence",
    "text": "2.3 Independence\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent, if given \\(H\\), we have \\(\\mathrm{Pr}(F \\cap G \\mid H) = \\mathrm{Pr}(F\\mid H)\\mathrm{Pr}(G\\mid H)\\).\n\nHow do we interpret conditional independence? By Axiom P3, the following is always true: \\[\n\\begin{array}{rlll}\n\\operatorname{Pr}(G \\mid H) \\operatorname{Pr}(F \\mid H \\cap G) \\stackrel{\\text { always }}{=} \\operatorname{Pr}(F \\cap G \\mid H) & \\stackrel{\\text { independence }}{=} \\operatorname{Pr}(F \\mid H) \\operatorname{Pr}(G \\mid H) \\\\\n\\operatorname{Pr}(G \\mid H) \\operatorname{Pr}(F \\mid H \\cap G) & = & \\operatorname{Pr}(F \\mid H) \\operatorname{Pr}(G \\mid H) \\\\\n\\operatorname{Pr}(F \\mid H \\cap G) & = & \\operatorname{Pr}(F \\mid H) .\n\\end{array}\n\\]\nThus, conditional independence implies that \\(\\mathrm{Pr}(F \\mid H \\cap G) = \\mathrm{Pr}(F\\mid H)\\). In other words, if we know \\(H\\) is true, and \\(F\\) and \\(G\\) are conditionally independent given \\(H\\), then knowing \\(G\\) does not change our belief about \\(F\\).\n\nLet’s consider the conditional dependence of \\(F\\) and \\(G\\) when \\(H\\) is assumed to be true in the following two situations:\nSiutation 1:\n\nF = { a hospital patient is a smoker }\nG = { a hospital patient has lung cancer }\nH = { smoking causes lung cancer}\n\nSituation 2:\n\nF = { a student studies regularly for an exam }\nG = { a student receives a high exam score }\nH = { studying improves exam performance }\n\nThink: In both of these situations, H being true implies a relationship between \\(F\\) and \\(G\\). What about when \\(H\\) is not true?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#random-variables",
    "href": "01_probability.html#random-variables",
    "title": "2  Belief function and Probability Review",
    "section": "2.4 Random Variables",
    "text": "2.4 Random Variables\nIn Bayesian inference a random variable is defined as an unknown numerical quantity about which we make probability statements. For example, the quantitative outcome of a survey, experiment or study is a random variable before the study is performed. Additionally, a fixed but unknown population parameter is also a random variable\n\n2.4.1 Discrete Ramdom variables\nLet \\(Y\\) be a random variable and let \\(\\mathcal{Y}\\) be the set of all possible values that \\(Y\\) can take. If \\(\\mathcal{Y}\\) is countable, meaning that \\(\\mathcal{Y} = \\{y_1,y_2,\\dots\\}\\), then \\(Y\\) is a discrete random variable.\n\nThe event that the outcome \\(Y\\) of our survey has the value \\(Y\\) is expressed as \\(\\{Y=y\\}\\). For each \\(y\\in\\mathcal{Y}\\), the shorthand notation fofr \\(\\mathrm{Pr}(Y=y)\\) is \\(p(y)\\), and \\(p(\\cdot)\\) is called the probability mass function of \\(Y\\), and with two properties\n\n\\(0 \\leq p(y) \\leq 1\\) for all \\(y\\in\\mathcal{Y}\\),\n\\(\\sum_{y\\in\\mathcal{Y}} p(y) = 1\\).\n\n\nGeneral probability statements about \\(Y\\) can be derived from the pdf/pmf, for example, for any subset \\(A \\subseteq \\mathcal{Y}\\), we have \\(\\mathrm{Pr}(Y\\in A) = \\sum_{y\\in A} p(y)\\). When we have two disjoint subsets \\(A\\) and \\(B\\) of \\(\\mathcal{Y}\\), we have \\[\\mathrm{Pr}(Y\\in A \\cup B) = \\mathrm{Pr}(Y\\in A) + \\mathrm{Pr}(Y\\in B)=\\sum_{y\\in A} p(y) + \\sum_{y\\in B} p(y).\\]\n\nLet \\(Y\\) be the number of successes in \\(n\\) independent Bernoulli trials, each with probability of success \\(\\theta\\). Then, \\(Y\\) follows a Binomial distribution with parameters \\(n\\) and \\(\\theta\\), denoted as \\(Y \\sim \\mathrm{Binomial}(n,p)\\). The probability mass function of \\(Y\\) is given by \\[\np(y) = \\mathrm{Pr}(Y=y) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y}, \\quad y=0,1,2,\\dots,n.\n\\] If \\(\\theta=0.3\\) and \\(n=3\\), then the probability of observing exactly 2 successes is \\[\np(2) = \\mathrm{Pr}(Y=2 \\mid \\theta=0.3) = \\binom{3}{2\n} (0.3)^2 (0.7)^{1} = 3 \\cdot 0.09 \\cdot 0.7 = 0.189.\n\\]\n\n\n\n2.4.2 Continuous random variables\nIf \\(\\mathcal{Y}\\) is uncountable, for example, \\(\\mathcal{Y} = \\mathbb{R}\\) or \\(\\mathcal{Y} = (0,1)\\), then \\(Y\\) is a continuous random variable. In this case, we cannot list all possible values of \\(Y\\) and assign probabilities to each value. Instead, we use a probability distribution to describe the distribution of \\(Y\\). That is, the cummulative distribution function (cdf) defined as follows.\n\nThe cummulative distribution function (cdf) of a continuous random variable \\(Y\\) is defined as \\[\nF(y) = \\mathrm{Pr}(Y \\leq y), \\quad y \\in \\mathcal{Y}.\n\\]\n\nNote that, for the cdf \\(F(y)\\), we have the following properties:\n\n\\(0 \\leq F(y) \\leq 1\\) for all \\(y\\in\\mathcal{Y}\\),\n\\(F(y)\\) is non-decreasing, meaning that if \\(y_1 &lt; y_2\\), then \\(F(y_1) \\leq F(y_2)\\),\n\\(\\lim_{y \\to -\\infty} F(y) = 0\\)\n\\(\\lim_{y \\to \\infty} F(y) = 1\\).\n\nProbability of various events can be derived from the cdf. For example, for any interval \\(A = (a,b] \\subseteq \\mathcal{Y}\\), we have \\[\n\\mathrm{Pr}(Y \\in A) = \\mathrm{Pr}(a &lt; Y \\leq b) =\nF(b) - F(a).\n\\] Also, \\(\\mathrm{Pr}(Y \\leq a) = F(a)\\) and \\(\\mathrm{Pr}(Y &gt; a) = 1 - F(a)\\).\n\n\n2.4.3 Description of distributions through quantiles and moments\nIn this subsection, we discuss a few ways to describe probability distributions: quantiles and moments. They are used to describe the behaviour of the distribution compressing them into summary statistics.\n\nThe expectation or mean of a random variable \\(Y\\) can be thought as the centre of mass or the location of the distribution, which is defined as\n\nFor discrete random variable: \\[\nE(Y) = \\sum_{y\\in\\mathcal{Y}} y p(y).\n\\]\nFor continuous random variable: \\[\nE(Y) = \\int_{\\mathcal{Y}} y f(y) dy.\n\\]\n\n\n\n\n\n\n\n\nNoteDifference bewtwen mean, mode and median\n\n\n\n\nMean: the centre of mass of the distribution\nMode: The most probable value of \\(Y\\)\nMedian: The value of Y in the middle of the distribution.\n\n\n\nIn skewed distribution, the three will not equal to each other.\n\nlibrary(ggplot2)\n\n# -----------------------------\n# Theoretical reference lines\n# -----------------------------\nlines_normal &lt;- data.frame(\n  value = c(0, 0, 0),\n  Statistic = c(\"Mean\", \"Median\", \"Mode\")\n)\n\nlines_lognormal &lt;- data.frame(\n  value = c(exp(1/8), 1, exp(-1/4)),\n  Statistic = c(\"Mean\", \"Median\", \"Mode\")\n)\n\ncols &lt;- c(\"Mean\" = \"red\", \"Median\" = \"darkgreen\", \"Mode\" = \"purple\")\n\n# -----------------------------\n# Normal distribution\n# -----------------------------\np1 &lt;- ggplot() +\n  stat_function(fun = dnorm, size = 1, color = \"black\") +\n  geom_vline(\n    data = lines_normal,\n    aes(xintercept = value, color = Statistic),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = cols) +\n  scale_x_continuous(limits = c(-4, 4)) +\n  labs(\n    title = \"Non-skewed Distribution (Normal)\",\n    x = \"Value\", y = \"Density\", color = \"Statistic\"\n  ) +\n  theme_minimal()\n\n# -----------------------------\n# Log-normal distribution: LN(0, 0.5)\n# -----------------------------\np2 &lt;- ggplot() +\n  stat_function(\n    fun = function(x) dlnorm(x, meanlog = 0, sdlog = 0.5),\n    size = 1,\n    color = \"black\"\n  ) +\n  geom_vline(\n    data = lines_lognormal,\n    aes(xintercept = value, color = Statistic),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = cols) +\n  scale_x_continuous(limits = c(0, 8)) +\n  labs(\n    title = \"Skewed Distribution (Log-normal)\",\n    x = \"Value\", y = \"Density\", color = \"Statistic\"\n  ) +\n  theme_minimal()\n\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy use mean?\n\n\n\nThe mean is widely used in statistics and data analysis for several reasons:\n\nMathematical properties: The mean has desirable mathematical properties, such as linearity, which makes it easier to work with in various statistical analyses and models.\nSensitivity to all values: The mean takes into account all values in the dataset, providing a comprehensive measure of central tendency. It is also a scaled version of the total, which is often an interest\nFoundation for other statistical measures: The mean serves as the basis for many other statistical measures, such as variance and standard deviation, which are essential for understanding the spread and variability of data.\nMean minimizes the sum of squared deviations: The mean is the value that minimizes the sum of squared deviations (i.e., the expected penalty by choosing one value) from itself, making it a natural choice for summarizing data.\nMay contains full information: In some distributions (e.g., bernoulli distribution), the mean contains all the information about the distribution, making it a sufficient statistic for inference.\n\n\n\n\nThe variance of a random variable \\(Y\\) measures the spread or dispersion of the distribution, and is defined as \\[\n\\mathrm{Var}(Y) = E\\left[(Y - E(Y))^2\\right] = E[Y^2]- E^2[Y].\n\\] The standard deviation is the square root of the variance, denoted as \\(\\mathrm{SD}(Y) = \\sqrt{\\mathrm{Var}(Y)}\\).\n\n\nThe quantile of order \\(\\alpha\\) of a random variable \\(Y\\) is defined as the value \\(y_\\alpha\\) such that \\[\n\\mathrm{Pr}(Y \\leq y_\\alpha) = F(y_\\alpha) = \\alpha\n\\] for \\(0 &lt; \\alpha &lt; 1\\).\n\nFor example, the median is the quantile of order 0.5, denoted as \\(y_{0.5}\\), which satisfies \\(\\mathrm{Pr}(Y \\leq y_{0.5}) = 0.5\\). Also, \\((y_{0.025},y_{0.975})\\) and \\((y_{0.25},y_{0.75})\\) contains 95% and 50% of the mass of the distribution, respectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#joint-disitrubiton",
    "href": "01_probability.html#joint-disitrubiton",
    "title": "2  Belief function and Probability Review",
    "section": "2.5 Joint Disitrubiton",
    "text": "2.5 Joint Disitrubiton\n\n2.5.1 Discrete random variables\nLet \\(Y_1\\) and \\(Y_2\\) be two random variables with possible values in \\(\\mathcal{Y}_1\\) and \\(\\mathcal{Y}_2\\), respectively. The joint distribution of \\(Y_1\\) and \\(Y_2\\) describes the probability of various combinations of values that \\((Y_1,Y_2)\\) can take.\nJoint beliefs about \\(Y_1\\) and \\(Y_2\\) can be represented with probabilities. For example, for subsets \\(A\\subset \\mathcal{Y}_1\\) and \\(B\\subset \\mathcal{Y}_2\\), \\(\\mathrm{Pr}(\\{Y_1\\in A\\} \\cap \\{Y_2 \\in B\\})\\) represents our belief that \\(Y_1\\) takes a value in \\(A\\) and \\(Y_2\\) takes a value in \\(B\\). The joint pdf or joint density of \\(Y_1\\) and \\(Y_2\\) is defined as\n\\[\np_{Y_1 Y_2}\\left(y_1, y_2\\right)=\\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right), \\text { for } y_1 \\in \\mathcal{Y}_1, y_2 \\in \\mathcal{Y}_2 .\n\\]\nThe marginal density of \\(Y_1\\) can be computed from the joint density: \\[\n\\begin{aligned}\np_{Y_1}\\left(y_1\\right) & \\equiv \\operatorname{Pr}\\left(Y_1=y_1\\right) \\\\\n& =\\sum_{y_2 \\in \\mathcal{Y}_2} \\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right) \\\\\n& \\equiv \\sum_{y_2 \\in \\mathcal{Y}_2} p_{Y_1 Y_2}\\left(y_1, y_2\\right)\n\\end{aligned}\n\\]\nThe conditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\) can be computed from the joint density and the marginal density: \\[\n\\begin{aligned}\np_{Y_2 \\mid Y_1}\\left(y_2 \\mid y_1\\right) & =\\frac{\\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right)}{\\operatorname{Pr}\\left(Y_1=y_1\\right)} \\\\\n& =\\frac{p_{Y_1 Y_2}\\left(y_1, y_2\\right)}{p_{Y_1}\\left(y_1\\right)} .\n\\end{aligned}\n\\]\nYou should be able to see that\n\n\\(\\left\\{p_{Y_1}, p_{Y_2 \\mid Y_1}\\right\\}\\) can be derived from \\(p_{Y_1 Y_2}\\),\n\\(\\left\\{p_{Y_2}, p_{Y_1 \\mid Y_2}\\right\\}\\) can be derived from \\(p_{Y_1 Y_2}\\)\n\\(p_{Y_1 Y_2}\\) can be derived from \\(\\left\\{p_{Y_1}, p_{Y_2 \\mid Y_1}\\right\\}\\)\n\\(p_{Y_1 Y_2}\\) can be derived from \\(\\left\\{p_{Y_2}, p_{Y_1 \\mid\nY_2}\\right\\}\\)\n\nBUT\n\n\\(p_{Y_1 Y_2}\\) cannot be derived from \\(\\left\\{p_{Y_1}, p_{Y_2}\\right\\}\\).\n\nThe subscripts of density functions are often dropped, in which case the type of density function is determined by the arguments. For example,\n\n\\(p(y_1,y_2)=p_{Y_1 Y_2}(y_1,y_2)\\) is the joint density of \\(Y_1\\) and \\(Y_2\\),\n\\(p(y_1)=p_{Y_1}(y_1)\\) is the marginal density of \\(Y_1\\)\n\\(p(y_2 \\mid y_1)=p_{Y_2\\mid Y_1}(y_2 \\mid y_1)\\) is the conditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\), and so on.\n\n\nSuppose a sociological study reports the following joint distribution of parents’ education level and children’s income level in a population.\nJoint distribution of education and income Suppose a sociological study reports the following joint distribution of parents’ education level and children’s income level in a population as shown in the Table below\n\n\n\nParent \\ Child\nLow Income\nMiddle Income\nHigh Income\n\n\n\n\nHigh School or Less\n0.18\n0.22\n0.10\n\n\nCollege\n0.08\n0.20\n0.12\n\n\nGraduate School\n0.04\n0.06\n0.10\n\n\n\nSuppose we randomly sample a parent–child pair from this population.\nLet\n- \\(Y_1\\) be the parent’s education level\n- \\(Y_2\\) be the child’s income level\nWe are interested in the conditional probability that the child has high income, given that the parent has a college education.\nWe may answer this question using the conditional probability formula:\n\\[\n\\Pr(Y_2 = \\text{High Income} \\mid Y_1 = \\text{College})\n= \\frac{\\Pr(Y_2 = \\text{High Income} \\cap Y_1 = \\text{College})}\n{\\Pr(Y_1 = \\text{College})}\n\\]\nFrom the table,\n\\[\n\\Pr(Y_2 = \\text{High Income} \\cap Y_1 = \\text{College}) = 0.12\n\\]\n\\[\n\\Pr(Y_1 = \\text{College}) = 0.08 + 0.20 + 0.12 = 0.40\n\\]\nTherefore,\n\\[\n\\Pr(Y_2 = \\text{High Income} \\mid Y_1 = \\text{College})\n= \\frac{0.12}{0.40}\n= 0.30\n\\]\nThus, our conclusion from the table is, among children whose parents have a college education, 30% attain high income.\n\n\n\n2.5.2 Continuous random variables\nLet \\(Y_1\\) and \\(Y_2\\) be two continuous random variables with possible values in \\(\\mathcal{Y}_1\\) and \\(\\mathcal{Y}_2\\), respectively. The joint distribution of \\(Y_1\\) and \\(Y_2\\) describes the probability of various combinations of values that \\((Y_1,Y_2)\\) can take. We again work with the cumulative distribution function (cdf). The defintion is given as follows.\n\nGiven a continuous joint cdf \\(F_{Y_1 Y_2}(y_1,y_2)\\), there is a function \\(p_{Y_1,Y_2}\\) such that \\[\nF_{Y_1,Y_2}(a,b) = \\int_{-\\infty}^a \\int_{-\\infty}^b p_{Y_1,Y_2}(y_1,y_2) dy_2 dy_1,\n\\] and \\(p_{Y_1,Y_2}(y_1,y_2)\\) is called the joint density function of \\(Y_1\\) and \\(Y_2\\).\n\nSimilar to the discrete case, we can derive marginal and conditional densities from the joint density as\n\nMarginal density of \\(Y_1\\): \\[\np_{Y_1}(y_1) = \\int_{\\mathcal{Y}_2} p_{Y_1,Y_2}(y_1,y_2) dy_2,\n\\]\nConditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\): \\[\np_{Y_2 \\mid Y_1}(y_2 \\mid y_1) = \\frac{p_{Y_1,Y_2}(y_1,y_2)}{p_{Y_1}(y_1)}.\n\\]\n\nThink about why \\(p_{Y_2 \\mid Y_1}(y_2 \\mid y_1)\\) is an actual pdf.\n\n\n2.5.3 Mixed continuous and discrete variables\nIt is possible to have joint distributions involving both discrete and continuous random variables. For example, let \\(Y_1\\) be a discrete random variable taking values in \\(\\mathcal{Y}_1\\) and \\(Y_2\\) be a continuous random variable taking values in \\(\\mathcal{Y}_2\\). The joint distribution of \\(Y_1\\) and \\(Y_2\\) can be described by the joint density function \\(p_{Y_1,Y_2}(y_1,y_2)\\), which gives the probability that \\(Y_1\\) takes the value \\(y_1\\) and \\(Y_2\\) takes a value in an infinitesimal interval around \\(y_2\\). One such as example is that \\(Y_1\\) is a binary variable indicating the presence or absence of a disease, and \\(Y_2\\) is a continuous variable representing the severity of symptoms. Suppose we define\n\nMarginal density \\(p_{Y_1}\\) from our belief \\(\\mathrm{Pr}(Y_1=y_1)\\)\na conditional density \\(p_{Y_2\\mid Y_1}\\) from \\(\\mathrm{Pr}(Y_2\\le y_2\\mid Y_1=y_1)\\doteq F_{Y_2\\mid Y_1}(y_2\\mid y_1)\\).\n\nThen, the joint density can be derived as \\[\np_{Y_1,Y_2}(y_1,y_2) = p_{Y_1}(y_1) p_{Y_2 \\mid Y_1}(y_2 \\mid y_1),\n\\] and the probability can be calculated as \\[\n\\mathrm{Pr}(Y_1\\in A,Y_2\\in B) = \\int_{y_2\\in B} \\left\\{\\sum_{y_1\\in A}p_{Y_1,Y_2}(y_1,y_2)\\right\\}dy_2.\n\\]\n\n\n2.5.4 Bayes’ rule and parameter estimation\nLet\n\n\\(\\theta\\): proportion of people in a large population who have a certain charactersitic.\n\\(Y\\): number of people in a small random sample from the population who have the charactersitic\n\nThen, in this case, we may threat \\(\\theta\\) as continuous random variable taking values in \\(\\Theta = (0,1)\\), and \\(Y\\) as a discrete random variable taking values in \\(\\mathcal{Y}= \\{0,1,2,\\dots,n\\}\\), where \\(n\\) is the sample size. Bayesian estimation of the parameter \\(\\theta\\) derives from the calculate of \\(p(\\theta\\mid y)\\) where \\(y\\) is the observed value of \\(Y\\). In Bayesian, this calculation first requires that we have a joint density \\(p(y,\\theta)\\) representing our belief about \\(\\theta\\) and the survey outcome \\(Y\\). Often, it is natural to construct this joint density from\n\n\\(p(\\theta)\\): our prior belief about \\(\\theta\\) before seeing the data, and\n\\(p(y \\mid \\theta)\\): belief about \\(Y\\) given \\(\\theta\\), often called the likelihood function.\n\nOnce we observed \\(\\{Y=y\\}\\), we need to compute our updated belief about \\(\\theta\\), represented by the posterior density \\(p(\\theta \\mid y)\\) as \\[\np(\\theta \\mid y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta) p(\\theta) d\\theta}.\n\\]\nIf we have two values \\(\\theta_1\\) and \\(\\theta_2\\) in \\(\\Theta\\) that may be true, then the ratio of their posterior densities is given by\n\\[\n\\frac{p(\\theta_1 \\mid y)}{p(\\theta_2 \\mid y)} = \\frac{p(y \\mid \\theta_1) p(\\theta_1) / p(y)}{p(y \\mid \\theta_2) p(\\theta_2) / p(y)} =\n\\frac{p(y \\mid \\theta_1) p(\\theta_1)}{p(y \\mid \\theta_2) p(\\theta_2)}.\n\\]\n\n\n\n\n\n\nNoteNote\n\n\n\nFrom this calculation, we notice when we are calculating the relative posterior probability between two parameter values we do not need calculate \\(p(y)\\) out.\n\n\nAnother way to think about this is, for a function of \\(\\theta\\), \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta).\n\\]\n\n\n\n\n\n\nNoteNote\n\n\n\nWe will see that the numerator is the important part, while the denominator is just a normalizing constant to make sure the posterior density integrates to 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#independence-random-variables",
    "href": "01_probability.html#independence-random-variables",
    "title": "2  Belief function and Probability Review",
    "section": "2.6 Independence Random Variables",
    "text": "2.6 Independence Random Variables\nLet \\(Y_1,\\dots,Y_n\\) be random variables with joint density \\(p(y_1,\\dots,y_n)\\), and \\(\\theta\\) is the parameter describe the conditions under which the random variables are generated. We say that \\(Y_1,\\dots,Y_n\\) are conditionally independent given \\(\\theta\\) if every collection of \\(n\\) sets \\(\\{A_1,\\dots,A_n\\}\\) satisfies \\[\n\\mathrm{Pr}(Y_1 \\in A_1,\\dots,Y_n \\in A_n \\mid \\theta) = \\prod_{i=1}^n \\mathrm{Pr}(Y_i \\in A_i \\mid \\theta).\n\\] If we have independence property, then \\[\n\\mathrm{Pr}(Y_i\\in A_i \\mid \\theta, Y_j\\in A_j) = \\mathrm{Pr}(Y_i \\in A_i \\mid \\theta),\n\\] so the conditional indpenddence can be interpreted as meaning that \\(Y_j\\) gives no additional information about \\(Y_i\\) once we know \\(\\theta\\). Also, under independence, the joint density can be factorized as \\[\np(y_1,\\dots,y_n \\mid \\theta) = \\prod_{i= 1}^n p_{Y_i}(y_i \\mid \\theta).\n\\]\nIf the samples are also identically distributed, meaning that each \\(Y_i\\) has the same marginal density \\(p_Y(y \\mid \\theta)\\), then the joint density can be further simplified as \\[\np(y_1,\\dots,y_n \\mid \\theta) = \\prod_{i= 1}^n p_Y(y_i \\mid \\theta).\n\\]\nIn this case , we say that \\(Y_1,\\dots,Y_n\\) are independent and identically distributed (i.i.d.) given \\(\\theta\\), with notation \\[  \nY_1,\\dots,Y_n\\mid \\theta \\stackrel{i.i.d.}{\\sim} p_Y(y \\mid \\theta).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#exchangeability",
    "href": "01_probability.html#exchangeability",
    "title": "2  Belief function and Probability Review",
    "section": "2.7 Exchangeability",
    "text": "2.7 Exchangeability\n\nA sequence of random variables \\(Y_1,Y_2,\\dots,Y_n\\) is exchangeable if for any permutation \\(\\pi\\) of the indices \\(\\{1,2,\\dots ,n\\}\\), we have \\[\np(y_1,y_2,\\dots,y_n) = p(y_{\\pi(1 )},y_{\\pi(2)},\\dots,y_{\\pi(n)}).\n\\]\n\nIn other words, the joint density of an exchangeable sequence is invariant to the order of the random variables. That is, the labels contains no information about the outcome.\n\nSuppose a factory produces a large batch of items. Each item may be either defective or non-defective.\nLet\n\\[\nY_i =\n\\begin{cases}\n1, & \\text{if the } i\\text{th inspected item is defective}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nWe inspect \\(n = 10\\) items chosen at random from the batch and record\n\\(Y_1, Y_2, \\dots, Y_{10}.\\)\nConsider the following three observed sequences:\n\n\\(p(1,0,1,0,1,0,0,1,0,1)\\)\n\\(p(0,1,0,1,0,1,1,0,0,1)\\)\n\\(p(1,1,0,0,1,0,1,0,0,1)\\)\n\nEach sequence contains 5 defective items and 5 non-defective items.\nQuestion: Is there a reason to assign these three sequences different probabilities?\nIf the inspection order conveys no additional information about quality, then only the number of defective items matters, not their positions in the sequence. This motivates the concept of exchangeability.\n\n\n2.7.1 Independence versus dependence\nConsider the probability assignments\n\\[\n\\begin{cases}\n\\Pr(Y_{10} = 1) = a, \\\\[6pt]\n\\Pr(Y_{10} = 1 \\mid Y_1 = \\cdots = Y_9 = 1) = b.\n\\end{cases}\n\\]\nIf \\(a \\neq b\\), then \\(Y_{10}\\) is not independent of \\(Y_1, \\dots, Y_9\\).\nHowever, lack of independence does not imply lack of exchangeability.\nQuestion: should we have \\(a = b\\), \\(a &gt; b\\) or \\(a  &lt; b\\)?\n\n\n2.7.2 A latent-parameter model\nSuppose the defect rate \\(\\theta\\) of the factory is unknown.\nConditional on \\(\\theta\\), \\[\nY_1, \\dots, Y_{10} \\mid \\theta \\sim \\text{i.i.d. Bernoulli}(\\theta).\n\\]\nThen \\[\n\\Pr(Y_1 = y_1, \\dots, Y_{10} = y_{10} \\mid \\theta)\n= \\theta^{\\sum y_i}(1-\\theta)^{10-\\sum y_i}.\n\\]\nIf our uncertainty about \\(\\theta\\) is described by a prior distribution \\(p(\\theta)\\), the marginal joint distribution is\n\\[\np(y_1, \\dots, y_{10})\n= \\int \\theta^{\\sum y_i}(1-\\theta)^{10-\\sum y_i} p(\\theta)\\, d\\theta.\n\\]\nThis probability depends only on the number of defective items, not their order.\nThus, we have exchangeability, even though the \\(Y_i\\) are not independent under this model of belief.\n\nConditional i.i.d. given a latent parameter implies marginal exchangeability. That is, if \\(\\theta \\sim p(\\theta)\\) and \\(Y_1,\\dots,Y_n\\) are conditionally i.i.d. given \\(\\theta\\), then \\(Y_1,\\dots,Y_n\\) (i.e., unconditional on \\(\\theta\\)) are exchangeable.\n\nFor the Proof, see page 28 in Hopf (2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#de-finettis-theorem",
    "href": "01_probability.html#de-finettis-theorem",
    "title": "2  Belief function and Probability Review",
    "section": "2.8 de Finetti’s Theorem",
    "text": "2.8 de Finetti’s Theorem\nAs of now, we have seen that conditional i.i.d. given a latent parameter implies marginal exchangeability. For example, \\[\n\\begin{cases}\nY_1,\\dots,Y_n \\mid \\theta \\stackrel{i.i.d.}{\\sim}  \\\\\n\\theta \\sim p(\\theta) \\end{cases} \\implies Y_1,\\dots,Y_n \\text{ are exchangeable}.\n\\]\nThe converse is also true, as stated in de Finetti’s theorem.\n\nLet \\(Y_i\\in\\mathcal{Y}\\) for all \\(i \\in\\{1,2,\\dots,n\\}\\) be an exchangeable sequence of random variables. Then, there exists a parameter space \\(\\Theta\\) and a prior distribution \\(p(\\theta)\\) on \\(\\Theta\\) such that the joint distribution of \\(Y_1,\\dots,Y_n\\) can be represented as \\[\np(y_1,\\dots,y_n) = \\int_{\\Theta} \\left\\{\\prod_{i=1}^n p_Y(y_i \\mid \\theta)\\right\\} p(\\theta) d\\theta,\n\\] where \\(p_Y(y \\mid \\theta)\\) is a probability density function on \\(\\mathcal{Y}\\) for each \\(\\theta \\in \\Theta\\). The prior and sampling model depend on the form of the belief model \\(p(y_1,\\dots,y_n)\\).\n\nThe probability distribution \\(p(\\theta)\\) represents our belief about the outcomes \\(\\{Y_1,Y_2,\\dots,Y_n\\}\\), induced by our belief model \\(p(y_1,\\dots,y_n)\\). That is,\n\n\\(p(\\theta)\\) represents our belief about \\(\\lim_{n\\to\\infty} \\sum Y_i/n\\) in the binary sense\n\\(p(\\theta)\\) represents our belief about \\(\\lim_{n\\to\\infty} \\sum (Y_i\\le c)/n\\) for each \\(c\\) in the general case.\n\nThe main idea ofthis and the previous section is as follows \\[\n\\begin{aligned}\nY_1, \\ldots, Y_n \\mid \\theta &\\stackrel{\\text{i.i.d.}}{\\sim} p(\\cdot \\mid \\theta), \\\\\n\\theta &\\sim p(\\theta)\n\\end{aligned}\n\\quad \\Longleftrightarrow \\quad\nY_1, \\ldots, Y_n \\text{ are exchangeable for all } n .\n\\]\nQuestion: When is the condition of “exchangeability for all \\(n\\)” reasonable?\n\nHave exchaneability and repeatability\n\nExchangeability holds if the labels convey no information\nrepeatability hold includes the follows\n\n\\(Y_1,\\dots,Y_n\\) are outcomes of a repeartable experiment\n\\(Y_1,\\dots,Y_n\\) are sampled from a finite population with repleacement\n\\(Y_1,\\dots,Y_n\\) are sampled from an infinite population without replacement.\n\n\n\n\n\n\n\n\n\nNoteIn large finite population\n\n\n\nNote, if \\(Y_1,\\dots,Y_n\\) are exchangeable and sampled from a finite population of size \\(N\\) that is way bigger than \\(n\\) without replacement, then they can be modelled as approximate being conditional i.i.d.\n\n\n\nThis Chapter follows closely with Chapter 2 in Hoff (2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "3  Week 2 — Conjugate Priors and Analytical Posteriors",
    "section": "",
    "text": "3.1 Overview\nThis week focuses on conjugate priors — special priors that yield posteriors in the same family of distributions as the prior.\nStudents will learn why conjugacy simplifies Bayesian inference, how to identify conjugate pairs for common likelihoods, and how to perform analytical posterior updates without simulation.\nWe will also introduce the concept of prior sensitivity analysis and noninformative (objective) priors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 — Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week02.html#learning-goals",
    "href": "week02.html#learning-goals",
    "title": "3  Week 2 — Conjugate Priors and Analytical Posteriors",
    "section": "3.2 Learning Goals",
    "text": "3.2 Learning Goals\nBy the end of Week 2, you should be able to:\n\nDefine and identify conjugate priors for standard likelihood models.\n\nDerive analytical posteriors for Binomial, Poisson, and Normal models.\n\nCompute posterior summaries and predictive distributions.\n\nDiscuss the influence of priors on posterior inference.\n\nPerform prior sensitivity analysis in R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 — Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week02.html#lecture-1-the-concept-of-conjugacy",
    "href": "week02.html#lecture-1-the-concept-of-conjugacy",
    "title": "3  Week 2 — Conjugate Priors and Analytical Posteriors",
    "section": "3.3 Lecture 1: The Concept of Conjugacy",
    "text": "3.3 Lecture 1: The Concept of Conjugacy\n\n3.3.1 1.1 Definition\nA conjugate prior for a likelihood \\(p(y \\mid \\theta)\\) is a prior distribution \\(p(\\theta)\\) such that the posterior \\(p(\\theta \\mid y)\\) belongs to the same family as the prior.\nFormally: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta)\\, p(\\theta)\n\\] If \\(p(\\theta \\mid y)\\) has the same functional form as \\(p(\\theta)\\), then \\(p(\\theta)\\) is conjugate to the likelihood.\n\n\n3.3.2 1.2 Why Conjugacy Matters\n\nProvides closed-form expressions for posterior means, variances, and credible intervals.\n\nFacilitates sequential updating — easy to update priors as new data arrive.\n\nUseful for educational and analytic illustration before moving to MCMC methods.\n\n\n\n3.3.3 1.3 Examples of Conjugate Pairs\n\n\n\n\n\n\n\n\nLikelihood\nConjugate Prior\nPosterior Family\n\n\n\n\nBinomial\\((n,\\theta)\\)\nBeta\\((\\alpha,\\beta)\\)\nBeta\\((\\alpha+y, \\beta+n-y)\\)\n\n\nPoisson\\((\\lambda)\\)\nGamma\\((a,b)\\)\nGamma\\((a+\\sum y_i, b+n)\\)\n\n\nNormal\\((\\mu,\\sigma^2)\\) (known variance)\nNormal\\((\\mu_0,\\tau_0^2)\\)\nNormal\\((\\mu_1,\\tau_1^2)\\)\n\n\nExponential\\((\\lambda)\\)\nGamma\\((a,b)\\)\nGamma\\((a+n, b+\\sum y_i)\\)\n\n\nNormal mean/variance (unknown \\(\\sigma^2\\))\nNormal–Inverse-Gamma\nNormal–Inverse-Gamma",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 — Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week02.html#lecture-2-betabinomial-and-gammapoisson-models",
    "href": "week02.html#lecture-2-betabinomial-and-gammapoisson-models",
    "title": "3  Week 2 — Conjugate Priors and Analytical Posteriors",
    "section": "3.4 Lecture 2: Beta–Binomial and Gamma–Poisson Models",
    "text": "3.4 Lecture 2: Beta–Binomial and Gamma–Poisson Models\n\n3.4.1 2.1 Beta–Binomial Model (Review and Generalization)\nLet \\(y \\mid \\theta \\sim \\text{Binomial}(n,\\theta)\\) and \\(\\theta \\sim \\text{Beta}(\\alpha_0,\\beta_0)\\).\nThen the posterior is: \\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha_0 + y, \\beta_0 + n - y).\n\\]\nPosterior Mean: \\[\nE[\\theta \\mid y] = \\frac{\\alpha_0 + y}{\\alpha_0 + \\beta_0 + n}.\n\\]\nPredictive Probability for a Future Success: \\[\np(\\tilde{y}=1 \\mid y) = E[\\theta \\mid y].\n\\]\nInterpretation:\nEach observation updates the Beta prior by adding one success or failure to the corresponding shape parameter.\n\n\n\n3.4.2 2.2 Gamma–Poisson Model (Counts)\nSuppose we model count data as \\(y_i \\sim \\text{Poisson}(\\lambda)\\), with prior \\(\\lambda \\sim \\text{Gamma}(a_0, b_0)\\)\n(where the Gamma density is parameterized as \\(p(\\lambda) \\propto \\lambda^{a_0-1} e^{-b_0\\lambda}\\)).\nPosterior: \\[\n\\lambda \\mid y_1,\\ldots,y_n \\sim \\text{Gamma}\\left(a_0 + \\sum_{i=1}^n y_i,\\; b_0 + n\\right).\n\\]\nPosterior Mean and Variance: \\[\nE[\\lambda \\mid y] = \\frac{a_0 + \\sum y_i}{b_0 + n}, \\quad\n\\text{Var}[\\lambda \\mid y] = \\frac{a_0 + \\sum y_i}{(b_0 + n)^2}.\n\\]\nPosterior Predictive: \\[\np(\\tilde{y} \\mid y) = \\int \\text{Poisson}(\\tilde{y} \\mid \\lambda)\\, p(\\lambda \\mid y)\\, d\\lambda,\n\\] which follows a Negative Binomial distribution.\nInterpretation:\nThe Gamma prior acts as if we had observed \\(a_0-1\\) pseudo-events over \\(b_0\\) pseudo-trials.\n\n\n\n3.4.3 2.3 R Example: Gamma–Poisson Updating\n\n# Posterior update for Gamma-Poisson model\ny &lt;- c(3, 2, 4, 1, 0, 2, 3)\na0 &lt;- 2; b0 &lt;- 1   # prior Gamma(2,1)\nn &lt;- length(y)\n\na1 &lt;- a0 + sum(y)\nb1 &lt;- b0 + n\n\nlambda &lt;- seq(0, 10, length.out = 400)\nprior &lt;- dgamma(lambda, a0, b0)\nposterior &lt;- dgamma(lambda, a1, b1)\n\nplot(lambda, prior, type=\"l\", lwd=2, col=\"blue\", ylim=c(0, max(posterior)),\n     ylab=\"Density\", xlab=expression(lambda),\n     main=\"Gamma-Poisson Updating\")\nlines(lambda, posterior, col=\"red\", lwd=2)\nlegend(\"topright\",\n       legend=c(\"Prior Gamma(2,1)\", paste0(\"Posterior Gamma(\", a1, \",\", b1, \")\")),\n       col=c(\"blue\", \"red\"), lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2 — Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "4  Week 3 — Monte Carlo Integration and Simulation-Based Bayesian Inference",
    "section": "",
    "text": "4.1 Overview\nThis week introduces Monte Carlo methods, which allow us to approximate Bayesian quantities when analytical solutions are unavailable.\nWe explore how random sampling can be used to estimate expectations, posterior summaries, and probabilities.\nBy the end of this week, students will understand how Monte Carlo simulation forms the foundation for modern Bayesian computation such as MCMC.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 — Monte Carlo Integration and Simulation-Based Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "week03.html#learning-goals",
    "href": "week03.html#learning-goals",
    "title": "4  Week 3 — Monte Carlo Integration and Simulation-Based Bayesian Inference",
    "section": "4.2 Learning Goals",
    "text": "4.2 Learning Goals\nBy the end of Week 3, you should be able to:\n\nExplain the motivation for Monte Carlo methods in Bayesian inference.\n\nApproximate expectations, integrals, and posterior summaries using random sampling.\n\nImplement crude Monte Carlo and importance sampling in R.\n\nAssess the accuracy and variance of Monte Carlo estimators.\n\nInterpret Monte Carlo errors and convergence diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 — Monte Carlo Integration and Simulation-Based Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "week03.html#lecture-1-motivation-and-fundamentals-of-monte-carlo",
    "href": "week03.html#lecture-1-motivation-and-fundamentals-of-monte-carlo",
    "title": "4  Week 3 — Monte Carlo Integration and Simulation-Based Bayesian Inference",
    "section": "4.3 Lecture 1: Motivation and Fundamentals of Monte Carlo",
    "text": "4.3 Lecture 1: Motivation and Fundamentals of Monte Carlo\n\n4.3.1 1.1 The Problem\nBayesian inference often requires evaluating integrals such as: \\[\nE[h(\\theta) \\mid y] = \\int h(\\theta)\\, p(\\theta \\mid y)\\, d\\theta,\n\\] which are rarely available in closed form.\n\n\n4.3.2 1.2 Monte Carlo Idea\nIf we can sample \\(\\theta^{(1)}, \\ldots, \\theta^{(M)}\\) from the posterior \\(p(\\theta \\mid y)\\),\nthen we can approximate the expectation by: \\[\n\\hat{E}[h(\\theta)] = \\frac{1}{M} \\sum_{m=1}^M h(\\theta^{(m)}).\n\\] This is called the Monte Carlo estimator.\nBy the Law of Large Numbers, \\(\\hat{E}[h(\\theta)] \\to E[h(\\theta)]\\) as \\(M \\to \\infty\\). The Central Limit Theorem gives: \\[\n\\sqrt{M}\\,(\\hat{E} - E[h(\\theta)]) \\approx N(0, \\text{Var}[h(\\theta)]).\n\\]\n\n\n4.3.3 1.3 Monte Carlo Error\nWe can estimate the simulation error by:\n\\[\n\\text{SE}(\\hat{E}) \\approx \\sqrt{\\frac{\\text{Var}(h(\\theta))}{M}}.\n\\] Larger \\(M\\) gives more accurate approximations but increases computation time.\n\n\n4.3.4 1.4 Simple Example\nCompute \\(E[\\theta]\\) for \\(\\theta \\sim \\text{Beta}(2,5)\\) using Monte Carlo.\n\nset.seed(1)\nM &lt;- 1e5\ntheta &lt;- rbeta(M, 2, 5)\nmean(theta)          # Monte Carlo estimate\n\n[1] 0.2861808\n\nvar(theta) / M       # Monte Carlo variance\n\n[1] 2.56548e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3 — Monte Carlo Integration and Simulation-Based Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "week04.html",
    "href": "week04.html",
    "title": "5  Week 4 — Markov Chain Monte Carlo (MCMC) Methods",
    "section": "",
    "text": "5.1 Overview\nThis week introduces Markov Chain Monte Carlo (MCMC) — a powerful class of algorithms for simulating from complex posterior distributions that are difficult to sample from directly.\nYou will learn the logic of constructing Markov chains with a desired stationary distribution, how to implement the Metropolis–Hastings (MH) and Gibbs samplers, and how to assess convergence and mixing of MCMC chains.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 — Markov Chain Monte Carlo (MCMC) Methods</span>"
    ]
  },
  {
    "objectID": "week04.html#learning-goals",
    "href": "week04.html#learning-goals",
    "title": "5  Week 4 — Markov Chain Monte Carlo (MCMC) Methods",
    "section": "5.2 Learning Goals",
    "text": "5.2 Learning Goals\nBy the end of Week 4, you should be able to:\n\nExplain the intuition behind MCMC and why it works.\n\nImplement simple Metropolis–Hastings and Gibbs algorithms in R.\n\nDiagnose convergence using trace plots and summary statistics.\n\nCompute posterior means, variances, and credible intervals from MCMC samples.\n\nUnderstand practical issues such as burn-in, thinning, and autocorrelation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 — Markov Chain Monte Carlo (MCMC) Methods</span>"
    ]
  },
  {
    "objectID": "week04.html#lecture-1-introduction-to-mcmc",
    "href": "week04.html#lecture-1-introduction-to-mcmc",
    "title": "5  Week 4 — Markov Chain Monte Carlo (MCMC) Methods",
    "section": "5.3 Lecture 1: Introduction to MCMC",
    "text": "5.3 Lecture 1: Introduction to MCMC\n\n5.3.1 1.1 Motivation\nFor many posteriors, sampling directly is infeasible.\nWe instead build a Markov chain whose stationary distribution is the target posterior \\(p(\\theta \\mid y)\\).\nAfter sufficient iterations, the draws from the chain behave like samples from the true posterior.\n\n\n5.3.2 1.2 Markov Chain Basics\nA Markov chain \\(\\{\\theta^{(t)}\\}\\) has the Markov property: \\[\np(\\theta^{(t)} \\mid \\theta^{(t-1)}, \\ldots, \\theta^{(1)}) = p(\\theta^{(t)} \\mid \\theta^{(t-1)}).\n\\] If the chain is ergodic, the distribution of \\(\\theta^{(t)}\\) converges to a stationary distribution \\(\\pi(\\theta)\\).\nMCMC constructs such chains so that \\(\\pi(\\theta) = p(\\theta \\mid y)\\).\n\n\n5.3.3 1.3 Core Idea\nRepeatedly propose a new value \\(\\theta^*\\) and decide whether to accept or reject it\nbased on how likely it is under the posterior.\nThis ensures that samples eventually represent the posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 — Markov Chain Monte Carlo (MCMC) Methods</span>"
    ]
  },
  {
    "objectID": "week04.html#lecture-2-the-metropolishastings-algorithm",
    "href": "week04.html#lecture-2-the-metropolishastings-algorithm",
    "title": "5  Week 4 — Markov Chain Monte Carlo (MCMC) Methods",
    "section": "5.4 Lecture 2: The Metropolis–Hastings Algorithm",
    "text": "5.4 Lecture 2: The Metropolis–Hastings Algorithm\n\n5.4.1 2.1 Algorithm Steps\n\nInitialize with \\(\\theta^{(0)}\\).\n\nFor each iteration \\(t=1,2,\\ldots,T\\):\n\nPropose \\(\\theta^* \\sim q(\\theta^* \\mid \\theta^{(t-1)})\\).\n\nCompute the acceptance probability: \\[\n\\alpha = \\min\\left(1,\\;\n\\frac{p(y \\mid \\theta^*)\\ p(\\theta^*)\\ q(\\theta^{(t-1)} \\mid \\theta^*)}\n     {p(y \\mid \\theta^{(t-1)})\\, p(\\theta^{(t-1)})\\, q(\\theta^* \\mid \\theta^{(t-1)})}\n\\right).\n\\]\nAccept \\(\\theta^*\\) with probability \\(\\alpha\\); otherwise, keep \\(\\theta^{(t)} = \\theta^{(t-1)}\\).\n\nAfter burn-in, the samples \\(\\{\\theta^{(t)}\\}\\) approximate draws from \\(p(\\theta \\mid y)\\).\n\n\n\n5.4.2 2.2 Special Case: Symmetric Proposal\nIf \\(q(\\theta^* \\mid \\theta^{(t-1)}) = q(\\theta^{(t-1)} \\mid \\theta^*)\\), then \\[\n\\alpha = \\min\\left(1,\\; \\frac{p(y \\mid \\theta^*)\\ p(\\theta^*)}\n                         {p(y \\mid \\theta^{(t-1)})\\, p(\\theta^{(t-1)})}\\right).\n\\] This is the Metropolis algorithm.\n\n\n5.4.3 2.3 Example: Posterior for a Normal Mean (Unknown Mean, Known Variance)\nLet \\(y_i \\sim N(\\mu, 1)\\) for \\(i=1,\\ldots,n\\) and prior \\(\\mu \\sim N(0,10^2)\\).\n\nset.seed(123)\n# Data\ny &lt;- rnorm(50, mean = 3, sd = 1)\nn &lt;- length(y)\npost_log &lt;- function(mu) {\n  sum(dnorm(y, mu, 1, log=TRUE)) + dnorm(mu, 0, 10, log=TRUE)\n}\n\n# Metropolis sampler\nT &lt;- 10000\nmu &lt;- numeric(T)\nmu[1] &lt;- 0\nproposal_sd &lt;- 0.5\n\nfor(t in 2:T) {\n  mu_star &lt;- rnorm(1, mu[t-1], proposal_sd)\n  log_alpha &lt;- post_log(mu_star) - post_log(mu[t-1])\n  if(log(runif(1)) &lt; log_alpha) mu[t] &lt;- mu_star else mu[t] &lt;- mu[t-1]\n}\n\nburnin &lt;- 1000\npost_samples &lt;- mu[(burnin+1):T]\n\nhist(post_samples, prob=TRUE, col=\"skyblue\", main=\"Posterior Samples for μ\")\nabline(v = mean(post_samples), col=\"red\", lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4 — Markov Chain Monte Carlo (MCMC) Methods</span>"
    ]
  },
  {
    "objectID": "week05.html",
    "href": "week05.html",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "",
    "text": "6.1 Learning Goals\nThis week introduces methods for evaluating Bayesian model adequacy and comparing models.\nWe focus on Posterior Predictive Checking (PPC) and Bayesian Model Comparison via Bayes factors, WAIC, and LOO.\nStudents will diagnose model fit using replicated data and compare predictive accuracy among competing models.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week05.html#learning-goals",
    "href": "week05.html#learning-goals",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "",
    "text": "Generate and interpret posterior predictive distributions.\n\nUse posterior predictive checks to detect model misspecification.\n\nCompute and interpret WAIC, LOO, and Bayes factors.\n\nEvaluate model adequacy visually and numerically in R.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week05.html#lecture-1-posterior-predictive-checking",
    "href": "week05.html#lecture-1-posterior-predictive-checking",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "6.2 Lecture 1 — Posterior Predictive Checking",
    "text": "6.2 Lecture 1 — Posterior Predictive Checking\n\n6.2.1 Posterior Predictive Distribution\nFor data \\(y\\) and parameters \\(\\theta\\), \\[\np(\\tilde{y}\\mid y) = \\int p(\\tilde{y}\\mid\\theta)\\,p(\\theta\\mid y)\\,d\\theta.\n\\] If a model is adequate, the observed data \\(y\\) should look typical among replicated datasets \\(\\tilde{y}\\) simulated from this distribution.\n\n\n6.2.2 Implementation Steps\n\nChoose a discrepancy statistic \\(T(y,\\theta)\\) capturing an aspect of fit.\n\nFor each posterior draw \\(\\theta^{(m)}\\):\n\nSimulate \\(\\tilde{y}^{(m)}\\!\\sim\\!p(\\tilde{y}\\mid\\theta^{(m)})\\).\n\nCompute \\(T(y,\\theta^{(m)})\\) and \\(T(\\tilde{y}^{(m)},\\theta^{(m)})\\).\n\n\nCompute posterior predictive \\(p\\)-value:\n\\[\np_{\\text{ppc}}=P\\!\\left(T(\\tilde{y},\\theta)\\!&gt;\\!T(y,\\theta)\\mid y\\right).\n\\]\n\nValues near 0 or 1 suggest lack of fit.\n\n\n6.2.3 Example A — Binomial Model\n\nset.seed(5)\nM &lt;- 5000\ny_obs &lt;- 7; n &lt;- 10\n\ntheta &lt;- rbeta(M, 2 + y_obs, 2 + n - y_obs)   # posterior draws\ny_rep &lt;- rbinom(M, n, theta)\n\nppc_p &lt;- mean(y_rep &gt;= y_obs)\nppc_p\n\n[1] 0.509\n\n\n\nhist(y_rep, breaks=seq(-0.5, n+0.5, by=1),\n     col=\"skyblue\", main=\"Posterior Predictive Distribution\", xlab=\"Replicated ỹ\")\nabline(v=y_obs, col=\"red\", lwd=2)\nlegend(\"topright\", legend=c(\"Observed y\"), col=\"red\", lwd=2, bty=\"n\")\n\n\n\n\nPosterior predictive distribution of ỹ\n\n\n\n\n\n\n6.2.4 Example B — Normal Model (Standard Deviation Check)\n\nset.seed(6)\ny &lt;- rnorm(30, mean=0, sd=1)\nmu_draw &lt;- rnorm(1000, 0, 1)\ny_rep_mat &lt;- replicate(200, rnorm(length(y), sample(mu_draw,1), 1))\n\nT_obs &lt;- sd(y)\nT_rep &lt;- apply(y_rep_mat, 2, sd)\n\nmean(T_rep &gt;= T_obs)\n\n[1] 0.145\n\n\n\nhist(T_rep, col=\"lightgray\", main=\"Posterior Predictive Check: SD\",\n     xlab=\"Replicated sd(ỹ)\")\nabline(v=T_obs, col=\"red\", lwd=2)\nlegend(\"topright\", legend=c(\"Observed sd(y)\"), col=\"red\", lwd=2, bty=\"n\")\n\n\n\n\nPPC for sample standard deviation\n\n\n\n\n\n\n6.2.5 Practical Tips\n\nUse multiple statistics (\\(T_1, T_2,\\dots\\)).\n\nVisual checks often outperform a single numeric \\(p_{\\text{ppc}}\\).\n\nIntegrate PPC with subject-matter knowledge and residual plots.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week05.html#lecture-2-bayesian-model-comparison",
    "href": "week05.html#lecture-2-bayesian-model-comparison",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "6.3 Lecture 2 — Bayesian Model Comparison",
    "text": "6.3 Lecture 2 — Bayesian Model Comparison\n\n6.3.1 Motivation\nCompeting Bayesian models are compared by their fit and complexity.\nCommon approaches include Bayes factors, WAIC, and LOO.\n\n\n\n6.3.2 Bayes Factors\nGiven two models \\(M_1\\) and \\(M_2\\), \\[\n\\text{BF}_{12} = \\frac{p(y\\mid M_1)}{p(y\\mid M_2)} ,\\qquad\np(y\\mid M)=\\int p(y\\mid\\theta_M,M)\\,p(\\theta_M\\mid M)\\,d\\theta_M.\n\\] - \\(\\text{BF}_{12}&gt;1\\) favors \\(M_1\\).\n- Express in \\(\\log_{10}\\) scale for interpretation.\n\n\n\n\\(\\log_{10}\\text{BF}_{12}\\)\nEvidence for \\(M_1\\)\n\n\n\n\n0–0.5\nBarely worth mentioning\n\n\n0.5–1\nSubstantial\n\n\n1–2\nStrong\n\n\n&gt;2\nDecisive\n\n\n\n\n\n\n6.3.3 WAIC and LOO (Predictive Criteria)\nWhen computing marginal likelihoods is infeasible, we use predictive criteria:\n\nWAIC (Watanabe–Akaike Information Criterion)\n\\[\n\\text{WAIC} = -2(\\text{lppd} - p_{\\text{WAIC}}),\n\\] where \\(\\text{lppd}=\\sum_i \\log\\!\\left(\\frac{1}{S}\\sum_{s=1}^S p(y_i\\mid\\theta^{(s)})\\right)\\).\nLOO (Leave-One-Out Cross-Validation)\nApproximates the out-of-sample predictive performance: \\[\n\\text{LOO} = \\sum_i \\log p(y_i \\mid y_{-i}),\n\\] usually estimated via Pareto-smoothed importance sampling (PSIS-LOO).\n\nLower WAIC (or higher elpd_loo) indicates better predictive performance.\n\n\n\n6.3.4 Example A — Comparing Two Regression Models with brms (optional heavy computation)\n\n# Uncomment and install if needed\n# install.packages(c(\"brms\", \"loo\"))\n\nlibrary(brms)\nlibrary(loo)\n\nset.seed(7)\ndat &lt;- data.frame(x = rnorm(200))\ndat$y &lt;- 2 + 3*dat$x + 1.5*dat$x^2 + rnorm(200, sd = 1)  # true quadratic\n\n# Fit linear vs quadratic models\nm1 &lt;- brm(y ~ x, data = dat, family = gaussian(), refresh = 0)\nm2 &lt;- brm(y ~ x + I(x^2), data = dat, family = gaussian(), refresh = 0)\n\nloo1 &lt;- loo(m1)\nloo2 &lt;- loo(m2)\nloo_compare(loo1, loo2)\n\npp_check(m1)\npp_check(m2)\n\n\n\n\n6.3.5 Example B — Quick WAIC Comparison via Frequentist Approximation\n\nset.seed(42)\nN &lt;- 150\nx &lt;- rnorm(N)\ny &lt;- 1.5 + 2.2*x + rnorm(N, sd=1.2)\n\nm1 &lt;- lm(y ~ x)\nm2 &lt;- lm(y ~ poly(x, 2, raw = TRUE))\n\nsigma1 &lt;- summary(m1)$sigma\nsigma2 &lt;- summary(m2)$sigma\n\n# Pseudo-WAIC: approximate lppd - 2p using residual sum of squares\nRSS1 &lt;- sum(resid(m1)^2)\nRSS2 &lt;- sum(resid(m2)^2)\nnpar1 &lt;- length(coef(m1))\nnpar2 &lt;- length(coef(m2))\n\nWAIC1 &lt;- -2 * (sum(dnorm(y, predict(m1), sigma1, log=TRUE)) - npar1)\nWAIC2 &lt;- -2 * (sum(dnorm(y, predict(m2), sigma2, log=TRUE)) - npar2)\n\ndata.frame(Model=c(\"Linear\",\"Quadratic\"), WAIC=c(WAIC1,WAIC2))\n\n      Model     WAIC\n1    Linear 473.9530\n2 Quadratic 475.7823\n\n\n\n\n\n6.3.6 Visual Predictive Comparison\n\nplot(x, y, pch=19, col=\"#00000055\", main=\"Observed Data with Fitted Models\")\nxs &lt;- seq(min(x), max(x), length.out=200)\nlines(xs, predict(m1, newdata=data.frame(x=xs)), col=\"steelblue\", lwd=2)\nlines(xs, predict(m2, newdata=data.frame(x=xs)), col=\"firebrick\", lwd=2)\nlegend(\"topleft\", lwd=2, col=c(\"steelblue\",\"firebrick\"),\n       legend=c(\"Linear\",\"Quadratic\"), bty=\"n\")\n\n\n\n\nOverlay of linear and quadratic model fits\n\n\n\n\n\n\n\n6.3.7 Practical Summary\n\n\n\n\n\n\n\n\nMethod\nStrength\nLimitation\n\n\n\n\nPosterior Predictive Check\nDiagnoses lack of fit to observed data\nNot inherently comparative\n\n\nBayes Factor\nTheoretically coherent model evidence\nSensitive to priors; hard integration\n\n\nWAIC / LOO\nOut-of-sample predictive performance\nApproximate; needs posterior draws",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week05.html#lab-5-model-checking-and-comparison",
    "href": "week05.html#lab-5-model-checking-and-comparison",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "6.4 Lab 5 — Model Checking and Comparison",
    "text": "6.4 Lab 5 — Model Checking and Comparison\nObjectives\n\nPerform PPC using both numerical and visual methods.\n\nCompute and interpret WAIC and LOO for model selection.\n\nVisualize predictive differences among models.\n\nPackages\nbrms, loo, bayesplot, ggplot2\nTasks\n\nFit two Bayesian regression models on the same dataset.\n\nConduct posterior predictive checks and compare simulated vs. observed data.\n\nCompute WAIC and LOO; summarize which model performs better.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week05.html#homework-5",
    "href": "week05.html#homework-5",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "6.5 Homework 5",
    "text": "6.5 Homework 5\n\nConceptual\n\nExplain the purpose of posterior predictive checks.\n\nCompare WAIC and LOO conceptually.\n\nComputational\n\nSimulate data from a known model. Fit two Bayesian models in R.\n\nUse PPC, WAIC, and LOO to assess fit.\n\nDiscuss how model choice depends on criterion used.\n\nReflection\n\nWhy might visual checks and numerical metrics disagree?\n\nWhich model would you report and why?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week05.html#key-takeaways",
    "href": "week05.html#key-takeaways",
    "title": "6  Week 5 — Model Checking and Comparison",
    "section": "6.6 Key Takeaways",
    "text": "6.6 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nPosterior Predictive Check\nCompares observed data to replicated draws under the posterior.\n\n\nPosterior Predictive p-Value\nQuantifies fit; extremes suggest model misfit.\n\n\nWAIC / LOO\nPredictive performance measures for Bayesian models.\n\n\nBayes Factor\nRatio of marginal likelihoods for model comparison.\n\n\nCombined Evaluation\nUse graphical and numerical criteria together.\n\n\n\n\nNext Week: Hierarchical Bayesian Models — introducing partial pooling and shrinkage.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5 — Model Checking and Comparison</span>"
    ]
  },
  {
    "objectID": "week06.html",
    "href": "week06.html",
    "title": "7  Week 6 — Hierarchical Bayesian Models",
    "section": "",
    "text": "7.1 Learning Goals\nThis week introduces hierarchical (multilevel) Bayesian models, which allow parameters to vary across groups while sharing information through higher-level priors.\nWe study partial pooling, shrinkage, and their implementation for normal and regression models.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 — Hierarchical Bayesian Models</span>"
    ]
  },
  {
    "objectID": "week06.html#learning-goals",
    "href": "week06.html#learning-goals",
    "title": "7  Week 6 — Hierarchical Bayesian Models",
    "section": "",
    "text": "Explain the motivation for hierarchical modeling.\nFormulate hierarchical models with group-level parameters.\nInterpret partial pooling and shrinkage.\nImplement a two-level Bayesian model in R using simulation or brms.\nCompare complete, no-pooling, and partial-pooling approaches.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 — Hierarchical Bayesian Models</span>"
    ]
  },
  {
    "objectID": "week06.html#lecture-1-motivation-and-structure-of-hierarchical-models",
    "href": "week06.html#lecture-1-motivation-and-structure-of-hierarchical-models",
    "title": "7  Week 6 — Hierarchical Bayesian Models",
    "section": "7.2 Lecture 1 — Motivation and Structure of Hierarchical Models",
    "text": "7.2 Lecture 1 — Motivation and Structure of Hierarchical Models\n\n7.2.1 1.1 Why Hierarchical Models?\nHierarchical models capture structured variability among related groups or units:\n\nRepeated measures within individuals\nStudents within classrooms\nMachines within factories\n\nThey balance within-group and between-group information by introducing group-specific parameters drawn from a common population distribution.\n\n\n\n7.2.2 1.2 Model Structure\nFor group \\(j = 1,\\ldots,J\\) and observations \\(i = 1,\\ldots,n_j\\):\n\\[\ny_{ij} \\mid \\theta_j, \\sigma^2 \\sim \\mathcal{N}(\\theta_j, \\sigma^2), \\qquad\n\\theta_j \\mid \\mu, \\tau^2 \\sim \\mathcal{N}(\\mu, \\tau^2).\n\\]\nTop-level priors: \\[\n\\mu \\sim \\mathcal{N}(0,10^2), \\quad \\tau \\sim \\text{Half-Cauchy}(0,5).\n\\]\n\n\\(\\mu\\): overall population mean\n\n\\(\\tau\\): between-group standard deviation (pooling strength)\n\\(\\sigma\\): within-group standard deviation\n\n\n\n\n7.2.3 1.3 Three Extremes of Pooling\n\n\n\n\n\n\n\n\nModel Type\nDescription\nBehavior\n\n\n\n\nNo pooling\nEstimate each \\(\\theta_j\\) separately\nIgnores commonality across groups\n\n\nComplete pooling\nForce all groups to share one parameter\nIgnores group differences\n\n\nPartial pooling\nCombine information via hierarchical prior\nBalances both; default Bayesian choice\n\n\n\n\n\n\n7.2.4 1.4 Shrinkage Intuition\nPosterior for each group mean $ _j$ shrinks toward the global mean \\(\\mu\\): \\[\nE[\\theta_j \\mid y] = w_j \\bar{y}_j + (1-w_j)\\mu,\n\\] where \\[\nw_j = \\frac{n_j/\\sigma^2}{n_j/\\sigma^2 + 1/\\tau^2}.\n\\]\n\nLarge \\(n_j\\) (lots of data): \\(w_j \\to 1\\) → less shrinkage.\nSmall \\(n_j\\): \\(w_j \\to 0\\) → stronger shrinkage toward \\(\\mu\\).\n\n\n\n\n7.2.5 1.5 Example — Simulated Group Means\n\nset.seed(6)\nJ &lt;- 8; n_j &lt;- rep(10, J)\nmu_true &lt;- 5; tau_true &lt;- 2; sigma_true &lt;- 1\n\ntheta_true &lt;- rnorm(J, mu_true, tau_true)\ny &lt;- sapply(theta_true, function(tj) rnorm(10, tj, sigma_true))\nybar &lt;- colMeans(y)\n\n# No pooling (group means)\nno_pool &lt;- ybar\n\n# Complete pooling (global mean)\ncomplete_pool &lt;- mean(y)\n\n# Partial pooling: simple empirical Bayes shrinkage\ntau_hat &lt;- sd(ybar)\nsigma_hat &lt;- sd(as.vector(y))\nw &lt;- (n_j/sigma_hat^2) / (n_j/sigma_hat^2 + 1/tau_hat^2)\npartial_pool &lt;- w*ybar + (1-w)*complete_pool\n\ndata.frame(Group=1:J,\n           ybar=round(ybar,2),\n           NoPool=round(no_pool,2),\n           Partial=round(partial_pool,2))\n\n  Group ybar NoPool Partial\n1     1 5.53   5.53    5.53\n2     2 4.06   4.06    4.21\n3     3 6.90   6.90    6.76\n4     4 8.19   8.19    7.91\n5     5 4.86   4.86    4.93\n6     6 5.42   5.42    5.43\n7     7 2.20   2.20    2.55\n8     8 6.99   6.99    6.84\n\n\nObserve how partial-pool estimates move small-sample groups toward the global mean.\n\n\n\n7.2.6 1.6 Advantages of Hierarchical Models\n\nBorrow strength across groups.\n\nNaturally incorporate uncertainty at multiple levels.\n\nHandle unbalanced data and missingness elegantly.\n\nAllow group-level predictors and complex dependence structures.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 — Hierarchical Bayesian Models</span>"
    ]
  },
  {
    "objectID": "week06.html#lecture-2-hierarchical-regression-and-implementation",
    "href": "week06.html#lecture-2-hierarchical-regression-and-implementation",
    "title": "7  Week 6 — Hierarchical Bayesian Models",
    "section": "7.3 Lecture 2 — Hierarchical Regression and Implementation",
    "text": "7.3 Lecture 2 — Hierarchical Regression and Implementation\n\n7.3.1 2.1 Hierarchical Linear Regression\nGeneral form: \\[\ny_{ij} = \\alpha_j + \\beta_j x_{ij} + \\varepsilon_{ij}, \\quad \\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^2),\n\\] \\[\n\\alpha_j \\sim \\mathcal{N}(\\mu_\\alpha, \\tau_\\alpha^2), \\quad\n\\beta_j \\sim \\mathcal{N}(\\mu_\\beta, \\tau_\\beta^2).\n\\]\nThis allows both intercepts and slopes to vary by group.\n\n\n\n7.3.2 2.2 Example — Hierarchical Regression with brms\n\nlibrary(brms)\nset.seed(7)\n\nJ &lt;- 10\nn_j &lt;- 20\ngroup &lt;- rep(1:J, each=n_j)\nx &lt;- rnorm(J*n_j, 0, 1)\n\nalpha_true &lt;- rnorm(J, 2, 1)\nbeta_true  &lt;- rnorm(J, 3, 0.5)\nsigma_true &lt;- 0.8\n\ny &lt;- alpha_true[group] + beta_true[group]*x + rnorm(J*n_j, 0, sigma_true)\ndat &lt;- data.frame(y, x, group=factor(group))\n\n# Hierarchical model (random intercept and slope)\nm_hier &lt;- brm(y ~ 1 + x + (1 + x | group),\n              data=dat, family=gaussian(),\n              chains=2, iter=2000, refresh=0)\n\nsummary(m_hier)\nplot(m_hier)\n\nThe (1 + x | group) formula defines a varying intercept and slope for each group.\n\n\n\n7.3.3 2.3 Interpretation\nPosterior summaries provide:\n\nGroup-level means \\(\\alpha_j, \\beta_j\\).\nPopulation-level means \\(\\mu_\\alpha, \\mu_\\beta\\).\nVariability estimates \\(\\tau_{\\alpha}, \\tau_{\\beta}\\) showing degree of pooling.\n\nVisualize partial pooling by comparing group-specific fits to the global regression line.\n\npp_check(m_hier)\nplot(conditional_effects(m_hier), points=TRUE)\n\n\n\n\n7.3.4 2.4 Practical Considerations\n\nChoose weakly informative hyperpriors for scale parameters (e.g., Half-Cauchy or Exponential).\n\nInspect group-level posterior intervals to assess pooling.\n\nCenter predictors for numerical stability.\n\nUse hierarchical models as the default when groups share a common process.\n\n\n\n\n7.3.5 2.5 Summary of Hierarchical Modeling Benefits\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nPartial pooling\nShares strength across groups while retaining group differences.\n\n\nShrinkage\nStabilizes small-sample estimates toward population mean.\n\n\nInterpretability\nCaptures multi-level variation naturally.\n\n\nPredictive accuracy\nUsually superior to separate or fully pooled models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 — Hierarchical Bayesian Models</span>"
    ]
  },
  {
    "objectID": "week06.html#homework-6",
    "href": "week06.html#homework-6",
    "title": "7  Week 6 — Hierarchical Bayesian Models",
    "section": "7.4 Homework 6",
    "text": "7.4 Homework 6\n\nConceptual\n\nExplain why hierarchical modeling is often superior to analyzing groups separately.\n\nDistinguish between complete pooling, no pooling, and partial pooling.\n\nComputational\n\nSimulate a small dataset with several groups and fit:\n\nSeparate regressions (no pooling).\n\nA single pooled regression.\n\nA hierarchical model (partial pooling).\n\n\nCompare estimates for each group and interpret shrinkage behavior.\n\nReflection\n\nIn what situations would you not use a hierarchical model?\n\nHow does the hierarchical prior act as a regularizer?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 — Hierarchical Bayesian Models</span>"
    ]
  },
  {
    "objectID": "week06.html#key-takeaways",
    "href": "week06.html#key-takeaways",
    "title": "7  Week 6 — Hierarchical Bayesian Models",
    "section": "7.5 Key Takeaways",
    "text": "7.5 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nHierarchical Model\nCombines group-level and population-level inference.\n\n\nPartial Pooling\nBalances within- and between-group information.\n\n\nShrinkage\nMoves noisy group estimates toward a global mean.\n\n\nHierarchical Regression\nExtends pooling to both intercepts and slopes.\n\n\nPractical Insight\nDefault choice when analyzing grouped or multilevel data.\n\n\n\n\nNext Week: Bayesian Decision Theory — introducing utilities, losses, and optimal decision rules under uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6 — Hierarchical Bayesian Models</span>"
    ]
  },
  {
    "objectID": "week07.html",
    "href": "week07.html",
    "title": "8  Week 7 — Bayesian Decision Theory",
    "section": "",
    "text": "8.1 Learning Goals\nThis week introduces the decision-theoretic foundation of Bayesian inference.\nWe study how posterior distributions lead naturally to optimal decisions when losses or utilities are specified, and apply the theory to point estimation and hypothesis testing.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 — Bayesian Decision Theory</span>"
    ]
  },
  {
    "objectID": "week07.html#learning-goals",
    "href": "week07.html#learning-goals",
    "title": "8  Week 7 — Bayesian Decision Theory",
    "section": "",
    "text": "Describe the Bayesian decision-theoretic framework.\n\nDefine loss functions and posterior expected loss.\n\nDerive Bayes rules for common loss functions.\n\nApply Bayesian decision principles to estimation and classification.\n\nDistinguish between point estimation, interval estimation, and decision-making contexts.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 — Bayesian Decision Theory</span>"
    ]
  },
  {
    "objectID": "week07.html#lecture-1-principles-of-bayesian-decision-theory",
    "href": "week07.html#lecture-1-principles-of-bayesian-decision-theory",
    "title": "8  Week 7 — Bayesian Decision Theory",
    "section": "8.2 Lecture 1 — Principles of Bayesian Decision Theory",
    "text": "8.2 Lecture 1 — Principles of Bayesian Decision Theory\n\n8.2.1 1.1 Motivation\nStatistical inference often involves making decisions under uncertainty:\nselect an action \\(a\\)based on observed data \\(y\\).\nEach action has a loss (or utility) depending on the true parameter value $$.\n\n\n8.2.2 1.2 The Decision-Theoretic Setup\n\nParameter: \\(\\theta \\in \\Theta\\)\nData: \\(y\\)\nAction space: \\(\\mathcal{A}\\)\nLoss function: \\(L(a,\\theta)\\)\n\nAfter observing \\(y\\), the Bayesian chooses an action \\(a(y)\\)minimizing posterior expected loss: \\[\n\\rho(a\\mid y) = E[L(a,\\theta)\\mid y] = \\int L(a,\\theta)\\,p(\\theta\\mid y)\\,d\\theta.\n\\]\nBayes rule:\n\\[\na^*(y) = \\arg\\min_a \\rho(a\\mid y).\n\\]\n\n\n\n8.2.3 1.3 Common Loss Functions and Bayes Rules\n\n\n\n\n\n\n\n\nLoss Function\nForm\nBayes Action\n\n\n\n\nSquared Error\n\\(L(a,\\theta)=(a-\\theta)^2\\)\nPosterior mean \\(E[\\theta\\mid y]\\)\n\n\nAbsolute Error\n\\(L(a,\\theta)=\\|a-\\theta\\|\\)\nPosterior median\n\n\n0–1 Loss\n\\(L(a,\\theta)=\\mathbb{1}\\{a\\neq\\theta\\}\\)\nPosterior mode (MAP)\n\n\n\nThese connect the posterior mean, median, and mode to optimal decisions under different losses.\n\n\n\n8.2.4 1.4 Example — Estimation under Quadratic Loss\nSuppose \\(y\\mid\\theta\\sim N(\\theta,1)\\)with prior \\(\\theta\\sim N(0,1)\\).\nPosterior: \\(\\theta\\mid y \\sim N\\!\\left(\\frac{y}{2}, \\frac{1}{2}\\right)\\).\nBayes estimator under squared loss: \\[\na^*(y)=E[\\theta\\mid y]=\\frac{y}{2}.\n\\]\n\nset.seed(7)\ny &lt;- seq(-4, 4, length=100)\nbayes_est &lt;- y/2\nplot(y, bayes_est, type=\"l\", lwd=2, col=\"steelblue\",\n     main=\"Bayes Estimator under Squared Loss\", xlab=\"y\", ylab=\"a*(y)\")\nabline(a=0, b=1, col=\"red\", lty=2)\nlegend(\"topleft\", legend=c(\"Bayes estimator\",\"MLE (a=y)\"),\n       col=c(\"steelblue\",\"red\"), lwd=2, lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\nInterpretation: The Bayes rule shrinks the estimate toward zero (the prior mean), especially for small |y|.\n\n\n\n8.2.5 1.5 Decision Rules and Risk\nThe Bayes risk is the expected loss averaged over data and parameters: \\[\nr(a) = E[L(a(Y),\\Theta)] = \\int\\!\\!\\int L(a(y),\\theta)\\,p(y,\\theta)\\,dy\\,d\\theta.\n\\]\nA decision rule minimizing Bayes risk across all priors is admissible (cannot be uniformly improved).\n\n\n\n8.2.6 1.6 Example — Hypothesis Testing with 0–1 Loss\nWe test \\(H_0:\\theta\\le0\\)vs \\(H_1:\\theta\\&gt;0\\).\nLoss: \\(L(a,\\theta)= \\begin{cases} 0 & \\text{if correct},\\\\ 1 & \\text{if wrong}. \\end{cases}\\)\nPosterior decision rule: \\[\n\\text{Accept } H_1 \\text{ if } P(\\theta&gt;0\\mid y) &gt; 0.5.\n\\]\n\nset.seed(8)\ntheta_draws &lt;- rnorm(5000, mean=1, sd=1)\nmean(theta_draws &gt; 0)   # posterior probability of H1\n\n[1] 0.8406",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 — Bayesian Decision Theory</span>"
    ]
  },
  {
    "objectID": "week07.html#lecture-2-applications-and-extensions",
    "href": "week07.html#lecture-2-applications-and-extensions",
    "title": "8  Week 7 — Bayesian Decision Theory",
    "section": "8.3 Lecture 2 — Applications and Extensions",
    "text": "8.3 Lecture 2 — Applications and Extensions\n\n8.3.1 2.1 Bayesian Credible Intervals as Decision Regions\nFor a given loss that penalizes excluding the true parameter,\na credible interval minimizing posterior expected loss corresponds to the shortest interval containing a fixed posterior probability (e.g. 95%).\n\ntheta_post &lt;- rnorm(5000, mean=2, sd=1)\nquantile(theta_post, c(0.025, 0.975))\n\n        2.5%        97.5% \n-0.004383415  4.024907476 \n\n\n\n\n\n8.3.2 2.2 Decision Theory for Classification\nFor a two-class problem with class probabilities \\(p_1 = P(Y=1\\mid x)\\)and \\(p_0 = 1-p_1\\):\nMinimize expected loss \\(L(a,y)\\)using a loss matrix.\n\n\n\nTrue Class\nPredict 0\nPredict 1\n\n\n\n\n0\n0\n\\(c_{10}\\)\n\n\n1\n\\(c_{01}\\)\n0\n\n\n\nBayes rule: choose class 1 if \\[\n\\frac{p_1}{p_0} &gt; \\frac{c_{10}}{c_{01}}.\n\\]\nThe usual 0–1 loss corresponds to \\(c\\_{10}=c\\_{01}=1\\), threshold = 0.5.\n\np1 &lt;- seq(0,1,length=200)\nthreshold &lt;- 0.5\nplot(p1, ifelse(p1&gt;threshold,1,0), type=\"s\", col=\"steelblue\", lwd=2,\n     main=\"Bayesian Decision Boundary (Two-Class)\", xlab=\"P(Y=1|x)\", ylab=\"Decision: 1=Class1\")\nabline(v=threshold, col=\"red\", lty=2)\nlegend(\"topleft\", legend=c(\"Decision Rule\",\"Threshold 0.5\"),\n       col=c(\"steelblue\",\"red\"), lwd=2, lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n8.3.3 2.3 Loss vs Utility\nUtility \\(U(a,\\theta)\\)is simply the negative of loss.\nMaximizing expected utility is equivalent to minimizing expected loss: \\[\na^*(y) = \\arg\\max_a E[U(a,\\theta)\\mid y].\n\\]This framing is often used in economics and decision analysis.\n\n\n\n8.3.4 2.4 Connection to Frequentist Estimation\nUnder certain priors and symmetric losses, Bayes rules coincide with frequentist estimators (e.g. posterior mean = MLE for flat priors).\nBayesian decision theory thus generalizes classical estimation.\n\n\n\n8.3.5 2.5 Example — Optimal Cutoff for a Diagnostic Test\nLet \\(\\theta\\)denote disease presence (1 = disease).\nIf false negatives cost 5× more than false positives, the optimal threshold satisfies \\[\n\\frac{p_1}{p_0} &gt; \\frac{1}{5} \\;\\Rightarrow\\; p_1 &gt; 0.17.\n\\]\n\np &lt;- seq(0,1,length=200)\ndecision &lt;- ifelse(p &gt; 0.17, 1, 0)\nplot(p, decision, type=\"s\", col=\"darkorange\", lwd=2,\n     main=\"Decision Boundary with Unequal Losses\", xlab=\"Posterior P(Disease=1)\", ylab=\"Decision (1=Treat)\")\nabline(v=0.17, col=\"red\", lty=2)\nlegend(\"topleft\", legend=c(\"Decision\",\"Optimal cutoff\"), col=c(\"darkorange\",\"red\"),\n       lwd=2, lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n8.3.6 2.6 Summary of Bayesian Decision Theory\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nLoss function\nQuantifies cost of wrong decisions\n\n\nPosterior expected loss\nAverage loss given observed data\n\n\nBayes rule\nAction minimizing posterior expected loss\n\n\nCommon losses\nSquared, absolute, 0–1\n\n\nApplications\nEstimation, hypothesis testing, classification, decision support",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 — Bayesian Decision Theory</span>"
    ]
  },
  {
    "objectID": "week07.html#homework-7",
    "href": "week07.html#homework-7",
    "title": "8  Week 7 — Bayesian Decision Theory",
    "section": "8.4 Homework 7",
    "text": "8.4 Homework 7\n\nConceptual\n\nDefine loss and risk in the Bayesian framework.\n\nWhat is the relationship between posterior mean, median, and mode under different losses?\n\nComputational\n\nSimulate data from \\(N(\\theta,1)\\)with prior \\(N(0,1)\\).\n\nCompute the Bayes estimator under squared loss and compare it with the MLE.\n\nRepeat using absolute loss and report the posterior median.\n\nReflection\n\nHow does changing the loss function alter your decision?\n\nGive a real-world example where asymmetric losses are important.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 — Bayesian Decision Theory</span>"
    ]
  },
  {
    "objectID": "week07.html#key-takeaways",
    "href": "week07.html#key-takeaways",
    "title": "8  Week 7 — Bayesian Decision Theory",
    "section": "8.5 Key Takeaways",
    "text": "8.5 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nDecision Theory\nProvides a unified framework linking inference to action.\n\n\nBayes Rule\nMinimizes posterior expected loss.\n\n\nCommon Losses\nSquared → mean; absolute → median; 0–1 → mode.\n\n\nApplications\nEstimation, testing, classification, optimal thresholds.\n\n\nPerspective\nInference as a special case of decision-making under uncertainty.\n\n\n\n\nNext Week: Advanced Bayesian Computation — Hamiltonian Monte Carlo (HMC) and Variational Inference.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7 — Bayesian Decision Theory</span>"
    ]
  },
  {
    "objectID": "week08.html",
    "href": "week08.html",
    "title": "9  Week 8 — Advanced Bayesian Computation",
    "section": "",
    "text": "9.1 Learning Goals\nThis week explores two major developments that enable scalable Bayesian inference for complex or high-dimensional models:\nHamiltonian Monte Carlo (HMC) and Variational Inference (VI).\nWe study their principles, intuition, and practical use in software such as Stan and brms.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8 — Advanced Bayesian Computation</span>"
    ]
  },
  {
    "objectID": "week08.html#learning-goals",
    "href": "week08.html#learning-goals",
    "title": "9  Week 8 — Advanced Bayesian Computation",
    "section": "",
    "text": "Explain the motivation for advanced sampling and approximation methods.\n\nDescribe the mechanics and intuition of Hamiltonian Monte Carlo.\n\nUnderstand the trade-offs between exact (MCMC) and approximate (VI) inference.\n\nRun basic HMC and VI fits using modern R interfaces.\n\nInterpret diagnostics for both approaches.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8 — Advanced Bayesian Computation</span>"
    ]
  },
  {
    "objectID": "week08.html#lecture-1-hamiltonian-monte-carlo-hmc",
    "href": "week08.html#lecture-1-hamiltonian-monte-carlo-hmc",
    "title": "9  Week 8 — Advanced Bayesian Computation",
    "section": "9.2 Lecture 1 — Hamiltonian Monte Carlo (HMC)",
    "text": "9.2 Lecture 1 — Hamiltonian Monte Carlo (HMC)\n\n9.2.1 1.1 Motivation\nTraditional MCMC (e.g., Metropolis–Hastings, Gibbs) can mix slowly in high dimensions.\nHamiltonian Monte Carlo accelerates exploration by using gradient information from the log posterior to simulate physical motion through parameter space.\n\n\n\n9.2.2 1.2 Hamiltonian Dynamics\nWe introduce an auxiliary “momentum” variable \\(p\\) and define the Hamiltonian: \\[\nH(\\theta,p) = U(\\theta) + K(p),\n\\] where\n- \\(U(\\theta) = -\\log p(\\theta\\mid y)\\) (potential energy = negative log posterior),\n- \\(K(p) = \\tfrac{1}{2} p^\\top M^{-1}p\\) (kinetic energy, with mass matrix \\(M\\)).\nThe system evolves via Hamilton’s equations: \\[\n\\frac{d\\theta}{dt} = \\frac{\\partial H}{\\partial p}, \\qquad\n\\frac{dp}{dt} = -\\frac{\\partial H}{\\partial \\theta}.\n\\]\n\n\n\n9.2.3 1.3 Leapfrog Integration\nTo approximate continuous motion, HMC uses a leapfrog integrator with step size \\(\\epsilon\\) and \\(L\\) steps:\n\n\\(p\\_{t+\\epsilon/2} = p_t - \\frac{\\epsilon}{2}\\nabla_\\theta U(\\theta_t)\\)\n\n\\(\\theta\\_{t+\\epsilon} = \\theta_t + \\epsilon M^{-1}p_{t+\\epsilon/2}\\)\n\n\\(p_{t+\\epsilon} = p_{t+\\epsilon/2} - \\frac{\\epsilon}{2}\\nabla_\\theta U(\\theta_{t+\\epsilon})\\)\n\nAfter simulating this path, we apply a Metropolis acceptance step using the change in \\(H\\).\n\n\n\n9.2.4 Intuition\n\nThe gradient \\(\\nabla_\\theta U(\\theta)\\) guides proposals along high-density regions, avoiding random walk behavior.\n\nProper tuning of step size \\(\\epsilon\\) and number of steps \\(L\\) yields efficient exploration.\n\nModern implementations (e.g., Stan) adapt these automatically via the No-U-Turn Sampler (NUTS).\n\n\n\n\n9.2.5 1.5 Example — Logistic Regression with HMC (Stan)\n\nlibrary(brms)\nset.seed(8)\n\n# Simulated logistic data\nN &lt;- 200\nx &lt;- rnorm(N)\ny &lt;- rbinom(N, 1, plogis(-1 + 2*x))\ndat &lt;- data.frame(x, y)\n\n# Fit via Hamiltonian Monte Carlo (NUTS)\nfit_hmc &lt;- brm(y ~ x, data=dat, family=bernoulli(), chains=2, iter=2000, refresh=0)\nsummary(fit_hmc)\nplot(fit_hmc)\n\nStan’s NUTS algorithm performs automatic adaptation of step size and trajectory length.\n\n\n\n9.2.6 1.6 Diagnosing HMC Performance\nKey diagnostics: - Divergent transitions → numerical instability (reduce step size or re-scale parameters).\n- Energy Bayesian Fraction of Missing Information (E-BFMI) → low values (\\(&lt;0.3\\)) indicate poor exploration.\n- \\(\\widehat{R}\\) and effective sample size → check convergence and mixing.\n\nlibrary(bayesplot)\nmcmc_nuts_divergence(fit_hmc)\nmcmc_trace(fit_hmc, pars=c(\"b_Intercept\",\"b_x\"))\n\n\n\n\n9.2.7 1.7 Advantages of HMC\n\n\n\n\n\n\n\nFeature\nBenefit\n\n\n\n\nGradient-based proposals\nRapid movement through high-density regions\n\n\nHigher acceptance rates\nFewer rejections than random-walk MH\n\n\nFewer tuning parameters\nAutomatic adaptation (NUTS)\n\n\nRobust for high-dimensional models\nUsed in most modern Bayesian software",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8 — Advanced Bayesian Computation</span>"
    ]
  },
  {
    "objectID": "week08.html#lecture-2-variational-inference-vi",
    "href": "week08.html#lecture-2-variational-inference-vi",
    "title": "9  Week 8 — Advanced Bayesian Computation",
    "section": "9.3 Lecture 2 — Variational Inference (VI)",
    "text": "9.3 Lecture 2 — Variational Inference (VI)\n\n9.3.1 2.1 Motivation\nWhen exact sampling is too costly (e.g., massive datasets, deep models),\nVariational Inference (VI) approximates the posterior by a simpler distribution \\(q\\_\\lambda(\\theta)\\) within a parameterized family.\n\n\n\n9.3.2 2.2 Objective Function\nWe minimize the Kullback–Leibler (KL) divergence: \\[\n\\text{KL}(q_\\lambda(\\theta) \\,\\|\\, p(\\theta\\mid y))\n  = \\int q_\\lambda(\\theta)\\log\\frac{q_\\lambda(\\theta)}{p(\\theta\\mid y)}\\,d\\theta.\n\\]\nEquivalently, we maximize the Evidence Lower Bound (ELBO): \\[\n\\text{ELBO}(\\lambda)\n= E_{q_\\lambda}[\\log p(y,\\theta)] - E_{q_\\lambda}[\\log q_\\lambda(\\theta)].\n\\] The higher the ELBO, the closer \\(q\\_\\lambda(\\theta)\\) is to the true posterior.\n\n\n\n9.3.3 2.3 Mean-Field Approximation\nA common simplification assumes factorization: \\[\nq_\\lambda(\\theta) = \\prod_j q_{\\lambda_j}(\\theta_j),\n\\] which allows coordinate-wise optimization of each factor.\n\n\n\n9.3.4 2.4 Example — Variational Bayes for a Normal Mean\nAssume \\(y_i\\sim N(\\theta,1)\\) with prior \\(\\theta\\sim N(0,1)\\).\nAnalytically, the posterior is \\(N\\left(\\frac{n\\bar{y}}{n+1}, \\frac{1}{n+1}\\right)\\).\nWe approximate it variationally by another normal \\(q(\\theta)=N(m,s^2)\\),\nand find \\(m,s^2\\) maximizing ELBO.\n\nset.seed(9)\ny &lt;- rnorm(50, mean=1)\nn &lt;- length(y)\nlog_joint &lt;- function(theta) sum(dnorm(y, theta, 1, log=TRUE)) + dnorm(theta, 0, 1, log=TRUE)\n\n# Closed-form optimal q is Normal(m,s^2) with same moments as true posterior:\nm_vi &lt;- n*mean(y)/(n+1)\ns2_vi &lt;- 1/(n+1)\nc(mean=m_vi, sd=sqrt(s2_vi))\n\n     mean        sd \n0.8884051 0.1400280 \n\n\n\n\n\n9.3.5 2.5 Automatic VI with brms\n\nlibrary(brms)\nset.seed(10)\nN &lt;- 1000\nx &lt;- rnorm(N)\ny &lt;- 2 + 1.5*x + rnorm(N)\ndat &lt;- data.frame(x,y)\n\nfit_vi &lt;- brm(y ~ x, data=dat, family=gaussian(),\n              algorithm=\"meanfield\", iter=5000, refresh=0)\nsummary(fit_vi)\n\nVI provides a fast deterministic approximation, trading off accuracy for scalability.\n\n\n\n9.3.6 2.6 Comparison: HMC vs VI\n\n\n\n\n\n\n\n\nFeature\nHMC (NUTS)\nVariational Inference\n\n\n\n\nType\nSampling (asymptotically exact)\nOptimization (approximate)\n\n\nAccuracy\nVery high\nDepends on variational family\n\n\nSpeed\nSlower\nVery fast\n\n\nDiagnostics\nConvergence via \\(\\widehat{R}\\), ESS\nELBO convergence\n\n\nUse case\nComplex or small data\nMassive or real-time problems\n\n\n\n\n\n\n9.3.7 2.7 Visual Comparison (Conceptual)\n\ntheta &lt;- seq(-3,3,length=400)\nposterior &lt;- dnorm(theta, 0, 1)              # true posterior\nvi_approx &lt;- dnorm(theta, 0, 1.5)            # wider variational approx\nplot(theta, posterior, type=\"l\", lwd=2, col=\"black\", ylim=c(0,0.5),\n     main=\"Posterior (HMC) vs Variational Approximation\",\n     ylab=\"Density\", xlab=expression(theta))\nlines(theta, vi_approx, col=\"orange\", lwd=2, lty=2)\nlegend(\"topright\", legend=c(\"Exact posterior (HMC)\",\"VI approximation\"),\n       col=c(\"black\",\"orange\"), lwd=2, lty=c(1,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n9.3.8 2.8 Practical Advice\n\nUse HMC (NUTS) as the default for accuracy and diagnostics.\n\nUse VI for large-scale models, initialization, or quick exploration.\n\nCompare results: if VI and HMC differ substantially, favor HMC.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8 — Advanced Bayesian Computation</span>"
    ]
  },
  {
    "objectID": "week08.html#homework-8",
    "href": "week08.html#homework-8",
    "title": "9  Week 8 — Advanced Bayesian Computation",
    "section": "9.4 Homework 8",
    "text": "9.4 Homework 8\n\nConceptual\n\nExplain the difference between sampling-based and optimization-based inference.\n\nWhat role does the ELBO play in VI?\n\nComputational\n\nFit a simple linear regression using both HMC and VI in brms.\n\nCompare posterior means, standard deviations, and computation time.\n\nReflection\n\nIn what types of real-world problems might VI be preferred over HMC?\n\nHow would you check whether your VI approximation is adequate?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8 — Advanced Bayesian Computation</span>"
    ]
  },
  {
    "objectID": "week08.html#key-takeaways",
    "href": "week08.html#key-takeaways",
    "title": "9  Week 8 — Advanced Bayesian Computation",
    "section": "9.5 Key Takeaways",
    "text": "9.5 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nHamiltonian Monte Carlo\nUses gradients to propose efficient moves through parameter space.\n\n\nNo-U-Turn Sampler (NUTS)\nAdapts step size and trajectory automatically.\n\n\nVariational Inference\nOptimizes a tractable approximation to the posterior.\n\n\nELBO\nObjective function for VI; measures closeness to the true posterior.\n\n\nTrade-off\nHMC = accuracy, VI = speed; choice depends on model and data size.\n\n\n\n\nNext Week: Bayesian Model Averaging and Ensemble Learning — combining multiple Bayesian models for improved predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8 — Advanced Bayesian Computation</span>"
    ]
  },
  {
    "objectID": "week09.html",
    "href": "week09.html",
    "title": "10  Week 9 — Bayesian Model Averaging and Ensemble Learning",
    "section": "",
    "text": "10.1 Learning Goals\nThis week introduces Bayesian Model Averaging (BMA), a principled framework to combine inferences from multiple Bayesian models, and contrasts it with ensemble methods common in machine learning.\nWe discuss model uncertainty, predictive averaging, and practical implementations for linear regression and classification.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9 — Bayesian Model Averaging and Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "week09.html#learning-goals",
    "href": "week09.html#learning-goals",
    "title": "10  Week 9 — Bayesian Model Averaging and Ensemble Learning",
    "section": "",
    "text": "Explain the motivation for Bayesian Model Averaging.\n\nDerive model-averaged predictions using posterior model probabilities.\n\nCompare BMA with frequentist model selection and ML ensembles.\n\nImplement BMA for simple regression models in R.\n\nDiscuss advantages and limitations of Bayesian model combination.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9 — Bayesian Model Averaging and Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "week09.html#lecture-1-bayesian-model-averaging-bma",
    "href": "week09.html#lecture-1-bayesian-model-averaging-bma",
    "title": "10  Week 9 — Bayesian Model Averaging and Ensemble Learning",
    "section": "10.2 Lecture 1 — Bayesian Model Averaging (BMA)",
    "text": "10.2 Lecture 1 — Bayesian Model Averaging (BMA)\n\n10.2.1 1.1 Model Uncertainty\nModel selection often ignores uncertainty about which model is true.\nBMA accounts for this by averaging over all candidate models weighted by their posterior probabilities.\nFor models \\(M_1, \\ldots, M_K\\): \\[\np(M_k \\mid y) = \\frac{p(y \\mid M_k)\\,p(M_k)}{\\sum_{j=1}^K p(y \\mid M_j)\\,p(M_j)}.\n\\]\nHere: - \\(p(y \\\\mid M_k)\\) = marginal likelihood under model \\(M_k\\).\n- \\(p(M_k)\\) = prior model probability.\n- \\(p(M_k \\\\mid y)\\) = posterior model probability.\n\n\n\n10.2.2 1.2 Model-Averaged Posterior and Predictions\nPosterior distribution for parameter θ:\n\\[\np(\\theta \\mid y) = \\sum_{k=1}^K p(\\theta \\mid y, M_k)\\,p(M_k \\mid y).\n\\]\nPosterior predictive distribution:\n\\[\np(\\tilde{y} \\mid y) = \\sum_{k=1}^K p(\\tilde{y} \\mid y, M_k)\\,p(M_k \\mid y).\n\\]\nBMA integrates out model uncertainty rather than conditioning on a single “best” model.\n\n\n\n10.2.3 1.3 Comparison with Model Selection\n\n\n\n\n\n\n\n\nApproach\nKey Idea\nLimitation\n\n\n\n\nModel Selection\nChoose one best model (e.g., by AIC, WAIC, LOO)\nIgnores model uncertainty\n\n\nModel Averaging\nCombine all models weighted by posterior probability\nComputationally heavier, prior sensitive\n\n\n\n\n\n\n10.2.4 1.4 Example — Two Competing Linear Models\n\nset.seed(9)\nn &lt;- 100\nx &lt;- rnorm(n)\ny &lt;- 1 + 2*x + 0.5*x^2 + rnorm(n, sd=1)\n\nm1 &lt;- lm(y ~ x)\nm2 &lt;- lm(y ~ x + I(x^2))\n\nlog_marglik1 &lt;- -AIC(m1)/2\nlog_marglik2 &lt;- -AIC(m2)/2\np_m1 &lt;- exp(log_marglik1)\np_m2 &lt;- exp(log_marglik2)\n\nw1 &lt;- p_m1 / (p_m1 + p_m2)\nw2 &lt;- p_m2 / (p_m1 + p_m2)\n\npred1 &lt;- predict(m1)\npred2 &lt;- predict(m2)\nbma_pred &lt;- w1*pred1 + w2*pred2\n\nc(weights=c(M1=w1, M2=w2)[1:2])\n\n  weights.M1   weights.M2 \n1.426496e-08 1.000000e+00 \n\n\n\nplot(x, y, pch=19, col=\"#00000055\", main=\"Bayesian Model Averaging (Linear vs Quadratic)\",\n     xlab=\"x\", ylab=\"y\")\nxs &lt;- seq(min(x), max(x), length.out=200)\nlines(xs, predict(m1, newdata=data.frame(x=xs)), col=\"steelblue\", lwd=2)\nlines(xs, predict(m2, newdata=data.frame(x=xs)), col=\"firebrick\", lwd=2)\nlines(xs, w1*predict(m1, newdata=data.frame(x=xs)) +\n          w2*predict(m2, newdata=data.frame(x=xs)),\n      col=\"darkgreen\", lwd=3, lty=2)\nlegend(\"topleft\", legend=c(\"Model 1 (linear)\",\"Model 2 (quadratic)\",\"BMA prediction\"),\n       col=c(\"steelblue\",\"firebrick\",\"darkgreen\"), lwd=c(2,2,3), lty=c(1,1,2), bty=\"n\")\n\n\n\n\nModel-averaged predictions vs. data\n\n\n\n\nInterpretation: The model-averaged prediction blends the strengths of both models, weighted by their posterior support.\n\n\n\n10.2.5 1.5 Advantages of BMA\n\nIncorporates model uncertainty directly.\n\nAvoids overconfidence from single-model conditioning.\n\nImproves predictive performance, especially in small samples.\n\nProvides model weights interpretable as probabilities.\n\n\n\n\n10.2.6 1.6 Limitations\n\nRequires marginal likelihoods (often hard to compute).\n\nSensitive to model priors and parameter priors.\n\nComputationally expensive for many models.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9 — Bayesian Model Averaging and Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "week09.html#lecture-2-bayesian-ensembles-and-predictive-stacking",
    "href": "week09.html#lecture-2-bayesian-ensembles-and-predictive-stacking",
    "title": "10  Week 9 — Bayesian Model Averaging and Ensemble Learning",
    "section": "10.3 Lecture 2 — Bayesian Ensembles and Predictive Stacking",
    "text": "10.3 Lecture 2 — Bayesian Ensembles and Predictive Stacking\n\n10.3.1 2.1 Beyond BMA: Ensemble Learning\nMachine learning often uses ensembles (e.g., bagging, boosting, stacking) to improve prediction.\nBayesian analogues combine predictive distributions rather than point estimates.\n\n\n\n10.3.2 2.2 Predictive Stacking\nRather than using posterior model probabilities, stacking optimizes weights to maximize predictive performance under cross-validation: \\[\nw^* = \\arg\\max_{w} \\sum_{i=1}^n \\log\\left(\\sum_k w_k\\, p(y_i \\mid y_{-i}, M_k)\\right),\n\\] subject to \\(w_k \\\\ge 0\\) and \\(\\\\sum_k w_k = 1\\).\nThis yields stacking weights that combine models for best out-of-sample prediction.\n\n\n\n10.3.3 2.3 Example — Predictive Stacking with loo\n\nlibrary(brms)\nlibrary(loo)\n\nset.seed(10)\ndat &lt;- data.frame(x = rnorm(200))\ndat$y &lt;- 1 + 2*dat$x + 0.5*dat$x^2 + rnorm(200)\n\nm1 &lt;- brm(y ~ x, data=dat, refresh=0)\nm2 &lt;- brm(y ~ x + I(x^2), data=dat, refresh=0)\n\nloo1 &lt;- loo(m1)\nloo2 &lt;- loo(m2)\n\n# Stacking weights based on LOO predictive densities\nw_stack &lt;- loo_model_weights(list(m1,m2), method=\"stacking\")\nw_pseudo &lt;- loo_model_weights(list(m1,m2), method=\"pseudobma\")\n\nw_stack\nw_pseudo\n\nInterpretation:\n- Stacking weights directly optimize predictive log-likelihood.\n- Pseudo-BMA provides a simpler (WAIC/LOO-based) approximation.\n\n\n\n10.3.4 2.4 Comparison: BMA vs Stacking\n\n\n\n\n\n\n\n\nFeature\nBayesian Model Averaging\nPredictive Stacking\n\n\n\n\nWeights\nPosterior model probabilities\nOptimized predictive weights\n\n\nGoal\nRepresent model uncertainty\nMaximize predictive performance\n\n\nComputation\nNeeds marginal likelihoods\nUses cross-validation\n\n\nPrior dependence\nSensitive\nWeak or none\n\n\nTypical use\nTheoretical coherence\nPractical prediction\n\n\n\n\n\n\n10.3.5 2.5 Ensemble Prediction Example\n\nset.seed(11)\nn &lt;- 100\nx &lt;- rnorm(n)\ny_true &lt;- 2 + 3*x - 1.5*x^2\ny &lt;- y_true + rnorm(n, sd=2)\n\nm1 &lt;- lm(y ~ x)\nm2 &lt;- lm(y ~ poly(x, 2, raw=TRUE))\n\npred_grid &lt;- seq(min(x), max(x), length=200)\np1 &lt;- predict(m1, newdata=data.frame(x=pred_grid))\np2 &lt;- predict(m2, newdata=data.frame(x=pred_grid))\n\n# Ensemble weighting (ad hoc stacking weights)\nw1 &lt;- 0.3; w2 &lt;- 0.7\np_ens &lt;- w1*p1 + w2*p2\n\nplot(x, y, pch=19, col=\"#00000055\", main=\"Model Ensemble Prediction\",\n     xlab=\"x\", ylab=\"y\")\nlines(pred_grid, p1, col=\"blue\", lwd=2)\nlines(pred_grid, p2, col=\"red\", lwd=2)\nlines(pred_grid, p_ens, col=\"darkgreen\", lwd=3, lty=2)\nlegend(\"topleft\", legend=c(\"Model 1\",\"Model 2\",\"Ensemble\"),\n       col=c(\"blue\",\"red\",\"darkgreen\"), lwd=c(2,2,3), lty=c(1,1,2), bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n10.3.6 2.6 Practical Guidance\n\nUse BMA when posterior model probabilities are available (few models, interpretable priors).\n\nUse stacking or ensemble averaging when prediction accuracy is the goal.\n\nAvoid double counting data — always base weights on held-out or cross-validation predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9 — Bayesian Model Averaging and Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "week09.html#homework-9",
    "href": "week09.html#homework-9",
    "title": "10  Week 9 — Bayesian Model Averaging and Ensemble Learning",
    "section": "10.4 Homework 9",
    "text": "10.4 Homework 9\n\nConceptual\n\nExplain how BMA differs from model selection.\n\nWhy does stacking avoid prior sensitivity found in BMA?\n\nComputational\n\nSimulate data where two Bayesian regression models compete.\n\nFit both models in R (e.g., using brms or lm).\n\nCompute stacking and pseudo-BMA weights using loo_model_weights().\n\nCompare model-averaged predictions to the true curve.\n\nReflection\n\nDiscuss when BMA and stacking might give very different results.\n\nHow can model averaging improve scientific interpretability?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9 — Bayesian Model Averaging and Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "week09.html#key-takeaways",
    "href": "week09.html#key-takeaways",
    "title": "10  Week 9 — Bayesian Model Averaging and Ensemble Learning",
    "section": "10.5 Key Takeaways",
    "text": "10.5 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nBayesian Model Averaging\nCombines models weighted by posterior probabilities.\n\n\nPredictive Stacking\nChooses weights that maximize predictive accuracy via cross-validation.\n\n\nModel Uncertainty\nAccounted for rather than ignored.\n\n\nPractical Use\nBMA for interpretability; stacking for prediction.\n\n\nModern Tools\nloo_model_weights() in R provides both stacking and pseudo-BMA weights.\n\n\n\n\nNext Week: Bayesian Nonparametrics — infinite-dimensional models such as Dirichlet processes and Gaussian processes for flexible Bayesian modeling.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9 — Bayesian Model Averaging and Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "week10.html",
    "href": "week10.html",
    "title": "11  Week 10 — Bayesian Nonparametrics",
    "section": "",
    "text": "11.1 Learning Goals\nThis week introduces Bayesian Nonparametric (BNP) models, which allow infinite flexibility in representing uncertainty about functions, densities, or model structure.\nWe focus on two cornerstone approaches: Dirichlet Processes for mixture modeling and clustering, and Gaussian Processes for regression on functions.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10 — Bayesian Nonparametrics</span>"
    ]
  },
  {
    "objectID": "week10.html#learning-goals",
    "href": "week10.html#learning-goals",
    "title": "11  Week 10 — Bayesian Nonparametrics",
    "section": "",
    "text": "Explain the motivation for nonparametric Bayesian models.\n\nDescribe the Dirichlet Process and its stick-breaking and Chinese Restaurant representations.\n\nExplain how DP mixture models perform clustering automatically.\n\nUnderstand Gaussian Processes for function estimation.\n\nImplement simple DP and GP examples in R using available packages.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10 — Bayesian Nonparametrics</span>"
    ]
  },
  {
    "objectID": "week10.html#lecture-1-dirichlet-process-models",
    "href": "week10.html#lecture-1-dirichlet-process-models",
    "title": "11  Week 10 — Bayesian Nonparametrics",
    "section": "11.2 Lecture 1 — Dirichlet Process Models",
    "text": "11.2 Lecture 1 — Dirichlet Process Models\n\n11.2.1 1.1 Motivation\nClassical parametric models fix the number of parameters (e.g., number of clusters).\nDirichlet Processes (DPs) let the data decide model complexity by placing a prior on infinite mixture components.\n\n\n\n11.2.2 1.2 The Dirichlet Process\nA Dirichlet Process \\(\\text{DP}(\\alpha, G_0)\\) is a distribution over distributions such that,\nfor any partition \\(A_1, \\ldots, A_k\\) of the space, \\[\n(G(A_1),\\ldots,G(A_k)) \\sim \\text{Dirichlet}(\\alpha G_0(A_1), \\ldots, \\alpha G_0(A_k)).\n\\]\n\n\\(G_0\\): base (prior mean) distribution.\n\n\\(\\alpha\\): concentration parameter controlling clustering strength.\n\nAs \\(\\alpha \\to 0\\): few clusters (more sharing).\nAs \\(\\alpha \\to \\infty\\): many clusters (approaches \\(G_0\\)).\n\n\n\n11.2.3 1.3 Stick-Breaking Representation\nA constructive definition (Sethuraman, 1994): \\[\nG = \\sum_{k=1}^{\\infty} \\pi_k \\delta_{\\theta_k}, \\quad\n\\theta_k \\sim G_0, \\quad\n\\pi_k = v_k \\prod_{l&lt;k}(1-v_l), \\quad\nv_k \\sim \\text{Beta}(1,\\alpha).\n\\]\nThe weights \\(\\pi_k\\) form an infinite sequence summing to 1 — conceptually “breaking a stick” into random lengths.\n\n\n\n11.2.4 1.4 DP Mixture Model\nFor data \\(y_1,\\ldots,y_n\\): \\[\ny_i \\mid \\theta_i \\sim F(\\theta_i), \\qquad\n\\theta_i \\mid G \\sim G, \\qquad\nG \\sim \\text{DP}(\\alpha, G_0).\n\\]\nMarginally, this induces clustering because multiple \\(y_i\\) can share the same \\(\\theta_i\\).\n\n\n\n11.2.5 1.5 Chinese Restaurant Process (CRP)\nEquivalent generative process:\n\nCustomer 1 starts a new table.\n\nCustomer \\(i\\) joins an existing table \\(k\\) with probability\n\\(n_k / (\\alpha + i - 1)\\),\nor starts a new one with probability \\(\\alpha / (\\alpha + i - 1)\\).\n\nThis describes how clusters grow adaptively as data arrive.\n\n\n\n11.2.6 1.6 Example — Simulated DP Mixture of Normals\n\nset.seed(10)\nlibrary(dirichletprocess)\n\n# Generate mixture data\ny &lt;- c(rnorm(50, -3, 0.5), rnorm(50, 3, 0.5))\n\n# Fit a Dirichlet Process Gaussian Mixture\ndp &lt;- DirichletProcessGaussian(y)\ndp &lt;- Fit(dp, its = 2000)\n\n# Cluster assignments\nplot(dp) + ggplot2::ggtitle(\"Dirichlet Process Gaussian Mixture\")\n\nWarning: `aes_()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`\nℹ The deprecated feature was likely used in the dirichletprocess package.\n  Please report the issue at\n  &lt;https://github.com/dm13450/dirichletprocess/issues&gt;.\n\n\n\n\n\n\n\n\n\nInterpretation: the DP mixture automatically discovers clusters without specifying their number in advance.\n\n\n\n11.2.7 1.7 Practical Notes\n\nThe posterior number of clusters depends on \\(\\alpha\\) and data separation.\n\nInference via Gibbs sampling or variational truncation.\n\nExtensions: Hierarchical DP, DP regression, and DP topic models.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10 — Bayesian Nonparametrics</span>"
    ]
  },
  {
    "objectID": "week10.html#lecture-2-gaussian-processes-for-regression",
    "href": "week10.html#lecture-2-gaussian-processes-for-regression",
    "title": "11  Week 10 — Bayesian Nonparametrics",
    "section": "11.3 Lecture 2 — Gaussian Processes for Regression",
    "text": "11.3 Lecture 2 — Gaussian Processes for Regression\n\n11.3.1 2.1 Motivation\nA Gaussian Process (GP) defines a prior directly over functions, enabling flexible nonlinear regression without specifying a parametric form.\n\n\n\n11.3.2 2.2 Definition\nA GP is a collection of random variables \\(f(x)\\) such that every finite subset has a joint multivariate normal distribution: \\[\nf(x) \\sim \\text{GP}(m(x), k(x,x')),\n\\] where\n\n\\(m(x) = E[f(x)]\\): mean function,\n\\(k(x,x') = \\text{Cov}(f(x),f(x'))\\): covariance (kernel) function.\n\n\n\n\n11.3.3 GP Regression Model\nFor data \\((x_i, y_i)\\): \\[\ny_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2).\n\\] Posterior of \\(f(x)\\) given \\(y\\): \\[\nf_* \\mid y, X, X_* \\sim \\mathcal{N}(\\bar{f}_*, \\text{Cov}(f_*)),\n\\] where the mean and covariance are computed using kernel matrices.\n\n\n\n11.3.4 2.4 Common Kernels\n\n\n\n\n\n\n\n\nKernel\nFormula\nProperty\n\n\n\n\nSquared Exponential\n\\(k(x,x') = \\tau^2 \\exp\\!\\left(-\\frac{(x-x')^2}{2\\ell^2}\\right)\\)\nSmooth, infinitely differentiable\n\n\nMatérn\n\\(k(x,x') = \\tau\\^2 \\frac{2\\^{1-\\nu}}{\\Gamma(\\nu)}(\\sqrt{2\\nu})\\)\nx-x’\n\n\nPeriodic\n\\(k(x,x') = \\tau^2\\exp(-2\\sin^2(\\pi))\\)\nx-x’\n\n\n\n\n\n\n11.3.5 2.5 Example — Gaussian Process Regression in R\n\nlibrary(GPfit)\nset.seed(11)\n\nx &lt;- seq(-3, 3, length.out = 50)\ny &lt;- sin(x) + rnorm(50, sd = 0.2)\n\n# Make X a column matrix (good practice for GPfit)\ngp_model &lt;- GP_fit(X = matrix(x, ncol = 1), Y = y)\n\nWarning in GP_fit(X = matrix(x, ncol = 1), Y = y): X should be in range (0, 1)\n\nplot(gp_model)\n\n\n\n\n\n\n\n\n\npred_grid &lt;- seq(-3, 3, length.out = 200)\npred &lt;- predict(gp_model, xnew = matrix(pred_grid, ncol = 1))\n\nyhat &lt;- as.numeric(pred$Y_hat)\nse   &lt;- sqrt(as.numeric(pred$MSE))  # MSE is a variance vector; no diag()\n\nplot(x, y, pch = 19, col = \"#00000055\",\n     main = \"Gaussian Process Regression\", xlab = \"x\", ylab = \"y\")\nlines(pred_grid, yhat, lwd = 2, col = \"darkorange\")\nlines(pred_grid, yhat + 2*se, lty = 2, col = \"gray40\")\nlines(pred_grid, yhat - 2*se, lty = 2, col = \"gray40\")\n\n\n\n\nGaussian Process Regression Fit\n\n\n\n\nInterpretation: the GP posterior mean tracks the true function smoothly, with uncertainty quantified by the shaded region.\n\n\n\n11.3.6 2.6 GP vs DP: Comparison\n\n\n\n\n\n\n\n\nAspect\nDirichlet Process\nGaussian Process\n\n\n\n\nDomain\nDistributions / Clusters\nFunctions\n\n\nOutput\nDiscrete clustering\nContinuous regression\n\n\nFlexibility\nUnknown number of components\nInfinite function space\n\n\nTypical Use\nDensity estimation, mixture modeling\nNonlinear regression, spatial data\n\n\n\n\n\n\n11.3.7 2.7 Practical Considerations\n\nGP computational cost is \\(O(n^3)\\); use sparse or inducing-point approximations for large \\(n\\).\n\nChoice of kernel determines function smoothness and inductive bias.\n\nIn practice, hyperparameters (e.g., \\(\\ell,~\\tau\\)) are learned via marginal likelihood maximization.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10 — Bayesian Nonparametrics</span>"
    ]
  },
  {
    "objectID": "week10.html#homework-10",
    "href": "week10.html#homework-10",
    "title": "11  Week 10 — Bayesian Nonparametrics",
    "section": "11.4 Homework 10",
    "text": "11.4 Homework 10\n\nConceptual\n\nCompare the roles of \\(\\alpha\\) in DP and the kernel parameters in GP.\n\nExplain the difference between parametric and nonparametric Bayesian models.\n\nComputational\n\nSimulate data from two Gaussian clusters and fit a DP mixture model using dirichletprocess.\n\nFit a GP regression to noisy sinusoidal data using GPfit.\n\nPlot both model fits and discuss flexibility.\n\nReflection\n\nWhen might nonparametric models be overkill?\n\nHow could hierarchical extensions of DP or GP handle grouped data?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10 — Bayesian Nonparametrics</span>"
    ]
  },
  {
    "objectID": "week10.html#key-takeaways",
    "href": "week10.html#key-takeaways",
    "title": "11  Week 10 — Bayesian Nonparametrics",
    "section": "11.5 Key Takeaways",
    "text": "11.5 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nDirichlet Process\nPrior over distributions enabling infinite mixture models.\n\n\nStick-Breaking Construction\nRepresents DP as weighted infinite discrete components.\n\n\nChinese Restaurant Process\nIntuitive clustering interpretation of DP.\n\n\nGaussian Process\nDefines prior over functions for regression and smoothing.\n\n\nCommon Feature\nBoth model complexity grows with data — no fixed parameter dimension.\n\n\n\n\nNext Week: Bayesian Time Series and State-Space Models — dynamic modeling and sequential inference using Bayesian methods.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10 — Bayesian Nonparametrics</span>"
    ]
  },
  {
    "objectID": "week11.html",
    "href": "week11.html",
    "title": "12  Week 11 — Bayesian Time Series and State-Space Models",
    "section": "",
    "text": "12.1 Learning Goals\nThis week introduces Bayesian approaches to time series analysis and state-space modeling, which unify filtering, forecasting, and dynamic parameter estimation under a probabilistic framework.\nWe study both classical dynamic linear models (DLMs) and modern Bayesian filtering methods.\nBy the end of this week, you should be able to:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 11 — Bayesian Time Series and State-Space Models</span>"
    ]
  },
  {
    "objectID": "week11.html#learning-goals",
    "href": "week11.html#learning-goals",
    "title": "12  Week 11 — Bayesian Time Series and State-Space Models",
    "section": "",
    "text": "Formulate Bayesian dynamic models for time series data.\n\nUnderstand latent (state) variable representations.\n\nApply Bayesian updating and filtering for sequential data.\n\nImplement simple state-space and autoregressive models in R.\n\nInterpret uncertainty propagation over time.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 11 — Bayesian Time Series and State-Space Models</span>"
    ]
  },
  {
    "objectID": "week11.html#lecture-1-dynamic-linear-models-dlms",
    "href": "week11.html#lecture-1-dynamic-linear-models-dlms",
    "title": "12  Week 11 — Bayesian Time Series and State-Space Models",
    "section": "12.2 Lecture 1 — Dynamic Linear Models (DLMs)",
    "text": "12.2 Lecture 1 — Dynamic Linear Models (DLMs)\n\n12.2.1 1.1 Motivation\nTime series exhibit temporal dependence.\nDynamic models describe how latent states evolve over time and how observations depend on those states: \\[\n\\text{State equation: } \\theta_t = G_t \\theta_{t-1} + \\omega_t, \\quad \\omega_t \\sim N(0,W_t),\n\\] \\[\n\\text{Observation equation: } y_t = F_t^\\top \\theta_t + \\nu_t, \\quad \\nu_t \\sim N(0,V_t).\n\\]\nHere,\n- \\[ \\\\theta_t \\]: latent state vector,\n- \\[ y_t \\]: observed data,\n- \\[ G_t, F_t \\]: known system matrices,\n- \\[ W_t, V_t \\]: process and observation noise covariances.\n\n\n\n12.2.2 1.2 Bayesian Updating\nGiven data up to time \\[t-1\\], the prior for \\[\\\\theta_t\\] is: \\[\np(\\theta_t \\mid y_{1:t-1}) = N(a_t, R_t),\n\\] where\n\\[ a_t = G_t m\\_{t-1} \\],\n\\[ R_t = G_t C\\_{t-1} G_t\\^\\\\top + W_t \\].\nAfter observing \\[ y_t \\]: \\[\np(\\theta_t \\mid y_{1:t}) = N(m_t, C_t),\n\\] where\n\\[ m_t = a_t + A_t (y_t - F_t\\^\\\\top a_t) \\],\n\\[ C_t = R_t - A_t F_t\\^\\\\top R_t \\],\nand \\[ A_t = R_t F_t (F_t\\^\\\\top R_t F_t + V_t)\\^{-1} \\] is the Kalman gain.\n\n\n\n12.2.3 1.3 Example — Local Level Model\nSimplest DLM: \\[\ny_t = \\theta_t + \\nu_t, \\quad \\theta_t = \\theta_{t-1} + \\omega_t,\n\\] with \\[ \\\\nu_t \\\\sim N(0,V) \\], \\[ \\\\omega_t \\\\sim N(0,W) \\].\n\nset.seed(11)\nn &lt;- 100\ntheta &lt;- numeric(n); y &lt;- numeric(n)\ntheta[1] &lt;- 0\nfor (t in 2:n) theta[t] &lt;- theta[t-1] + rnorm(1, 0, 0.2)\ny &lt;- theta + rnorm(n, 0, 0.5)\nplot.ts(cbind(y, theta), col=c(\"black\",\"blue\"), lwd=2,\n        main=\"Local Level Model: True State vs Observed y\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"True θ\"), col=c(\"black\",\"blue\"), lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\n\n\n\n12.2.4 1.4 Filtering with the Kalman Algorithm\nWe estimate the evolving state mean \\[ m_t \\] recursively:\n\nm &lt;- numeric(n); C &lt;- numeric(n)\nm[1] &lt;- 0; C[1] &lt;- 1\nV &lt;- 0.5^2; W &lt;- 0.2^2\n\nfor (t in 2:n) {\n  a &lt;- m[t-1]\n  R &lt;- C[t-1] + W\n  A &lt;- R / (R + V)\n  m[t] &lt;- a + A * (y[t] - a)\n  C[t] &lt;- (1 - A) * R\n}\n\nplot.ts(cbind(y, m), col=c(\"black\",\"red\"), lwd=2,\n        main=\"Kalman Filter Estimate of Latent State\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"Filtered mean m_t\"),\n       col=c(\"black\",\"red\"), lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\nThe red line tracks the smoothed latent process inferred from noisy data.\n\n\n\n12.2.5 1.5 Forecasting and Uncertainty\nPredictive distribution for the next observation: \\[\ny_{t+1} \\mid y_{1:t} \\sim N(F_{t+1}^\\top a_{t+1}, F_{t+1}^\\top R_{t+1} F_{t+1} + V_{t+1}).\n\\]\nForecast variance increases as the state uncertainty grows over time.\n\n\n\n12.2.6 1.6 Advantages of Bayesian DLMs\n\nNaturally handle missing observations.\n\nFlexible hierarchical extensions (time-varying parameters).\n\nProbabilistic forecasting with credible intervals.\n\nOnline updating suitable for real-time applications.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 11 — Bayesian Time Series and State-Space Models</span>"
    ]
  },
  {
    "objectID": "week11.html#lecture-2-state-space-models-and-bayesian-filtering",
    "href": "week11.html#lecture-2-state-space-models-and-bayesian-filtering",
    "title": "12  Week 11 — Bayesian Time Series and State-Space Models",
    "section": "12.3 Lecture 2 — State-Space Models and Bayesian Filtering",
    "text": "12.3 Lecture 2 — State-Space Models and Bayesian Filtering\n\n12.3.1 2.1 General State-Space Models\nGeneral form: \\[\nx_t = f(x_{t-1}) + \\omega_t, \\qquad y_t = g(x_t) + \\nu_t,\n\\] where \\[ f \\] and \\[ g \\] may be nonlinear or non-Gaussian.\nExamples: stochastic volatility, epidemic dynamics, tracking models.\n\n\n\n12.3.2 2.2 Bayesian Filtering\nWe recursively compute: \\[\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t)\\int p(x_t \\mid x_{t-1})\\,p(x_{t-1}\\mid y_{1:t-1})\\,dx_{t-1}.\n\\]\nClosed forms exist only for linear-Gaussian models (Kalman).\nOtherwise, approximate methods are required: - Particle Filtering (Sequential Monte Carlo).\n- Extended Kalman Filter (linearization).\n- Unscented Kalman Filter (deterministic sampling).\n\n\n\n12.3.3 2.3 Particle Filtering (Sequential Monte Carlo)\nIdea: Represent \\[ p(x_t\\\\mid y\\_{1:t}) \\] by weighted particles \\[ \\\\{x_t\\^{(i)}, w_t\\^{(i)}\\\\} \\].\n\nPrediction: Sample \\[ x_t\\^{(i)} \\\\sim p(x_t\\\\mid x\\_{t-1}\\^{(i)}) \\]$.\n\nWeighting: \\[ w_t\\^{(i)} \\\\propto p(y_t\\\\mid x_t\\^{(i)}) \\].\n\nResampling: Normalize and resample particles based on \\[ w_t\\^{(i)} \\].\n\nAs \\[ N \\\\to \\\\infty \\], the empirical distribution approximates the true posterior.\n\n\n\n12.3.4 2.4 Example — Particle Filter for a Simple Nonlinear Model\n\nset.seed(12)\nn &lt;- 50; Np &lt;- 500\nx_true &lt;- numeric(n); y &lt;- numeric(n)\nx_true[1] &lt;- 0\nfor (t in 2:n) x_true[t] &lt;- 0.7*x_true[t-1] + rnorm(1,0,0.5)\ny &lt;- x_true^2/2 + rnorm(n,0,0.2)\n\n# Initialize particles\nx_pf &lt;- matrix(0, nrow=n, ncol=Np)\nw &lt;- matrix(1/Np, nrow=n, ncol=Np)\nx_pf[1,] &lt;- rnorm(Np, 0, 1)\n\nfor (t in 2:n) {\n  # Propagate\n  x_pf[t,] &lt;- 0.7*x_pf[t-1,] + rnorm(Np,0,0.5)\n  # Weight\n  w[t,] &lt;- dnorm(y[t], mean=x_pf[t,]^2/2, sd=0.2)\n  w[t,] &lt;- w[t,]/sum(w[t,])\n  # Resample\n  idx &lt;- sample(1:Np, Np, replace=TRUE, prob=w[t,])\n  x_pf[t,] &lt;- x_pf[t,idx]\n}\n\nx_est &lt;- colMeans(x_pf)\nplot.ts(cbind(y, x_true, x_est), col=c(\"black\",\"blue\",\"red\"), lwd=2,\n        main=\"Particle Filter Tracking\", ylab=\"\")\nlegend(\"topleft\", legend=c(\"Observed y\",\"True state\",\"PF estimate\"),\n       col=c(\"black\",\"blue\",\"red\"), lwd=2, bty=\"n\")\n\n\n\n\n\n\n\n\nThe particle filter captures nonlinear state dynamics unavailable to standard Kalman filtering.\n\n\n\n12.3.5 2.5 Extensions and Modern Bayesian State Models\n\nDynamic GLMs for count or binary data.\n\nStochastic Volatility Models in finance.\n\nDynamic Factor Models for multivariate time series.\n\nBayesian Structural Time Series (BSTS) — trend + seasonality decomposition.\n\n\nlibrary(bsts)\ndata(Seatbelts)\ny &lt;- Seatbelts[,\"drivers\"]\nss &lt;- AddLocalLevel(list(), y)\nbsts_fit &lt;- bsts(y ~ 1, state.specification = ss, niter = 2000)\nplot(bsts_fit)\n\n\n\n\n12.3.6 2.6 Practical Considerations\n\nChoose priors for \\[ V_t, W_t \\] that balance smoothness and responsiveness.\n\nUse HMC or SMC samplers for full Bayesian inference.\n\nCheck convergence and predictive calibration through one-step-ahead forecasts.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 11 — Bayesian Time Series and State-Space Models</span>"
    ]
  },
  {
    "objectID": "week11.html#homework-11",
    "href": "week11.html#homework-11",
    "title": "12  Week 11 — Bayesian Time Series and State-Space Models",
    "section": "12.4 Homework 11",
    "text": "12.4 Homework 11\n\nConceptual\n\nExplain the difference between filtering and smoothing in Bayesian time series analysis.\n\nWhen does the Kalman filter provide exact inference?\n\nComputational\n\nSimulate a local-level model and implement a Kalman filter in R.\n\nExtend it with time-varying variance or drift.\n\nCompare filtered estimates to true latent states.\n\nReflection\n\nHow does particle filtering generalize the Kalman filter?\n\nDiscuss a potential application of Bayesian state-space modeling in your field.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 11 — Bayesian Time Series and State-Space Models</span>"
    ]
  },
  {
    "objectID": "week11.html#key-takeaways",
    "href": "week11.html#key-takeaways",
    "title": "12  Week 11 — Bayesian Time Series and State-Space Models",
    "section": "12.5 Key Takeaways",
    "text": "12.5 Key Takeaways\n\n\n\n\n\n\n\nConcept\nSummary\n\n\n\n\nDynamic Linear Model (DLM)\nLinear-Gaussian state-space model solved by Kalman filter.\n\n\nKalman Filter\nSequential Bayesian updating for latent states.\n\n\nParticle Filter\nSimulation-based approach for nonlinear or non-Gaussian models.\n\n\nForecasting\nNaturally derived from predictive posterior distribution.\n\n\nExtensions\nBSTS, stochastic volatility, dynamic regression, multivariate models.\n\n\n\n\nNext Week: Hierarchical Bayesian Inference for Complex Systems — multi-level time series and dynamic hierarchical modeling.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 11 — Bayesian Time Series and State-Space Models</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html",
    "href": "App_A-intro-R.html",
    "title": "14  Appendix: Introduction to R",
    "section": "",
    "text": "14.1 R\nFor conducting analyses with data sets of hundreds to thousands of observations, calculating by hand is not feasible and you will need a statistical software. R is one of those. R can also be thought of as a high-level programming language. In fact, R is one of the top languages to be used by data analysts and data scientists. There are a lot of analysis packages in R that are currently developed and maintained by researchers around the world to deal with different data problems. Most importantly, R is free! In this section, we will learn how to use R to conduct basic statistical analyses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#ide",
    "href": "App_A-intro-R.html#ide",
    "title": "14  Appendix: Introduction to R",
    "section": "14.2 IDE",
    "text": "14.2 IDE\n\n14.2.1 Rstudio\nRStudio is an integrated development environment (IDE) designed specifically for working with the R programming language. It provides a user-friendly interface that includes a source editor, console, environment pane, and tools for plotting, debugging, version control, and package management. RStudio supports both R and Python and is widely used for data analysis, statistical modeling, and reproducible research. It also integrates seamlessly with tools like R Markdown, Shiny, and Quarto, making it popular among data scientists, statisticians, and educators.\n\n\n14.2.2 Visual Studio Code (VS Code)\nVS Code is a versatile code editor that supports multiple programming languages, including R. With the R extension for VS Code, users can write and execute R code, access R’s console, and utilize features like syntax highlighting, code completion, and debugging. While not as specialized as RStudio for R development, VS Code offers a lightweight alternative with extensive customization options and support for various programming tasks.\n\n\n14.2.3 Positron\nPositron IDE is the next-generation integrated development environment developed by Posit, the company behind RStudio. Designed to be a modern, extensible, and language-agnostic IDE, Positron builds on the strengths of RStudio while supporting a broader range of languages and workflows, including R, Python, and Quarto.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#rstudio-layout",
    "href": "App_A-intro-R.html#rstudio-layout",
    "title": "14  Appendix: Introduction to R",
    "section": "14.3 RStudio Layout",
    "text": "14.3 RStudio Layout\nRStudio consists of several panes: - Source: Where you write scripts and markdown documents. - Console: Where you type and execute R commands. - Environment/History: Shows your variables and command history. - Files/Plots/Packages/Help/Viewer: For file management, viewing plots, managing packages, accessing help, and viewing web content.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-scripts",
    "href": "App_A-intro-R.html#r-scripts",
    "title": "14  Appendix: Introduction to R",
    "section": "14.4 R Scripts",
    "text": "14.4 R Scripts\nR scripts are plain text files containing R code. You can create a new script in RStudio by clicking File &gt; New File &gt; R Script.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-help",
    "href": "App_A-intro-R.html#r-help",
    "title": "14  Appendix: Introduction to R",
    "section": "14.5 R Help",
    "text": "14.5 R Help\nUse ?function_name or help(function_name) to access help for any R function. For example:\n?mean\nhelp(mean)",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-packages",
    "href": "App_A-intro-R.html#r-packages",
    "title": "14  Appendix: Introduction to R",
    "section": "14.6 R Packages",
    "text": "14.6 R Packages\nPackages extend R’s functionality. There are thousands of packages available in R ecosystem. You may install them from different sources.\n\n14.6.1 With Comprehensive R Archive Network (CRAN)\nCRAN is the primary repository for R packages. It contains thousands of packages that can be easily installed and updated.\nInstall a package with:\ninstall.packages(\"package_name\")\n\n\n14.6.2 With Bioconductor\nBioconductor is a repository for bioinformatics packages in R. It provides tools for the analysis and comprehension of high-throughput genomic data.\nInstall Bioconductor packages using the BiocManager package:\nBiocManager::install(\"package_name\")\n\n\n14.6.3 From GitHub\nMany of the authors of R packages host their work on GitHub. You can install these packages using the devtools package:\ndevtools::install_github(\"username/package_name\")\n\n\n14.6.4 Load a package\nOnce a package is installed, you need to load it into your R session to use its functions:\nlibrary(package_name)\nAlternatively, you may use a function in the package with package_name::function_name() without loading the entire package.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-markdown",
    "href": "App_A-intro-R.html#r-markdown",
    "title": "14  Appendix: Introduction to R",
    "section": "14.7 R Markdown",
    "text": "14.7 R Markdown\nR Markdown allows you to combine text, code, and output in a single document. Create a new R Markdown file in RStudio via File &gt; New File &gt; R Markdown....\nRecently, the posit team has developed a new version of the R Markdown called quarto document, with the file extension .qmd. It is still under rapid development.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#vectors",
    "href": "App_A-intro-R.html#vectors",
    "title": "14  Appendix: Introduction to R",
    "section": "14.8 Vectors",
    "text": "14.8 Vectors\nVectors are the most basic data structure in R.\n\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\n\nYou can perform operations on vectors:\n\nx * 2\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#data-sets",
    "href": "App_A-intro-R.html#data-sets",
    "title": "14  Appendix: Introduction to R",
    "section": "14.9 Data Sets",
    "text": "14.9 Data Sets\nData frames are used for storing data tables. Create a data frame:\n\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Score = c(90, 85))\ndf\n\n   Name Score\n1 Alice    90\n2   Bob    85\n\n\nYou can import data from files using read.csv() or read.table().\n\nThis appendix is adapted from Why R?.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  }
]