[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Description",
    "text": "Description\nThis course will cover the topics in the theory and practice of Bayesian statistical inference, ranging from a review of fundamentals to questions of current research interest. Motivation for the Bayesian approach. Bayesian computation, Monte Carlo methods, asymptotics. Model checking and comparison. A selection of examples and issues in modelling and data analysis. Discussion of advantages and difficulties of the Bayesian approach. This course will be computationally intensive through analysis of data sets using the R statistical computing language.\n\nPrerequisites\nMATH 4752/6752 ‚Äì Mathematical Statistics II or equivalent, and the ability to program in a high-level language.\n\n\nInstructor\nChi-Kuang Yeh, Assistant Professor in the Department of Mathematics and Statistics, Georgia State University.\n\nOffice: Suite 1407, 25 Park Place.\nEmail: cyeh@gsu.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#office-hour",
    "href": "index.html#office-hour",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Office Hour",
    "text": "Office Hour\n10:00‚Äì13:00 on Monday, or by appointment.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#grade-distribution",
    "href": "index.html#grade-distribution",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\nHomework ‚Äì 50%\nExam ‚Äì 30%\nFinal ‚Äì 20%",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assignment",
    "href": "index.html#assignment",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Assignment",
    "text": "Assignment\n\nA1, due on Jan 29, 2026\nA2 TBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#midterm",
    "href": "index.html#midterm",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Midterm",
    "text": "Midterm\n\nMarch 3, 2026",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-and-corresponding-lectures",
    "href": "index.html#topics-and-corresponding-lectures",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Topics and Corresponding Lectures",
    "text": "Topics and Corresponding Lectures\nThose chapters are based on the lecture notes. This part will be updated frequently.\n\n\n\nStatus\nTopic\nLecture\n\n\n\n\n‚úÖ\nWelcome and Overview\n1\n\n\n‚úÖ\nIntro to R Programming\n2\n\n\n‚úÖ\nProbability and Exchangeability\n3‚Äì5\n\n\nüõ†Ô∏è\nOne Parameter Models\n6‚Äì",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-textbooks",
    "href": "index.html#recommended-textbooks",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Recommended Textbooks",
    "text": "Recommended Textbooks\n\nGelman, A., Carlin, J., Stern, H., Rubin, D., Dunson, D., and Vehtari, A. (2021). Bayesian Data Analysis, CRC Press, 3rd Ed.\nHoff, P.D. (2009). A First Course in Bayesian Statistical Methods, Springer.\nMcElreath, R. (2018). Statistical Rethinking: A Bayesian Course with Examples in R and Stan, CRC Press.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#side-readings",
    "href": "index.html#side-readings",
    "title": "STAT8310 - Bayesian Data Analysis",
    "section": "Side Readings",
    "text": "Side Readings\n\nTBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1¬† Quick Overview",
    "section": "",
    "text": "1.1 Why Bayesian?\nThe posterior distribution is obtained from the prior distribution and sampling model via Bayes‚Äô rule:\n\\[p(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta') p(\\theta') d \\theta'}.\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#why-bayesian",
    "href": "intro.html#why-bayesian",
    "title": "1¬† Quick Overview",
    "section": "",
    "text": "Intuitive probability interpretation: Directly quantifies uncertainty about parameters as probability distributions\nIncorporates prior knowledge: Systematically combines domain expertise with data through the prior distribution\nPrincipled inference: Bayes‚Äô rule provides a coherent framework for updating beliefs based on evidence\nNatural handling of uncertainty: Posterior distributions capture full uncertainty, not just point estimates\nSequential analysis: Easily updates beliefs as new data arrives (posterior becomes new prior)\nSmall sample inference: Performs well with limited data by leveraging prior information\nPrediction with uncertainty: Generates predictive distributions that quantify uncertainty in future observations\nDecision-making: Naturally incorporates loss functions for optimal decision rules\nModel comparison: Bayes factors provide a principled approach to comparing competing models",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#some-bayesian-topics-and-their-computational-focus",
    "href": "intro.html#some-bayesian-topics-and-their-computational-focus",
    "title": "1¬† Quick Overview",
    "section": "1.2 Some Bayesian Topics and their Computational Focus",
    "text": "1.2 Some Bayesian Topics and their Computational Focus\n\n\n\nTable¬†1.1: Some of the Bayesian Topics and its computational related focuses.\n\n\n\n\n\n\n\n\n\n\nTopics\nKey Concepts / Readings\nComputing Focus\n\n\n\n\nIntroduction to Bayesian Thinking\nBayesian vs.¬†Frequentist paradigms; Prior, likelihood, posterior\nReview of R basics and reproducible workflows\n\n\nBayesian Inference for Simple Models\nConjugate priors, Beta-Binomial, Normal-Normal, Poisson-Gamma\nSimulating posteriors, visualization\n\n\nPrior Elicitation and Sensitivity\nInformative vs.¬†noninformative priors, Jeffreys prior\nPrior sensitivity plots\n\n\nMonte Carlo Integration\nLaw of large numbers, sampling-based inference\nRandom sampling and Monte Carlo approximation\n\n\nMarkov Chain Monte Carlo (MCMC)\nMetropolis-Hastings, Gibbs sampler\nImplementing MCMC in R\n\n\nConvergence Diagnostics\nTrace plots, autocorrelation, Gelman‚ÄìRubin statistic\ncoda, rstan, and bayesplot packages\n\n\nHierarchical Bayesian Models\nPartial pooling, shrinkage, multilevel structures\nrstanarm / brms\n\n\nMidterm Project: Bayesian Linear Regression\nPosterior inference for regression, model selection\nbrms, rstanarm, custom Gibbs samplers\n\n\nBayesian Model Comparison\nBayes factors, BIC, DIC, WAIC, LOO\nPractical comparison via cross-validation\n\n\nModel Checking and Diagnostics\nPosterior predictive checks, residual analysis\npp_check in brms\n\n\nAdvanced Computation\nHamiltonian Monte Carlo (HMC), Variational Inference\nUsing Stan and CmdStanR\n\n\nBayesian Decision Theory\nUtility functions, decision rules, loss minimization\nSimple decision problems in R\n\n\nModern Bayesian Methods\nApproximate Bayesian computation (ABC), Bayesian neural networks\nExamples via rstan or tensorflow-probability\n\n\nStudent Project Presentations\nApplications and case studies\nFull workflow demonstration in R",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "intro.html#interesting-article",
    "href": "intro.html#interesting-article",
    "title": "1¬† Quick Overview",
    "section": "1.3 Interesting Article:",
    "text": "1.3 Interesting Article:\n\nGoligher, E.C., Harhay, M.O. (2023). What Is the Point of Bayesian Analysis?, American Journal of Respiratory and Critical Care Medicine, 209, 485‚Äì487.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Quick Overview</span>"
    ]
  },
  {
    "objectID": "01_probability.html",
    "href": "01_probability.html",
    "title": "2¬† Belief function and Probability Review",
    "section": "",
    "text": "2.1 Belief functions\nProbability is a way to express rational beliefs.\nSome more notations:\nHow should we interpret these properties, and do they make sense?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#belief-functions",
    "href": "01_probability.html#belief-functions",
    "title": "2¬† Belief function and Probability Review",
    "section": "",
    "text": "A belief function \\(\\mathrm{Be}(\\cdot)\\) is a function that assigns number to statements such that the large the number, the higher the degree of belief.\n\n\nLet \\(F, G\\), and \\(H\\) be three possibly overlapping statements about the world.\nFor example:\n\nF = { a person owns a smartphone }\nG = { a person uses social media daily }\nH = { a person works remotely at least part of the time }\n\nor\n\nF = { a person has a graduate degree }\nG = { a person works in a STEM field }\nH = { a person is employed in the private sector }\n\nThe perference over bets involving these statements can be used to define a belief function\n\n\\(\\mathrm{Be}(F)&gt;\\mathrm{Be}(G)\\) means you prefer a bet \\(F\\) is true over that \\(G\\) is true.\n\nAlso, we want \\(\\mathrm{Be}(\\cdot)\\) to describe our beliefs under certain conditions\n\n\\(\\mathrm{Be}(F\\mid H) &gt; \\mathrm{Be}(G\\mid H)\\) means that if we knew that \\(H\\) were true, then we would perfer to bet that \\(F\\) is also true over \\(G\\) is also true.\n\\(\\mathrm{Be}(F\\mid G) &gt; \\mathrm{Be}(F\\mid H)\\) means that if we bet on \\(F\\), we would perfer to do it under the condition that \\(G\\) is true rather than \\(H\\) is true.\n\n\n\n\nLet \\(\\neg\\) denote negation. That is, \\(\\neg F\\) is the statement that \\(F\\) is not true.\nLet \\(F \\vee G\\) denote the disjunction (or) of statements \\(F\\) and \\(G\\), meaning that at least one of \\(F\\) or \\(G\\) is true.\nLet \\(F \\wedge G\\) denote the conjunction (and) of statements \\(F\\) and \\(G\\), meaning that both \\(F\\) and \\(G\\) are true.\n\n\nIt has been argued by many that any function that is to numerically represent our beliefs should have the following properties:\n\nB1: \\(\\mathrm{Be}(\\neg H\\mid H) \\le \\mathrm{Be}(F \\mid H) \\le \\mathrm{Be}(H \\mid H)\\)\nB2: \\(\\mathrm{Be}(F \\vee G\\mid H) \\ge  \\max\\{\\mathrm{Be}(F \\mid H), \\mathrm{Be}(G\\mid H)\\}\\)\nB3: \\(\\mathrm{Be}(F \\wedge G\\mid H)\\) can be derived from \\(\\mathrm{Be}(G\\mid H)\\) and \\(\\mathrm{Be}(F\\mid G \\wedge H)\\).\n\n\n\n\nB1 means that the number we assign to \\(\\mathrm{Be}(F \\mid H)\\), our conditional belief in \\(F\\) given \\(H\\), is bounded below and above by the numbers we assign to complete disbelief \\(\\mathrm{Be}(\\neg H \\mid H)\\) and complete belief \\(\\mathrm{Be}(H \\mid H)\\).\nB2 says that our belief that the truth lies in a given set of possibilities should not decrease as we add to the set of possibilities.\nB3 is a bit trickier. To see why it makes sense, imagine you have to decide whether or not \\(F\\) and \\(G\\) are true, knowing that \\(H\\) is true. You could do this by first deciding whether or not \\(G\\) is true given \\(H\\), and if so, then deciding whether or not \\(F\\) is true given \\(G\\) and \\(H\\).\n\n\nRecall the notation from (elementary) probability that, \\(F\\cup G\\) means F or G, and \\(F\\cap G\\) means F and G, and \\(\\emptyset\\) is the empty set.\n\nP1: \\[\n0=\\mathrm{Pr}(\\neg H \\mid H) \\leq \\mathrm{Pr}(F \\mid H) \\leq \\mathrm{Pr}(H \\mid H)=1\n\\]\nP2: \\[\\mathrm{Pr}(F \\cup G \\mid H)=\\mathrm{Pr}(F \\mid H)+\\mathrm{Pr}(G \\mid H),\\quad \\text{if}\\quad F \\cap G=\\emptyset\\]\nP3: \\[\\mathrm{Pr}(F \\cap G \\mid H)=\\mathrm{Pr}(G \\mid H) \\mathrm{Pr}(F \\mid G \\cap H)\\]\n\n\n\n2.1.1 Conclusion\nYou can see that, a probability function satisfy P1‚ÄìP3 also satisfies B1‚ÄìB3. Therefore, probability functions are a special case of belief functions, and we can use it to describe our belief.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#events-partitions-and-bayes-rule",
    "href": "01_probability.html#events-partitions-and-bayes-rule",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.2 Events, Partitions and Bayes‚Äô Rule",
    "text": "2.2 Events, Partitions and Bayes‚Äô Rule\n\nA collectiion of sets \\(\\{H_1,\\dots,H_K\\}\\) is a partition of another set \\(\\mathcal{H}\\) if\n\n\\(H_i \\cap H_j = \\emptyset\\) for all \\(i \\neq j\\) (mutually exclusive);\n\\(\\bigcup_{i=1}^K H_i = \\mathcal{H}\\) (collectively exhaustive).\n\n\nIn the context of indetifying which of several statements is true, if \\(\\mathcal{H}\\) is the set of all possible truths and \\(\\{H_1,\\dots,H_K\\}\\) is a partition of \\(\\mathcal{H}\\), then exactly one set \\(H_j\\) contains the truth.\n\nLet \\(\\mathcal{H}\\) be the status of a statistical model.\nValid partitions include:\n\n\\(\\{\\text{correctly specified}, \\text{misspecified}\\}\\)\n\\(\\{\\text{underfitting}, \\text{well-specified}, \\text{overfitting}\\}\\)\n\n\n\n2.2.1 Partition and Probability\nSuppose \\(\\{H_1,\\dots,H_K\\}\\) is a partition of \\(\\mathcal{H}\\),\\(\\mathrm{Pr}(\\mathcal{H})=1\\) and \\(E\\) is some specific event. Then, by the axioms of probability, we have\n\nLaw of total probability \\[\n\\sum_{k=1}^K \\mathrm{Pr}(H_k)=\\mathrm{Pr}\\left(\\bigcup_{k=1}^K H_k\\right)=\\mathrm{Pr}(\\mathcal{H})=1\n\\]\nLaw of marginal probability \\[\n\\mathrm{Pr}(E)=\\sum_{k=1}^K \\mathrm{Pr}(E \\cap H_k)=\\sum_{k=1}^K \\mathrm{Pr}(E \\mid H_k) \\mathrm{Pr}(H_k)\n\\]\nBayes‚Äô rule \\[\n\\mathrm{Pr}(H_j \\mid E)=\\frac{\\mathrm{Pr}(E \\mid H_j) \\mathrm{Pr}(H_j)}{\\mathrm{Pr}(E)}=\\frac{\\mathrm{Pr}(E \\mid H_j) \\mathrm{Pr}(H_j)}{\\sum_{k=1}^K \\mathrm{Pr}(E \\mid H_k) \\mathrm{Pr}(H_k)}\n\\]\n\n\nA subset of the 1996 General Social Survey includes data on the education level and income for a sample of males over 30 years of age. Let {H1,H2,H3,H4} be the events that a randomly selected person in this sample is in, respectively, the lower 25th percentile, the second 25th percentile, the third 25th percentile and the upper 25th percentile in terms of income. By definition,\n\\[\n\\left\\{\\operatorname{Pr}\\left(H_1\\right), \\operatorname{Pr}\\left(H_2\\right), \\operatorname{Pr}\\left(H_3\\right), \\operatorname{Pr}\\left(H_4\\right)\\right\\}=\\{.25, .25, .25, .25\\} .\n\\]\nNote that \\(\\{H1,H2,H3,H4\\}\\) is a partition and so these probabilities sum to 1. Let \\(E\\) be the event that a randomly sampled person from the survey has a college education. From the survey data, we have \\[\\{\\mathrm{Pr}(E\\mid H_1), \\mathrm{Pr}(E\\mid H_2),\\mathrm{Pr}(E\\mid H_3), \\mathrm{Pr}(E\\mid H_4)\\} = \\{.11, .19, .31, .53\\}.\\] These probabilities do not sum to 1 - they represent the proportions of people with college degrees in the four different income subpopulations \\(H_1, H_2, H_3\\) and \\(H_4\\). Now let‚Äôs consider the income distribution of the college-educated population. Using Bayes‚Äô rule we can obtain\n\\(\\{\\mathrm{Pr}(H_1\\mid E), \\mathrm{Pr}(H_2\\mid E), \\mathrm{Pr}(H_3\\mid E), \\mathrm{Pr}(H_4 \\mid E)\\} = \\{0.09, 0.17, 0.27, 0.47\\} ,\\) and we see that the income distribution for people in the college-educated population differs markedly from \\(\\{0.25, 0.25,0.25,0.25\\}\\), the distribution for the general population. Note that these probabilities do sum to 1 - they are the conditional probabilities of the events in the partition, given \\(E\\).\n\nIn Bayesian inference, \\({H_1, . . . ,H_K}\\) often refer to disjoint hypotheses or states of nature and \\(E\\) refers to the outcome of a survey, study or experiment. To compare hypotheses post-experimentally, we often calculate the following ratio: \\[\n\\begin{aligned}\n\\frac{\\operatorname{Pr}\\left(H_i \\mid E\\right)}{\\operatorname{Pr}\\left(H_j \\mid E\\right)} & =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right) \\operatorname{Pr}\\left(H_i\\right) / \\operatorname{Pr}(E)}{\\operatorname{Pr}\\left(E \\mid H_j\\right) \\operatorname{Pr}\\left(H_j\\right) / \\operatorname{Pr}(E)} \\\\\n& =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right) \\operatorname{Pr}\\left(H_i\\right)}{\\operatorname{Pr}\\left(E \\mid H_j\\right) \\operatorname{Pr}\\left(H_j\\right)} \\\\\n& =\\frac{\\operatorname{Pr}\\left(E \\mid H_i\\right)}{\\operatorname{Pr}\\left(E \\mid H_j\\right)} \\times \\frac{\\operatorname{Pr}\\left(H_i\\right)}{\\operatorname{Pr}\\left(H_j\\right)} \\\\\n& =\\text { \"Bayes factor\" √ó \"prior beliefs\". }\n\\end{aligned}\n\\] This calculation reminds us that Bayes‚Äô rule does not determine what our beliefs should be after seeing the data, it only tells us how they should change after seeing the data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#independence",
    "href": "01_probability.html#independence",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.3 Independence",
    "text": "2.3 Independence\n\nTwo events \\(F\\) and \\(G\\) are conditionally independent, if given \\(H\\), we have \\(\\mathrm{Pr}(F \\cap G \\mid H) = \\mathrm{Pr}(F\\mid H)\\mathrm{Pr}(G\\mid H)\\).\n\nHow do we interpret conditional independence? By Axiom P3, the following is always true: \\[\n\\begin{array}{rlll}\n\\operatorname{Pr}(G \\mid H) \\operatorname{Pr}(F \\mid H \\cap G) \\stackrel{\\text { always }}{=} \\operatorname{Pr}(F \\cap G \\mid H) & \\stackrel{\\text { independence }}{=} \\operatorname{Pr}(F \\mid H) \\operatorname{Pr}(G \\mid H) \\\\\n\\operatorname{Pr}(G \\mid H) \\operatorname{Pr}(F \\mid H \\cap G) & = & \\operatorname{Pr}(F \\mid H) \\operatorname{Pr}(G \\mid H) \\\\\n\\operatorname{Pr}(F \\mid H \\cap G) & = & \\operatorname{Pr}(F \\mid H) .\n\\end{array}\n\\]\nThus, conditional independence implies that \\(\\mathrm{Pr}(F \\mid H \\cap G) = \\mathrm{Pr}(F\\mid H)\\). In other words, if we know \\(H\\) is true, and \\(F\\) and \\(G\\) are conditionally independent given \\(H\\), then knowing \\(G\\) does not change our belief about \\(F\\).\n\nLet‚Äôs consider the conditional dependence of \\(F\\) and \\(G\\) when \\(H\\) is assumed to be true in the following two situations:\nSiutation 1:\n\nF = { a hospital patient is a smoker }\nG = { a hospital patient has lung cancer }\nH = { smoking causes lung cancer}\n\nSituation 2:\n\nF = { a student studies regularly for an exam }\nG = { a student receives a high exam score }\nH = { studying improves exam performance }\n\nThink: In both of these situations, H being true implies a relationship between \\(F\\) and \\(G\\). What about when \\(H\\) is not true?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#random-variables",
    "href": "01_probability.html#random-variables",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.4 Random Variables",
    "text": "2.4 Random Variables\nIn Bayesian inference a random variable is defined as an unknown numerical quantity about which we make probability statements. For example, the quantitative outcome of a survey, experiment or study is a random variable before the study is performed. Additionally, a fixed but unknown population parameter is also a random variable\n\n2.4.1 Discrete Ramdon variables\nLet \\(Y\\) be a random variable and let \\(\\mathcal{Y}\\) be the set of all possible values that \\(Y\\) can take. If \\(\\mathcal{Y}\\) is countable, meaning that \\(\\mathcal{Y} = \\{y_1,y_2,\\dots\\}\\), then \\(Y\\) is a discrete random variable.\n\nThe event that the outcome \\(Y\\) of our survey has the value \\(Y\\) is expressed as \\(\\{Y=y\\}\\). For each \\(y\\in\\mathcal{Y}\\), the shorthand notation for \\(\\mathrm{Pr}(Y=y)\\) is \\(p(y)\\), and \\(p(\\cdot)\\) is called the probability mass function of \\(Y\\), and with two properties\n\n\\(0 \\leq p(y) \\leq 1\\) for all \\(y\\in\\mathcal{Y}\\),\n\\(\\sum_{y\\in\\mathcal{Y}} p(y) = 1\\).\n\n\nGeneral probability statements about \\(Y\\) can be derived from the pdf/pmf, for example, for any subset \\(A \\subseteq \\mathcal{Y}\\), we have \\(\\mathrm{Pr}(Y\\in A) = \\sum_{y\\in A} p(y)\\). When we have two disjoint subsets \\(A\\) and \\(B\\) of \\(\\mathcal{Y}\\), we have \\[\\mathrm{Pr}(Y\\in A \\cup B) = \\mathrm{Pr}(Y\\in A) + \\mathrm{Pr}(Y\\in B)=\\sum_{y\\in A} p(y) + \\sum_{y\\in B} p(y).\\]\n\nLet \\(Y\\) be the number of successes in \\(n\\) independent Bernoulli trials, each with probability of success \\(\\theta\\). Then, \\(Y\\) follows a Binomial distribution with parameters \\(n\\) and \\(\\theta\\), denoted as \\(Y \\sim \\mathrm{Binomial}(n,p)\\). The probability mass function of \\(Y\\) is given by \\[\np(y) = \\mathrm{Pr}(Y=y) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y}, \\quad y=0,1,2,\\dots,n.\n\\] If \\(\\theta=0.3\\) and \\(n=3\\), then the probability of observing exactly 2 successes is \\[\np(2) = \\mathrm{Pr}(Y=2 \\mid \\theta=0.3) = \\binom{3}{2\n} (0.3)^2 (0.7)^{1} = 3 \\cdot 0.09 \\cdot 0.7 = 0.189.\n\\]\n\n\n\n2.4.2 Continuous random variables\nIf \\(\\mathcal{Y}\\) is uncountable, for example, \\(\\mathcal{Y} = \\mathbb{R}\\) or \\(\\mathcal{Y} = (0,1)\\), then \\(Y\\) is a continuous random variable. In this case, we cannot list all possible values of \\(Y\\) and assign probabilities to each value. Instead, we use a probability distribution to describe the distribution of \\(Y\\). That is, the cummulative distribution function (cdf) defined as follows.\n\nThe cumulative distribution function (cdf) of a continuous random variable \\(Y\\) is defined as \\[\nF(y) = \\mathrm{Pr}(Y \\leq y), \\quad y \\in \\mathcal{Y}.\n\\]\n\nNote that, for the cdf \\(F(y)\\), we have the following properties:\n\n\\(0 \\leq F(y) \\leq 1\\) for all \\(y\\in\\mathcal{Y}\\),\n\\(F(y)\\) is non-decreasing, meaning that if \\(y_1 &lt; y_2\\), then \\(F(y_1) \\leq F(y_2)\\),\n\\(\\lim_{y \\to -\\infty} F(y) = 0\\)\n\\(\\lim_{y \\to \\infty} F(y) = 1\\).\n\nProbability of various events can be derived from the cdf. For example, for any interval \\(A = (a,b] \\subseteq \\mathcal{Y}\\), we have \\[\n\\mathrm{Pr}(Y \\in A) = \\mathrm{Pr}(a &lt; Y \\leq b) =\nF(b) - F(a).\n\\] Also, \\(\\mathrm{Pr}(Y \\leq a) = F(a)\\) and \\(\\mathrm{Pr}(Y &gt; a) = 1 - F(a)\\).\n\n\n2.4.3 Description of distributions through quantiles and moments\nIn this subsection, we discuss a few ways to describe probability distributions: quantiles and moments. They are used to describe the behaviour of the distribution compressing them into summary statistics.\n\nThe expectation or mean of a random variable \\(Y\\) can be thought as the centre of mass or the location of the distribution, which is defined as\n\nFor discrete random variable: \\[\nE(Y) = \\sum_{y\\in\\mathcal{Y}} y p(y).\n\\]\nFor continuous random variable: \\[\nE(Y) = \\int_{\\mathcal{Y}} y f(y) dy.\n\\]\n\n\n\n\n\n\n\n\nNoteDifference between mean, mode and median\n\n\n\n\nMean: the centre of mass of the distribution\nMode: The most probable value of \\(Y\\)\nMedian: The value of Y in the middle of the distribution.\n\n\n\nIn skewed distribution, the three will not equal to each other.\n\nlibrary(ggplot2)\n\n# -----------------------------\n# Theoretical reference lines\n# -----------------------------\nlines_normal &lt;- data.frame(\n  value = c(0, 0, 0),\n  Statistic = c(\"Mean\", \"Median\", \"Mode\")\n)\n\nlines_lognormal &lt;- data.frame(\n  value = c(exp(1/8), 1, exp(-1/4)),\n  Statistic = c(\"Mean\", \"Median\", \"Mode\")\n)\n\ncols &lt;- c(\"Mean\" = \"red\", \"Median\" = \"darkgreen\", \"Mode\" = \"purple\")\n\n# -----------------------------\n# Normal distribution\n# -----------------------------\np1 &lt;- ggplot() +\n  stat_function(fun = dnorm, size = 1, color = \"black\") +\n  geom_vline(\n    data = lines_normal,\n    aes(xintercept = value, color = Statistic),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = cols) +\n  scale_x_continuous(limits = c(-4, 4)) +\n  labs(\n    title = \"Non-skewed Distribution (Normal)\",\n    x = \"Value\", y = \"Density\", color = \"Statistic\"\n  ) +\n  theme_minimal()\n\n# -----------------------------\n# Log-normal distribution: LN(0, 0.5)\n# -----------------------------\np2 &lt;- ggplot() +\n  stat_function(\n    fun = function(x) dlnorm(x, meanlog = 0, sdlog = 0.5),\n    size = 1,\n    color = \"black\"\n  ) +\n  geom_vline(\n    data = lines_lognormal,\n    aes(xintercept = value, color = Statistic),\n    linetype = \"dashed\",\n    size = 1\n  ) +\n  scale_color_manual(values = cols) +\n  scale_x_continuous(limits = c(0, 8)) +\n  labs(\n    title = \"Skewed Distribution (Log-normal)\",\n    x = \"Value\", y = \"Density\", color = \"Statistic\"\n  ) +\n  theme_minimal()\n\np1\n\n\n\n\n\n\n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy use mean?\n\n\n\nThe mean is widely used in statistics and data analysis for several reasons:\n\nMathematical properties: The mean has desirable mathematical properties, such as linearity, which makes it easier to work with in various statistical analyses and models.\nSensitivity to all values: The mean takes into account all values in the dataset, providing a comprehensive measure of central tendency. It is also a scaled version of the total, which is often an interest\nFoundation for other statistical measures: The mean serves as the basis for many other statistical measures, such as variance and standard deviation, which are essential for understanding the spread and variability of data.\nMean minimizes the sum of squared deviations: The mean is the value that minimizes the sum of squared deviations (i.e., the expected penalty by choosing one value) from itself, making it a natural choice for summarizing data.\nMay contains full information: In some distributions (e.g., bernoulli distribution), the mean contains all the information about the distribution, making it a sufficient statistic for inference.\n\n\n\n\nThe variance of a random variable \\(Y\\) measures the spread or dispersion of the distribution, and is defined as \\[\n\\mathrm{Var}(Y) = E\\left[(Y - E(Y))^2\\right] = E[Y^2]- E^2[Y].\n\\] The standard deviation is the square root of the variance, denoted as \\(\\mathrm{SD}(Y) = \\sqrt{\\mathrm{Var}(Y)}\\).\n\n\nThe quantile of order \\(\\alpha\\) of a random variable \\(Y\\) is defined as the value \\(y_\\alpha\\) such that \\[\n\\mathrm{Pr}(Y \\leq y_\\alpha) = F(y_\\alpha) = \\alpha\n\\] for \\(0 &lt; \\alpha &lt; 1\\).\n\nFor example, the median is the quantile of order 0.5, denoted as \\(y_{0.5}\\), which satisfies \\(\\mathrm{Pr}(Y \\leq y_{0.5}) = 0.5\\). Also, \\((y_{0.025},y_{0.975})\\) and \\((y_{0.25},y_{0.75})\\) contains 95% and 50% of the mass of the distribution, respectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#joint-disitrubiton",
    "href": "01_probability.html#joint-disitrubiton",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.5 Joint Disitrubiton",
    "text": "2.5 Joint Disitrubiton\n\n2.5.1 Discrete random variables\nLet \\(Y_1\\) and \\(Y_2\\) be two random variables with possible values in \\(\\mathcal{Y}_1\\) and \\(\\mathcal{Y}_2\\), respectively. The joint distribution of \\(Y_1\\) and \\(Y_2\\) describes the probability of various combinations of values that \\((Y_1,Y_2)\\) can take.\nJoint beliefs about \\(Y_1\\) and \\(Y_2\\) can be represented with probabilities. For example, for subsets \\(A\\subset \\mathcal{Y}_1\\) and \\(B\\subset \\mathcal{Y}_2\\), \\(\\mathrm{Pr}(\\{Y_1\\in A\\} \\cap \\{Y_2 \\in B\\})\\) represents our belief that \\(Y_1\\) takes a value in \\(A\\) and \\(Y_2\\) takes a value in \\(B\\). The joint pdf or joint density of \\(Y_1\\) and \\(Y_2\\) is defined as\n\\[\np_{Y_1 Y_2}\\left(y_1, y_2\\right)=\\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right), \\text { for } y_1 \\in \\mathcal{Y}_1, y_2 \\in \\mathcal{Y}_2 .\n\\]\nThe marginal density of \\(Y_1\\) can be computed from the joint density: \\[\n\\begin{aligned}\np_{Y_1}\\left(y_1\\right) & \\equiv \\operatorname{Pr}\\left(Y_1=y_1\\right) \\\\\n& =\\sum_{y_2 \\in \\mathcal{Y}_2} \\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right) \\\\\n& \\equiv \\sum_{y_2 \\in \\mathcal{Y}_2} p_{Y_1 Y_2}\\left(y_1, y_2\\right)\n\\end{aligned}\n\\]\nThe conditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\) can be computed from the joint density and the marginal density: \\[\n\\begin{aligned}\np_{Y_2 \\mid Y_1}\\left(y_2 \\mid y_1\\right) & =\\frac{\\operatorname{Pr}\\left(\\left\\{Y_1=y_1\\right\\} \\cap\\left\\{Y_2=y_2\\right\\}\\right)}{\\operatorname{Pr}\\left(Y_1=y_1\\right)} \\\\\n& =\\frac{p_{Y_1 Y_2}\\left(y_1, y_2\\right)}{p_{Y_1}\\left(y_1\\right)} .\n\\end{aligned}\n\\]\nYou should be able to see that\n\n\\(\\left\\{p_{Y_1}, p_{Y_2 \\mid Y_1}\\right\\}\\) can be derived from \\(p_{Y_1 Y_2}\\),\n\\(\\left\\{p_{Y_2}, p_{Y_1 \\mid Y_2}\\right\\}\\) can be derived from \\(p_{Y_1 Y_2}\\)\n\\(p_{Y_1 Y_2}\\) can be derived from \\(\\left\\{p_{Y_1}, p_{Y_2 \\mid Y_1}\\right\\}\\)\n\\(p_{Y_1 Y_2}\\) can be derived from \\(\\left\\{p_{Y_2}, p_{Y_1 \\mid\nY_2}\\right\\}\\)\n\nBUT\n\n\\(p_{Y_1 Y_2}\\) cannot be derived from \\(\\left\\{p_{Y_1}, p_{Y_2}\\right\\}\\).\n\nThe subscripts of density functions are often dropped, in which case the type of density function is determined by the arguments. For example,\n\n\\(p(y_1,y_2)=p_{Y_1 Y_2}(y_1,y_2)\\) is the joint density of \\(Y_1\\) and \\(Y_2\\),\n\\(p(y_1)=p_{Y_1}(y_1)\\) is the marginal density of \\(Y_1\\)\n\\(p(y_2 \\mid y_1)=p_{Y_2\\mid Y_1}(y_2 \\mid y_1)\\) is the conditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\), and so on.\n\n\nSuppose a sociological study reports the following joint distribution of parents‚Äô education level and children‚Äôs income level in a population.\nJoint distribution of education and income Suppose a sociological study reports the following joint distribution of parents‚Äô education level and children‚Äôs income level in a population as shown in the Table below\n\n\n\nParent \\ Child\nLow Income\nMiddle Income\nHigh Income\n\n\n\n\nHigh School or Less\n0.18\n0.22\n0.10\n\n\nCollege\n0.08\n0.20\n0.12\n\n\nGraduate School\n0.04\n0.06\n0.10\n\n\n\nSuppose we randomly sample a parent‚Äìchild pair from this population.\nLet\n- \\(Y_1\\) be the parent‚Äôs education level\n- \\(Y_2\\) be the child‚Äôs income level\nWe are interested in the conditional probability that the child has high income, given that the parent has a college education.\nWe may answer this question using the conditional probability formula:\n\\[\n\\Pr(Y_2 = \\text{High Income} \\mid Y_1 = \\text{College})\n= \\frac{\\Pr(Y_2 = \\text{High Income} \\cap Y_1 = \\text{College})}\n{\\Pr(Y_1 = \\text{College})}\n\\]\nFrom the table,\n\\[\n\\Pr(Y_2 = \\text{High Income} \\cap Y_1 = \\text{College}) = 0.12\n\\]\n\\[\n\\Pr(Y_1 = \\text{College}) = 0.08 + 0.20 + 0.12 = 0.40\n\\]\nTherefore,\n\\[\n\\Pr(Y_2 = \\text{High Income} \\mid Y_1 = \\text{College})\n= \\frac{0.12}{0.40}\n= 0.30\n\\]\nThus, our conclusion from the table is, among children whose parents have a college education, 30% attain high income.\n\n\n\n2.5.2 Continuous random variables\nLet \\(Y_1\\) and \\(Y_2\\) be two continuous random variables with possible values in \\(\\mathcal{Y}_1\\) and \\(\\mathcal{Y}_2\\), respectively. The joint distribution of \\(Y_1\\) and \\(Y_2\\) describes the probability of various combinations of values that \\((Y_1,Y_2)\\) can take. We again work with the cumulative distribution function (cdf). The definition is given as follows.\n\nGiven a continuous joint cdf \\(F_{Y_1 Y_2}(y_1,y_2)\\), there is a function \\(p_{Y_1,Y_2}\\) such that \\[\nF_{Y_1,Y_2}(a,b) = \\int_{-\\infty}^a \\int_{-\\infty}^b p_{Y_1,Y_2}(y_1,y_2) dy_2 dy_1,\n\\] and \\(p_{Y_1,Y_2}(y_1,y_2)\\) is called the joint density function of \\(Y_1\\) and \\(Y_2\\).\n\nSimilar to the discrete case, we can derive marginal and conditional densities from the joint density as\n\nMarginal density of \\(Y_1\\): \\[\np_{Y_1}(y_1) = \\int_{\\mathcal{Y}_2} p_{Y_1,Y_2}(y_1,y_2) dy_2,\n\\]\nConditional density of \\(Y_2\\) given \\(\\{Y_1=y_1\\}\\): \\[\np_{Y_2 \\mid Y_1}(y_2 \\mid y_1) = \\frac{p_{Y_1,Y_2}(y_1,y_2)}{p_{Y_1}(y_1)}.\n\\]\n\nThink about why \\(p_{Y_2 \\mid Y_1}(y_2 \\mid y_1)\\) is an actual pdf.\n\n\n2.5.3 Mixed continuous and discrete variables\nIt is possible to have joint distributions involving both discrete and continuous random variables. For example, let \\(Y_1\\) be a discrete random variable taking values in \\(\\mathcal{Y}_1\\) and \\(Y_2\\) be a continuous random variable taking values in \\(\\mathcal{Y}_2\\). The joint distribution of \\(Y_1\\) and \\(Y_2\\) can be described by the joint density function \\(p_{Y_1,Y_2}(y_1,y_2)\\), which gives the probability that \\(Y_1\\) takes the value \\(y_1\\) and \\(Y_2\\) takes a value in an infinitesimal interval around \\(y_2\\). One such as example is that \\(Y_1\\) is a binary variable indicating the presence or absence of a disease, and \\(Y_2\\) is a continuous variable representing the severity of symptoms. Suppose we define\n\nMarginal density \\(p_{Y_1}\\) from our belief \\(\\mathrm{Pr}(Y_1=y_1)\\)\na conditional density \\(p_{Y_2\\mid Y_1}\\) from \\(\\mathrm{Pr}(Y_2\\le y_2\\mid Y_1=y_1)\\doteq F_{Y_2\\mid Y_1}(y_2\\mid y_1)\\).\n\nThen, the joint density can be derived as \\[\np_{Y_1,Y_2}(y_1,y_2) = p_{Y_1}(y_1) p_{Y_2 \\mid Y_1}(y_2 \\mid y_1),\n\\] and the probability can be calculated as \\[\n\\mathrm{Pr}(Y_1\\in A,Y_2\\in B) = \\int_{y_2\\in B} \\left\\{\\sum_{y_1\\in A}p_{Y_1,Y_2}(y_1,y_2)\\right\\}dy_2.\n\\]\n\n\n2.5.4 Bayes‚Äô rule and parameter estimation\nLet\n\n\\(\\theta\\): proportion of people in a large population who have a certain charactersitic.\n\\(Y\\): number of people in a small random sample from the population who have the charactersitic\n\nThen, in this case, we may threat \\(\\theta\\) as continuous random variable taking values in \\(\\Theta = (0,1)\\), and \\(Y\\) as a discrete random variable taking values in \\(\\mathcal{Y}= \\{0,1,2,\\dots,n\\}\\), where \\(n\\) is the sample size. Bayesian estimation of the parameter \\(\\theta\\) derives from the calculate of \\(p(\\theta\\mid y)\\) where \\(y\\) is the observed value of \\(Y\\). In Bayesian, this calculation first requires that we have a joint density \\(p(y,\\theta)\\) representing our belief about \\(\\theta\\) and the survey outcome \\(Y\\). Often, it is natural to construct this joint density from\n\n\\(p(\\theta)\\): our prior belief about \\(\\theta\\) before seeing the data, and\n\\(p(y \\mid \\theta)\\): belief about \\(Y\\) given \\(\\theta\\), often called the likelihood function.\n\nOnce we observed \\(\\{Y=y\\}\\), we need to compute our updated belief about \\(\\theta\\), represented by the posterior density \\(p(\\theta \\mid y)\\) as \\[\np(\\theta \\mid y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta) p(\\theta) d\\theta}.\n\\]\nIf we have two values \\(\\theta_1\\) and \\(\\theta_2\\) in \\(\\Theta\\) that may be true, then the ratio of their posterior densities is given by\n\\[\n\\frac{p(\\theta_1 \\mid y)}{p(\\theta_2 \\mid y)} = \\frac{p(y \\mid \\theta_1) p(\\theta_1) / p(y)}{p(y \\mid \\theta_2) p(\\theta_2) / p(y)} =\n\\frac{p(y \\mid \\theta_1) p(\\theta_1)}{p(y \\mid \\theta_2) p(\\theta_2)}.\n\\]\n\n\n\n\n\n\nNoteNote\n\n\n\nFrom this calculation, we notice when we are calculating the relative posterior probability between two parameter values we do not need calculate \\(p(y)\\) out.\n\n\nAnother way to think about this is, for a function of \\(\\theta\\), \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta).\n\\]\n\n\n\n\n\n\nNoteNote\n\n\n\nWe will see that the numerator is the important part, while the denominator is just a normalizing constant to make sure the posterior density integrates to 1.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#independence-random-variables",
    "href": "01_probability.html#independence-random-variables",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.6 Independence Random Variables",
    "text": "2.6 Independence Random Variables\nLet \\(Y_1,\\dots,Y_n\\) be random variables with joint density \\(p(y_1,\\dots,y_n)\\), and \\(\\theta\\) is the parameter describe the conditions under which the random variables are generated. We say that \\(Y_1,\\dots,Y_n\\) are conditionally independent given \\(\\theta\\) if every collection of \\(n\\) sets \\(\\{A_1,\\dots,A_n\\}\\) satisfies \\[\n\\mathrm{Pr}(Y_1 \\in A_1,\\dots,Y_n \\in A_n \\mid \\theta) = \\prod_{i=1}^n \\mathrm{Pr}(Y_i \\in A_i \\mid \\theta).\n\\] If we have independence property, then \\[\n\\mathrm{Pr}(Y_i\\in A_i \\mid \\theta, Y_j\\in A_j) = \\mathrm{Pr}(Y_i \\in A_i \\mid \\theta),\n\\] so the conditional indpenddence can be interpreted as meaning that \\(Y_j\\) gives no additional information about \\(Y_i\\) once we know \\(\\theta\\). Also, under independence, the joint density can be factorized as \\[\np(y_1,\\dots,y_n \\mid \\theta) = \\prod_{i= 1}^n p_{Y_i}(y_i \\mid \\theta).\n\\]\nIf the samples are also identically distributed, meaning that each \\(Y_i\\) has the same marginal density \\(p_Y(y \\mid \\theta)\\), then the joint density can be further simplified as \\[\np(y_1,\\dots,y_n \\mid \\theta) = \\prod_{i= 1}^n p_Y(y_i \\mid \\theta).\n\\]\nIn this case , we say that \\(Y_1,\\dots,Y_n\\) are independent and identically distributed (i.i.d.) given \\(\\theta\\), with notation \\[  \nY_1,\\dots,Y_n\\mid \\theta \\stackrel{i.i.d.}{\\sim} p_Y(y \\mid \\theta).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#exchangeability",
    "href": "01_probability.html#exchangeability",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.7 Exchangeability",
    "text": "2.7 Exchangeability\n\nA sequence of random variables \\(Y_1,Y_2,\\dots,Y_n\\) is exchangeable if for any permutation \\(\\pi\\) of the indices \\(\\{1,2,\\dots ,n\\}\\), we have \\[\np(y_1,y_2,\\dots,y_n) = p(y_{\\pi(1 )},y_{\\pi(2)},\\dots,y_{\\pi(n)}).\n\\]\n\nIn other words, the joint density of an exchangeable sequence is invariant to the order of the random variables. That is, the labels contains no information about the outcome.\n\nSuppose a factory produces a large batch of items. Each item may be either defective or non-defective.\nLet \\[\nY_i =\n\\begin{cases}\n1, & \\text{if the } i\\text{th inspected item is defective}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nWe inspect \\(n = 10\\) items chosen at random from the batch and record\n\\(Y_1, Y_2, \\dots, Y_{10}.\\)\nConsider the following three observed sequences:\n\n\\(p(1,0,1,0,1,0,0,1,0,1)\\)\n\\(p(0,1,0,1,0,1,1,0,0,1)\\)\n\\(p(1,1,0,0,1,0,1,0,0,1)\\)\n\nEach sequence contains 5 defective items and 5 non-defective items.\nQuestion: Is there a reason to assign these three sequences different probabilities?\nIf the inspection order conveys no additional information about quality, then only the number of defective items matters, not their positions in the sequence. This motivates the concept of exchangeability.\n\n\n2.7.1 Independence versus dependence\nConsider the probability assignments\n\\[\n\\begin{cases}\n\\Pr(Y_{10} = 1) = a, \\\\[6pt]\n\\Pr(Y_{10} = 1 \\mid Y_1 = \\cdots = Y_9 = 1) = b.\n\\end{cases}\n\\]\nIf \\(a \\neq b\\), then \\(Y_{10}\\) is not independent of \\(Y_1, \\dots, Y_9\\).\nHowever, lack of independence does not imply lack of exchangeability.\nQuestion: should we have \\(a = b\\), \\(a &gt; b\\) or \\(a  &lt; b\\)?\n\n\n2.7.2 A latent-parameter model\nSuppose the defect rate \\(\\theta\\) of the factory is unknown.\nConditional on \\(\\theta\\), \\[\nY_1, \\dots, Y_{10} \\mid \\theta \\sim \\text{i.i.d. Bernoulli}(\\theta).\n\\]\nThen \\[\n\\Pr(Y_1 = y_1, \\dots, Y_{10} = y_{10} \\mid \\theta)\n= \\theta^{\\sum y_i}(1-\\theta)^{10-\\sum y_i}.\n\\]\nIf our uncertainty about \\(\\theta\\) is described by a prior distribution \\(p(\\theta)\\), the marginal joint distribution is\n\\[\np(y_1, \\dots, y_{10})\n= \\int \\theta^{\\sum y_i}(1-\\theta)^{10-\\sum y_i} p(\\theta)\\, d\\theta.\n\\]\nThis probability depends only on the number of defective items, not their order.\nThus, we have exchangeability, even though the \\(Y_i\\) are not independent under this model of belief.\n\nConditional i.i.d. given a latent parameter implies marginal exchangeability. That is, if \\(\\theta \\sim p(\\theta)\\) and \\(Y_1,\\dots,Y_n\\) are conditionally i.i.d. given \\(\\theta\\), then \\(Y_1,\\dots,Y_n\\) (i.e., unconditional on \\(\\theta\\)) are exchangeable.\n\nFor the Proof, see page 28 in Hopf (2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "01_probability.html#de-finettis-theorem",
    "href": "01_probability.html#de-finettis-theorem",
    "title": "2¬† Belief function and Probability Review",
    "section": "2.8 de Finetti‚Äôs Theorem",
    "text": "2.8 de Finetti‚Äôs Theorem\nAs of now, we have seen that conditional i.i.d. given a latent parameter implies marginal exchangeability. For example, \\[\n\\begin{cases}\nY_1,\\dots,Y_n \\mid \\theta \\stackrel{i.i.d.}{\\sim}  \\\\\n\\theta \\sim p(\\theta) \\end{cases} \\implies Y_1,\\dots,Y_n \\text{ are exchangeable}.\n\\]\nThe converse is also true, as stated in de Finetti‚Äôs theorem.\n\nLet \\(Y_i\\in\\mathcal{Y}\\) for all \\(i \\in\\{1,2,\\dots,n\\}\\) be an exchangeable sequence of random variables. Then, there exists a parameter space \\(\\Theta\\) and a prior distribution \\(p(\\theta)\\) on \\(\\Theta\\) such that the joint distribution of \\(Y_1,\\dots,Y_n\\) can be represented as \\[\np(y_1,\\dots,y_n) = \\int_{\\Theta} \\left\\{\\prod_{i=1}^n p_Y(y_i \\mid \\theta)\\right\\} p(\\theta) d\\theta,\n\\] where \\(p_Y(y \\mid \\theta)\\) is a probability density function on \\(\\mathcal{Y}\\) for each \\(\\theta \\in \\Theta\\). The prior and sampling model depend on the form of the belief model \\(p(y_1,\\dots,y_n)\\).\n\nThe probability distribution \\(p(\\theta)\\) represents our belief about the outcomes \\(\\{Y_1,Y_2,\\dots,Y_n\\}\\), induced by our belief model \\(p(y_1,\\dots,y_n)\\). That is,\n\n\\(p(\\theta)\\) represents our belief about \\(\\lim_{n\\to\\infty} \\sum Y_i/n\\) in the binary sense\n\\(p(\\theta)\\) represents our belief about \\(\\lim_{n\\to\\infty} \\sum (Y_i\\le c)/n\\) for each \\(c\\) in the general case.\n\nThe main idea of this and the previous section is as follows \\[\n\\begin{aligned}\nY_1, \\ldots, Y_n \\mid \\theta &\\stackrel{\\text{i.i.d.}}{\\sim} p(\\cdot \\mid \\theta), \\\\\n\\theta &\\sim p(\\theta)\n\\end{aligned}\n\\quad \\Longleftrightarrow \\quad\nY_1, \\ldots, Y_n \\text{ are exchangeable for all } n .\n\\]\nQuestion: When is the condition of ‚Äúexchangeability for all \\(n\\)‚Äù reasonable?\n\nHave exchaneability and repeatability\n\nExchangeability holds if the labels convey no information\nrepeatability hold includes the follows\n\n\\(Y_1,\\dots,Y_n\\) are outcomes of a repeartable experiment\n\\(Y_1,\\dots,Y_n\\) are sampled from a finite population with replacement\n\\(Y_1,\\dots,Y_n\\) are sampled from an infinite population without replacement.\n\n\n\n\n\n\n\n\n\nNoteIn large finite population\n\n\n\nNote, if \\(Y_1,\\dots,Y_n\\) are exchangeable and sampled from a finite population of size \\(N\\) that is way bigger than \\(n\\) without replacement, then they can be modelled as approximate being conditional i.i.d.\n\n\n\nThis Chapter follows closely with Chapter 2 in Hoff (2009).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Belief function and Probability Review</span>"
    ]
  },
  {
    "objectID": "02_bi-1par.html",
    "href": "02_bi-1par.html",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "",
    "text": "3.1 Three basic ingredients of Bayesian inference\nRecall the important ingredients of Bayesian inference:",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "02_bi-1par.html#three-basic-ingredients-of-bayesian-inference",
    "href": "02_bi-1par.html#three-basic-ingredients-of-bayesian-inference",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "",
    "text": "3.1.1 Prior\nThe prior distribution encodes our beliefs about the parameter \\(\\theta\\) before conduct any experiments.\n\n\n\n\n\n\nNotePrior and Data are independent\n\n\n\nNote that, the prior distribution is independent of the data. It represents our knowledge or beliefs about the parameter before seeing the data.\n\n\nHow do we choose a prior?\n\nInformative priors: Based on previous studies or expert knowledge\nWeakly informative priors: Provide some regularization without dominating the data\nNon-informative priors: Attempt to be ‚Äúobjective‚Äù (e.g., uniform, Jeffreys prior)\n\n\n\n3.1.2 Likelihood\nThe likelihood function represents the probability of observing the data given the parameter \\(\\theta\\). It can be derived from the assumed statistical model for the data or experiment, i.e., \\(y \\sim p(y \\mid \\theta)\\), or we can estimate this non-parametrically (i.e., without assuming the underlying distribution is the one we know.).\n\n\n\n\n\n\nNoteLikelihood is NOT a probability distribution for \\(\\theta\\)\n\n\n\nNote that, the likelihood function is not a probability distribution for \\(\\theta\\) itself. It is a function of \\(\\theta\\) for fixed data \\(y\\).\n\n\n\n\n3.1.3 Posterior\nThe posterior distribution combines the prior and likelihood to update our beliefs about \\(\\theta\\) after observing the data. It is given by Bayes‚Äô theorem: \\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)},\n\\] where \\(p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta\\) is the marginal likelihood or evidence.\n\n\n3.1.4 An simple example\nExamples:\n\nBeta prior + Binomial likelihood ‚Üí Beta posterior\nNormal prior + Normal likelihood (known variance) ‚Üí Normal posterior\nGamma prior + Poisson likelihood ‚Üí Gamma posterior\n\nAdvantages: - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\nLimitations:\n\nMay not reflect true prior beliefs\nModern computing makes non-conjugate priors feasible\n\n\nLet‚Äôs look a simple example to illustrate the convenience of conjugate priors. Consider a Binomial model with unknown success probability \\(\\theta\\) and known number of trials \\(n\\). We can use a Beta prior for \\(\\theta\\).\nSuppose we have a Binomial model with known number of trials \\(n\\) and unknown success probability \\(\\theta\\). We can use a Beta prior for \\(\\theta\\).\n\nPrior: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\nLikelihood: \\(y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)\\)\n\nThe derivation of the posterior is as follows:\n\\[\n\\begin{aligned}\np(y \\mid \\theta) & = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\\\\np(\\theta) & = \\frac{\\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)},\n\\end{aligned}\n\\] where \\(B(\\alpha, \\beta)\\) is the Beta function. Then the posterior is proportional to: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta) \\propto \\theta^{y + \\alpha - 1} (1 -  \\theta)^{n - y + \\beta - 1}.\n\\] This is the kernel of a Beta distribution with parameters \\((\\alpha + y, \\beta + n - y)\\). Thus, the posterior distribution is: \\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta\n+ n - y).\n\\]\nThus, the Posterior is \\(\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "02_bi-1par.html#happiness-data-the-first-example-of-bayesian-inference-procedure",
    "href": "02_bi-1par.html#happiness-data-the-first-example-of-bayesian-inference-procedure",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.2 Happiness Data ‚Äì the first example of Bayesian inference procedure",
    "text": "3.2 Happiness Data ‚Äì the first example of Bayesian inference procedure\nWe study Bayesian inference for a binomial proportion \\(\\theta\\) when the sample size \\(n\\) is fixed. In this example, we want to see what is the procedure of doing Bayesian inference\n\nIn the 1998 General Social Survey, each female respondent aged 65 or over was asked whether she was generally happy.\nDefine the response variable \\[\nY_i =\n\\begin{cases}\n1, & \\text{if respondent } i \\text{ reports being generally happy},\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\qquad i = 1,\\ldots,n,\n\\] where \\(n = 129\\).\nBecause we lack information that distinguishes individuals, it is reasonable to treat the responses as exchangeable.\nThat is, before observing the data, the labels or ordering of respondents carry no information.\nSince the sample size \\(n\\) is small relative to the population size \\(N\\) of senior women, results from the previous chapter justify the following modeling approximation.\nModeling Assumptions: Our beliefs about \\((Y_1,\\ldots,Y_{129})\\) are described by:\n\nAn unknown population proportion \\[\n\\theta = \\frac{1}{N}\\sum_{i=1}^N Y_i,\n\\] where \\(\\theta\\) represents the proportion of generally happy individuals in the population.\nA sampling model given \\(\\theta\\)\nConditional on \\(\\theta\\), the responses \\(Y_1,\\ldots,Y_{129}\\) are independent and identically distributed Bernoulli random variables with \\[\n\\Pr(Y_i = 1 \\mid \\theta) = \\theta.\n\\]\n\n\nGiven the population proportion \\(\\theta\\), each respondent independently reports being happy with probability \\(\\theta\\).\n\nLikelihood: Under this model, the probability of observing data \\(\\{y_1,\\ldots,y_{129}\\}\\) given \\(\\theta\\) is \\[\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{\\sum_{i=1}^{129} y_i}\n(1-\\theta)^{129-\\sum_{i=1}^{129} y_i}.\n\\]\nThis expression depends on the data only through the sufficient statistic \\[\nS = \\sum_{i=1}^{129} Y_i,\n\\] the total number of respondents who report being generally happy.\nFor the happiness data, \\[\nS = 118,\n\\] so the likelihood simplifies to \\[\np(y_1,\\ldots,y_{129} \\mid \\theta)\n=\n\\theta^{118}(1-\\theta)^{11}.\n\\]\nQ: Which prior to be used?\nA prior distribution is conjugate to a likelihood if the posterior distribution belongs to the same family as the prior. For the binomial likelihood, the Beta distribution is conjugate. But we have another choice of prior, to use non-informative prior.\nA Uniform Prior Distribution: Suppose our prior information about \\(\\theta\\) is very weak, in the sense that all subintervals of \\([0,1]\\) with equal length are equally plausible.\nSymbolically, for any \\(0 \\le a &lt; b &lt; b+c \\le 1\\), \\[\n\\Pr(a \\le \\theta \\le b)\n=\n\\Pr(a+c \\le \\theta \\le b+c).\n\\]\nThis implies a uniform prior: \\[\np(\\theta) = 1, \\qquad 0 \\le \\theta \\le 1.\n\\]\nPosterior Distribution: Bayes‚Äô rule gives \\[\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{p(y_1,\\ldots,y_{129} \\mid \\theta)\\,p(\\theta)}\n     {p(y_1,\\ldots,y_{129})}.\n\\]\nWith a uniform prior, this reduces to \\[\np(\\theta \\mid y_1,\\ldots,y_{129})\n\\propto\n\\theta^{118}(1-\\theta)^{11}.\n\\]\n\nKey idea: with a uniform prior, the posterior has the same shape as the likelihood.\n\nTo obtain a proper probability distribution, we must normalize.\nNormalizing Constant and the Beta Distribution: Using the identity \\[\n\\int_0^1 \\theta^{a-1}(1-\\theta)^{b-1}\\,d\\theta\n=\n\\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\n\\] we find \\[\np(y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(119)\\Gamma(12)}{\\Gamma(131)}.\n\\]\nTherefore, the posterior density is \\[\np(\\theta \\mid y_1,\\ldots,y_{129})\n=\n\\frac{\\Gamma(131)}{\\Gamma(119)\\Gamma(12)}\n\\theta^{119-1}(1-\\theta)^{12-1}.\n\\]\nThat is, \\[\n\\theta \\mid y \\sim \\mathrm{Beta}(119,\\,12).\n\\]\nRecall that, a random variable \\(\\theta \\sim \\mathrm{Beta}(a,b)\\) distribution if \\[\np(\\theta)\n=\n\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\n\\theta^{a-1}(1-\\theta)^{b-1}.\n\\]\nFor \\(\\theta \\sim \\mathrm{Beta}(a,b)\\), the expectation (i.e., mean or the first moment) is \\(\\mathbb{E}(\\theta) = \\frac{a}{a+b}\\), and the variance is \\(\\mathrm{Var}(\\theta)=\\frac{ab}{(a+b)^2(a+b+1)}.\\)\nIn our example, the happiness data, the posterior distribution is \\[\n\\theta \\mid y \\sim \\mathrm{Beta}(119,12).\n\\]\nThus, the posterior mean is \\(\\mathbb{E}(\\theta \\mid y) = 0.915\\), and the posterior standard deviation is \\(\\mathrm{sd}(\\theta \\mid y) = 0.025\\).\nThese summaries quantify both our best estimate of the population proportion and our remaining uncertainty after observing the data.\n\n\n3.2.1 Inference about exchangeable binary data\nPosterior Inference under a Uniform Prior\nSuppose \\(Y_1, \\ldots, Y_n \\mid \\theta \\stackrel{\\text{i.i.d.}}{\\sim} \\text{Bernoulli}(\\theta)\\), and we place a uniform prior on \\(\\theta\\). The posterior distribution of \\(\\theta\\) given the observed data \\(y_1, \\ldots, y_n\\) is proportional to \\[\n\\begin{aligned}\np(\\theta \\mid y_1, \\ldots, y_n)\n&= \\frac{p(y_1, \\ldots, y_n \\mid \\theta) p(\\theta)}{p(y_1, \\ldots, y_n)} \\\\\n&= \\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i} \\times \\frac{p(\\theta)}{p(y_1, \\ldots, y_n)}\\\\\n&\\propto\n\\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i}.\n\\end{aligned}\n\\]\nConsider two parameter values \\(\\theta_a\\) and \\(\\theta_b\\). The ratio of their posterior densities is \\[\n\\begin{aligned}\n\\frac{p(\\theta_a \\mid y_1, \\ldots, y_n)}\n     {p(\\theta_b \\mid y_1, \\ldots, y_n)}\n&=\\frac{\\theta_a^{\\sum y_i}\\left(1-\\theta_a\\right)^{n-\\sum y_i} \\times p\\left(\\theta_a\\right) / p\\left(y_1, \\ldots, y_n\\right)}{\\theta_b^{\\sum y_i}\\left(1-\\theta_b\\right)^{n-\\sum y_i} \\times p\\left(\\theta_b\\right) / p\\left(y_1, \\ldots, y_n\\right)} \\\\\n&=\n\\left(\\frac{\\theta_a}{\\theta_b}\\right)^{\\sum_i y_i}\n\\left(\\frac{1 - \\theta_a}{1 - \\theta_b}\\right)^{n - \\sum_i y_i}\n\\frac{p(\\theta_a)}{p(\\theta_b)}.\n\\end{aligned}\n\\]\nThis expression shows that the data affect the posterior distribution only through the sum of the data \\(\\sum_{i=1}^n y_i\\) based on the relative probability density at \\(\\theta_a\\) to \\(\\theta_b\\).\nAs a result, for any set \\(A\\), one can show that \\[\n\\Pr(\\theta \\in A \\mid Y_1 = y_1, \\ldots, Y_n = y_n)\n=\n\\Pr\\left(\\theta \\in A \\mid \\sum_{i=1}^n Y_i = \\sum_{i=1}^n y_i\\right).\n\\]\nThis means that \\(\\sum_{i=1}^n Y_i\\) contains all the information in the data relevant for inference about \\(\\theta\\). We therefore say that \\(Y = \\sum_{i=1}^n Y_i\\) is a sufficient statistic for \\(\\theta\\). The term sufficient is used because knowing \\(\\sum_{i=1}^n Y_i\\) is sufficient to carry out inference about \\(\\theta\\); no additional information from the individual observations \\(Y_1, \\ldots, Y_n\\) is required.\nIn the case where \\(Y_1, \\ldots, Y_n \\mid \\theta\\) are i.i.d. Bernoulli\\((\\theta)\\) random variables, the sufficient statistic \\(Y = \\sum_{i=1}^n Y_i\\) follows a binomial distribution with parameters \\((n, \\theta)\\).\nThe Binomial Model\nBecause each \\(Y_i\\) is Bernoulli\\((\\theta)\\) and the observations are independent, the sufficient statistic \\(Y = \\sum_{i=1}^n Y_i\\) follows a binomial distribution with parameters \\((n, \\theta)\\).\nThat is, \\(\\Pr(Y = y \\mid \\theta)=\\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}, \\qquad y = 0, 1, \\ldots, n.\\) For a binomial\\((n, \\theta)\\) random variable \\(Y\\),\n\n\\(\\mathbb{E}[Y \\mid \\theta] = n\\theta\\),\n\\(\\mathrm{Var}(Y \\mid \\theta) = n\\theta(1 - \\theta).\\)\n\n\n\n\n\n\n\n\n\n\nPosterior inference under a uniform prior distribution\nHaving observed \\(Y = y\\) our task is to obtain the posterior distribution of \\(\\theta\\). By Bayes‚Äô theorem, \\[\np(\\theta \\mid y)\n= \\frac{p(y \\mid \\theta),p(\\theta)}{p(y)}.\n\\]\nFor a binomial model with \\(Y \\sim \\text{Binomial}(n,\\theta)\\), the likelihood is \\[\np(y \\mid \\theta) = \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}.\n\\]\nTherefore, \\[\np(\\theta \\mid y)\n= \\frac{\\binom{n}{y} \\theta^y(1-\\theta)^{n-y}p(\\theta)}{p(y)}\n= c(y) \\theta^y(1-\\theta)^{n-y}p(\\theta),\n\\] where \\(c(y)\\) is a normalizing constant that depends only on \\(y\\), not on \\(\\theta\\).\n\n\n3.2.2 Confidence regions",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "02_bi-1par.html#poisson-distribution",
    "href": "02_bi-1par.html#poisson-distribution",
    "title": "3¬† Bayesian Inference for single parameter models",
    "section": "3.3 Poisson distribution",
    "text": "3.3 Poisson distribution\n\nThis Chapter follows closely with Chapter 3 in Hoff (2009).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Bayesian Inference for single parameter models</span>"
    ]
  },
  {
    "objectID": "week01.html",
    "href": "week01.html",
    "title": "4¬† Week 1 ‚Äî Introduction and Bayesian Thinking",
    "section": "",
    "text": "4.1 Introduction to Bayesian Inference\nBayesian inference is based on a simple principle: the posterior distribution (our updated beliefs after observing data) is obtained from the prior distribution (our initial beliefs) and the sampling model (how data are generated) via Bayes‚Äô rule:\n\\[p(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{\\int_{\\Theta} p(y \\mid \\theta') p(\\theta') d \\theta'}\\]\nThis elegant formula is the foundation of all Bayesian inference. It tells us how to update our beliefs in light of new evidence.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Week 1 ‚Äî Introduction and Bayesian Thinking</span>"
    ]
  },
  {
    "objectID": "week01.html#foundational-concepts",
    "href": "week01.html#foundational-concepts",
    "title": "4¬† Week 1 ‚Äî Introduction and Bayesian Thinking",
    "section": "4.2 Foundational Concepts",
    "text": "4.2 Foundational Concepts\n\n4.2.1 Why Use Bayesian Methods?\nProbability as Uncertainty: The Bayesian framework treats probability as a measure of uncertainty about unknown quantities, not just long-run frequencies. This allows direct probability statements about parameters given observed data.\nIncorporates Prior Knowledge: Bayesian methods naturally combine prior information (from expert judgment, previous studies, or domain knowledge) with observed data. This is particularly valuable in: - Medical research where historical trials exist - Engineering where physical constraints are known - Sequential analysis where data arrive over time\nDirect Inference: Bayesian inference answers the questions researchers actually ask: - ‚ÄúWhat is the probability that treatment B is better than A given my data?‚Äù - ‚ÄúWhat is a plausible range for this parameter?‚Äù - Rather than ‚ÄúIf the null hypothesis were true, what is the probability of observing this data?‚Äù\nFlexible Modeling: Complex models with multiple parameters, hierarchical structures, or missing data are more naturally expressed in Bayesian frameworks.\nBetter Small-Sample Performance: With limited data, informative priors can stabilize estimates and provide more stable inference than frequentist methods.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Week 1 ‚Äî Introduction and Bayesian Thinking</span>"
    ]
  },
  {
    "objectID": "week01.html#bayesian-vs.-frequentist-comparison",
    "href": "week01.html#bayesian-vs.-frequentist-comparison",
    "title": "4¬† Week 1 ‚Äî Introduction and Bayesian Thinking",
    "section": "4.3 Bayesian vs.¬†Frequentist Comparison",
    "text": "4.3 Bayesian vs.¬†Frequentist Comparison\nBoth approaches have merits and limitations. The choice depends on the problem context, available prior information, and the questions being asked.\n\n4.3.1 Motivating Examples\n\n4.3.1.1 Example 1.1: Inference for a proportion\nSuppose we are interested in estimating the rate at which a disease occurs in a population. We sample \\(n = 20\\) individuals and observe \\(y = 8\\) with the disease.\nQuestions: - What is our estimate of the disease rate \\(\\theta\\)? - How certain are we about this estimate? - How would we predict the number with disease in a future sample?\nTwo approaches:\nFrequentist approach: - Point estimate: \\(\\hat{\\theta} = y/n = 8/20 = 0.4\\) - Confidence interval based on sampling distribution - Does not directly provide \\(P(\\theta \\in [a,b] \\mid \\text{data})\\)\nBayesian approach: - Treat \\(\\theta\\) as a random variable with a prior distribution - Update beliefs using data via Bayes‚Äô theorem - Obtain posterior distribution: direct probability statements about \\(\\theta\\)\n\n\n4.3.1.2 Example 1.2: Comparing two groups\nConsider two treatment groups with success rates \\(\\theta_A\\) and \\(\\theta_B\\).\n\nGroup A: 8 successes out of 20 trials\nGroup B: 12 successes out of 20 trials\n\nQuestions: - Is treatment B better than treatment A? - What is \\(P(\\theta_B &gt; \\theta_A \\mid \\text{data})\\)? - Bayesian methods provide direct answers to such questions.\n\n\n\n4.3.2 Probability as a Measure of Uncertainty\n\nFrequentist interpretation: Probability as long-run frequency of repeated events.\nBayesian interpretation: Probability as a degree of belief or uncertainty about unknown quantities.\n\nThe Bayesian view allows us to: - Make probability statements about parameters (not just data) - Incorporate prior information naturally - Update beliefs coherently as new data arrive\n\n\n4.3.3 Building Blocks of Bayesian Inference\nFor parameter \\(\\theta\\) and observed data \\(y\\), we need three components:\n\nPrior distribution \\(p(\\theta)\\): expresses our beliefs about \\(\\theta\\) before seeing data\nLikelihood \\(p(y \\mid \\theta)\\): probability model for the data given \\(\\theta\\)\n\nPosterior distribution \\(p(\\theta \\mid y)\\): updated beliefs after seeing data\n\n\n\n4.3.4 Bayes‚Äô Theorem\nThese three components are combined via Bayes‚Äô theorem:\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta)\\, p(\\theta)}{p(y)} =\n\\frac{p(y \\mid \\theta)\\, p(\\theta)}{\\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}\n\\]\nIn words: \\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Marginal likelihood}}\\]\nKey insight: The posterior is proportional to the likelihood times the prior: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\times p(\\theta)\n\\]\nThe denominator \\(p(y) = \\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta\\) is a normalizing constant ensuring \\(\\int p(\\theta \\mid y)\\, d\\theta = 1\\).\n\n\n4.3.5 Inference from the Posterior Distribution\nOnce we obtain the posterior \\(p(\\theta \\mid y)\\), we can:\n\nPoint estimation:\n\nPosterior mean: \\(E[\\theta \\mid y]\\)\nPosterior median or mode\n\nInterval estimation:\n\nCredible intervals: \\(P(a &lt; \\theta &lt; b \\mid y) = 0.95\\)\nDirect probability statements about parameters\n\nHypothesis testing:\n\n\\(P(\\theta_B &gt; \\theta_A \\mid y)\\)\n\nPrediction:\n\nPosterior predictive distribution for future data \\(\\tilde{y}\\): \\[p(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Week 1 ‚Äî Introduction and Bayesian Thinking</span>"
    ]
  },
  {
    "objectID": "week01.html#one-parameter-models",
    "href": "week01.html#one-parameter-models",
    "title": "4¬† Week 1 ‚Äî Introduction and Bayesian Thinking",
    "section": "4.4 One-Parameter Models",
    "text": "4.4 One-Parameter Models\n\n4.4.1 The Beta-Binomial Model\n\n4.4.1.1 Setup\nConsider binary outcome data: \\(y_1, \\ldots, y_n\\) where each \\(y_i \\in \\{0, 1\\}\\).\nLet \\(y = \\sum_{i=1}^n y_i\\) be the number of successes. We model:\n\\[y \\mid \\theta \\sim \\text{Binomial}(n, \\theta)\\]\nwhere \\(\\theta\\) is the probability of success.\nLikelihood: \\[p(y \\mid \\theta) = \\binom{n}{y} \\theta^y (1-\\theta)^{n-y} \\propto \\theta^y (1-\\theta)^{n-y}\\]\n\n\n4.4.1.2 Prior Distribution\nWe use a Beta prior for \\(\\theta\\):\n\\[\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\]\nwith density: \\[p(\\theta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}, \\quad 0 &lt; \\theta &lt; 1\\]\nPrior properties: - \\(E[\\theta] = \\frac{\\alpha}{\\alpha + \\beta}\\) - \\(\\text{Mode}[\\theta] = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}\\) (if \\(\\alpha, \\beta &gt; 1\\)) - \\(\\text{Var}[\\theta] = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\nInterpretation: Think of \\(\\alpha\\) as prior successes and \\(\\beta\\) as prior failures.\n\n\n4.4.1.3 Posterior Distribution\nBy Bayes‚Äô theorem: \\[\\begin{align}\np(\\theta \\mid y) &\\propto p(y \\mid \\theta) \\times p(\\theta) \\\\\n&\\propto \\theta^y (1-\\theta)^{n-y} \\times \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\\\\n&= \\theta^{y + \\alpha - 1}(1-\\theta)^{n - y + \\beta - 1}\n\\end{align}\\]\nThis is the kernel of a \\(\\text{Beta}(\\alpha + y, \\beta + n - y)\\) distribution.\nPosterior: \\[\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\]\nPosterior mean: \\[E[\\theta \\mid y] = \\frac{\\alpha + y}{\\alpha + \\beta + n}\\]\nThis is a weighted average of the prior mean \\(\\frac{\\alpha}{\\alpha+\\beta}\\) and the sample proportion \\(\\frac{y}{n}\\).\n\n\n4.4.1.4 Example 2.1: Disease Rate\nReturn to the disease rate example: \\(n = 20\\), \\(y = 8\\).\nSuppose we use a weakly informative prior: \\(\\theta \\sim \\text{Beta}(2, 2)\\) (prior mean = 0.5).\nPosterior: \\(\\theta \\mid y \\sim \\text{Beta}(10, 14)\\)\nPosterior mean: \\(E[\\theta \\mid y] = \\frac{10}{24} = 0.417\\)\n95% credible interval: We can compute quantiles of \\(\\text{Beta}(10, 14)\\) to get a 95% interval for \\(\\theta\\).\n\n\n\n\n4.4.2 The Normal Model with Known Variance\n\n4.4.2.1 Setup\nSuppose we observe data \\(y_1, \\ldots, y_n\\) that are i.i.d. from:\n\\[y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)\\]\nwhere \\(\\theta\\) is the unknown mean and \\(\\sigma^2\\) is a known variance.\nLikelihood: For the sample mean \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i\\): \\[\\bar{y} \\mid \\theta \\sim \\mathcal{N}\\left(\\theta, \\frac{\\sigma^2}{n}\\right)\\]\nThe likelihood is: \\[p(y_1, \\ldots, y_n \\mid \\theta) \\propto \\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\theta)^2\\right\\}\\]\n\n\n4.4.2.2 Prior Distribution\nWe use a Normal prior for \\(\\theta\\):\n\\[\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)\\]\nwith density: \\[p(\\theta) \\propto \\exp\\left\\{-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right\\}\\]\n\n\n4.4.2.3 Posterior Distribution\nBy Bayes‚Äô theorem: \\[\\begin{align}\np(\\theta \\mid y) &\\propto p(y \\mid \\theta) \\times p(\\theta) \\\\\n&\\propto \\exp\\left\\{-\\frac{n}{2\\sigma^2}(\\bar{y} - \\theta)^2\\right\\} \\times \\exp\\left\\{-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right\\}\n\\end{align}\\]\nAfter completing the square, we obtain:\n\\[\\theta \\mid y \\sim \\mathcal{N}(\\mu_n, \\tau_n^2)\\]\nwhere: \\[\n\\tau_n^2 = \\left(\\frac{1}{\\tau_0^2} + \\frac{n}{\\sigma^2}\\right)^{-1} = \\frac{1}{\\text{prior precision} + \\text{data precision}}\n\\]\n\\[\n\\mu_n = \\tau_n^2\\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{n\\bar{y}}{\\sigma^2}\\right)\n\\]\nAlternative form: \\[\\mu_n = w \\mu_0 + (1-w)\\bar{y}\\]\nwhere \\(w = \\frac{\\sigma^2/n}{\\sigma^2/n + \\tau_0^2}\\) is the weight on the prior mean.\n\n\n4.4.2.4 Interpretation\n\nThe posterior mean is a weighted average of the prior mean and the sample mean\nAs \\(n \\to \\infty\\), the posterior mean converges to \\(\\bar{y}\\) (data dominate)\nWith little data, the posterior is pulled toward the prior\nThe posterior precision is the sum of the prior and data precisions\n\n\n\n\n\n4.4.3 Posterior Predictive Distribution\nAfter observing data \\(y = (y_1, \\ldots, y_n)\\), we often want to predict a future observation \\(\\tilde{y}\\).\nThe posterior predictive distribution is:\n\\[p(\\tilde{y} \\mid y) = \\int p(\\tilde{y} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta\\]\nThis averages the conditional distribution of \\(\\tilde{y}\\) given \\(\\theta\\) over the posterior uncertainty in \\(\\theta\\).\n\n4.4.3.1 Example: Beta-Binomial\nFor the beta-binomial model with \\(\\theta \\mid y \\sim \\text{Beta}(\\alpha + y, \\beta + n - y)\\):\n\\[P(\\tilde{y} = 1 \\mid y) = E[\\theta \\mid y] = \\frac{\\alpha + y}{\\alpha + \\beta + n}\\]\n\n\n4.4.3.2 Example: Normal Model\nFor the normal model with known variance, if \\(\\theta \\mid y \\sim \\mathcal{N}(\\mu_n, \\tau_n^2)\\) and \\(\\tilde{y} \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)\\):\n\\[\\tilde{y} \\mid y \\sim \\mathcal{N}(\\mu_n, \\tau_n^2 + \\sigma^2)\\]\nThe predictive variance includes both parameter uncertainty (\\(\\tau_n^2\\)) and sampling variability (\\(\\sigma^2\\)).\n\n\n\n\n4.4.4 Conjugate Priors\nDefinition: A prior distribution is conjugate to a likelihood if the posterior distribution is in the same family as the prior.\nExamples: - Beta prior + Binomial likelihood ‚Üí Beta posterior - Normal prior + Normal likelihood (known variance) ‚Üí Normal posterior - Gamma prior + Poisson likelihood ‚Üí Gamma posterior\nAdvantages: - Analytical posteriors (no numerical integration needed) - Interpretable parameters - Computationally efficient\nLimitations: - May not reflect true prior beliefs - Modern computing makes non-conjugate priors feasible\n\n\n\n4.4.5 Practical Considerations\n\n4.4.5.1 Prior Elicitation\nHow do we choose a prior?\n\nInformative priors: Based on previous studies or expert knowledge\nWeakly informative priors: Provide some regularization without dominating the data\nNon-informative priors: Attempt to be ‚Äúobjective‚Äù (e.g., uniform, Jeffreys prior)\n\n\n\n4.4.5.2 Sensitivity Analysis\n\nTry different priors and check if conclusions change substantially\nIf posterior is sensitive to prior choice with substantial data, investigate further\n\n\n\n4.4.5.3 Comparing Bayesian and Frequentist Inference\n\n\n\n\n\n\n\n\nAspect\nBayesian\nFrequentist\n\n\n\n\nParameters\nRandom variables with distributions\nFixed unknown constants\n\n\nProbability statements\nDirect: \\(P(\\theta \\in [a,b] \\mid y)\\)\nIndirect: confidence intervals\n\n\nPrior information\nNaturally incorporated\nDifficult to include\n\n\nSmall samples\nCan be more stable\nMay have poor properties\n\n\nInterpretation\nConditional on observed data\nBased on repeated sampling\n\n\n\n\n\n\n\n4.4.6 R Examples\n\n4.4.6.1 Example 3.1: Beta-Binomial Model\n\n# Data\nn &lt;- 20\ny &lt;- 8\n\n# Prior: Beta(2, 2)\nalpha0 &lt;- 2\nbeta0 &lt;- 2\n\n# Posterior: Beta(10, 14)\nalpha1 &lt;- alpha0 + y\nbeta1 &lt;- beta0 + n - y\n\n# Grid for plotting\ntheta &lt;- seq(0, 1, length.out = 500)\n\n# Plot prior and posterior\nplot(theta, dbeta(theta, alpha0, beta0), type = \"l\", lwd = 2, col = \"blue\",\n     ylab = \"Density\", xlab = expression(theta),\n     main = \"Beta-Binomial Model: Prior and Posterior\")\nlines(theta, dbeta(theta, alpha1, beta1), col = \"red\", lwd = 2)\nabline(v = y/n, lty = 2, col = \"gray\")\n\nlegend(\"topright\",\n       legend = c(\"Prior Beta(2,2)\", \"Posterior Beta(10,14)\", \"MLE\"),\n       col = c(\"blue\", \"red\", \"gray\"), lwd = c(2, 2, 1), lty = c(1, 1, 2))\n\n\n\n\nPrior, Likelihood, and Posterior for Beta-Binomial Model\n\n\n\n# Posterior summary\ncat(\"Posterior mean:\", alpha1/(alpha1 + beta1), \"\\n\")\n\nPosterior mean: 0.4166667 \n\ncat(\"95% credible interval:\", qbeta(c(0.025, 0.975), alpha1, beta1), \"\\n\")\n\n95% credible interval: 0.2319142 0.614581 \n\n\n\n\n4.4.6.2 Example 3.2: Normal Model\n\n# Data\ny &lt;- c(1.2, 0.8, 1.5, 1.1, 0.9)\nn &lt;- length(y)\nybar &lt;- mean(y)\nsigma2 &lt;- 0.25  # known variance\n\n# Prior: N(0, 1)\nmu0 &lt;- 0\ntau0_sq &lt;- 1\n\n# Posterior\ntau_n_sq &lt;- 1 / (1/tau0_sq + n/sigma2)\nmu_n &lt;- tau_n_sq * (mu0/tau0_sq + n*ybar/sigma2)\n\ncat(\"Posterior: N(\", round(mu_n, 3), \",\", round(tau_n_sq, 3), \")\\n\")\n\nPosterior: N( 1.048 , 0.048 )\n\n# Plot\ntheta &lt;- seq(-1, 3, length.out = 500)\nplot(theta, dnorm(theta, mu0, sqrt(tau0_sq)), type = \"l\", lwd = 2, col = \"blue\",\n     ylab = \"Density\", xlab = expression(theta),\n     main = \"Normal Model: Prior and Posterior\")\nlines(theta, dnorm(theta, mu_n, sqrt(tau_n_sq)), col = \"red\", lwd = 2)\nabline(v = ybar, lty = 2, col = \"gray\")\n\nlegend(\"topright\",\n       legend = c(\"Prior\", \"Posterior\", \"Sample mean\"),\n       col = c(\"blue\", \"red\", \"gray\"), lwd = c(2, 2, 1), lty = c(1, 1, 2))\n\n\n\n\nPrior, Likelihood, and Posterior for Normal Model",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Week 1 ‚Äî Introduction and Bayesian Thinking</span>"
    ]
  },
  {
    "objectID": "week02.html",
    "href": "week02.html",
    "title": "5¬† Week 2 ‚Äî Conjugate Priors and Analytical Posteriors",
    "section": "",
    "text": "5.1 Overview\nThis week focuses on conjugate priors ‚Äî special priors that yield posteriors in the same family of distributions as the prior.\nStudents will learn why conjugacy simplifies Bayesian inference, how to identify conjugate pairs for common likelihoods, and how to perform analytical posterior updates without simulation.\nWe will also introduce the concept of prior sensitivity analysis and noninformative (objective) priors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Week 2 ‚Äî Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week02.html#learning-goals",
    "href": "week02.html#learning-goals",
    "title": "5¬† Week 2 ‚Äî Conjugate Priors and Analytical Posteriors",
    "section": "5.2 Learning Goals",
    "text": "5.2 Learning Goals\nBy the end of Week 2, you should be able to:\n\nDefine and identify conjugate priors for standard likelihood models.\n\nDerive analytical posteriors for Binomial, Poisson, and Normal models.\n\nCompute posterior summaries and predictive distributions.\n\nDiscuss the influence of priors on posterior inference.\n\nPerform prior sensitivity analysis in R.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Week 2 ‚Äî Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week02.html#lecture-1-the-concept-of-conjugacy",
    "href": "week02.html#lecture-1-the-concept-of-conjugacy",
    "title": "5¬† Week 2 ‚Äî Conjugate Priors and Analytical Posteriors",
    "section": "5.3 Lecture 1: The Concept of Conjugacy",
    "text": "5.3 Lecture 1: The Concept of Conjugacy\n\n5.3.1 1.1 Definition\nA conjugate prior for a likelihood \\(p(y \\mid \\theta)\\) is a prior distribution \\(p(\\theta)\\) such that the posterior \\(p(\\theta \\mid y)\\) belongs to the same family as the prior.\nFormally: \\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta)\\, p(\\theta)\n\\] If \\(p(\\theta \\mid y)\\) has the same functional form as \\(p(\\theta)\\), then \\(p(\\theta)\\) is conjugate to the likelihood.\n\n\n5.3.2 1.2 Why Conjugacy Matters\n\nProvides closed-form expressions for posterior means, variances, and credible intervals.\n\nFacilitates sequential updating ‚Äî easy to update priors as new data arrive.\n\nUseful for educational and analytic illustration before moving to MCMC methods.\n\n\n\n5.3.3 1.3 Examples of Conjugate Pairs\n\n\n\n\n\n\n\n\nLikelihood\nConjugate Prior\nPosterior Family\n\n\n\n\nBinomial\\((n,\\theta)\\)\nBeta\\((\\alpha,\\beta)\\)\nBeta\\((\\alpha+y, \\beta+n-y)\\)\n\n\nPoisson\\((\\lambda)\\)\nGamma\\((a,b)\\)\nGamma\\((a+\\sum y_i, b+n)\\)\n\n\nNormal\\((\\mu,\\sigma^2)\\) (known variance)\nNormal\\((\\mu_0,\\tau_0^2)\\)\nNormal\\((\\mu_1,\\tau_1^2)\\)\n\n\nExponential\\((\\lambda)\\)\nGamma\\((a,b)\\)\nGamma\\((a+n, b+\\sum y_i)\\)\n\n\nNormal mean/variance (unknown \\(\\sigma^2\\))\nNormal‚ÄìInverse-Gamma\nNormal‚ÄìInverse-Gamma",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Week 2 ‚Äî Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week02.html#lecture-2-betabinomial-and-gammapoisson-models",
    "href": "week02.html#lecture-2-betabinomial-and-gammapoisson-models",
    "title": "5¬† Week 2 ‚Äî Conjugate Priors and Analytical Posteriors",
    "section": "5.4 Lecture 2: Beta‚ÄìBinomial and Gamma‚ÄìPoisson Models",
    "text": "5.4 Lecture 2: Beta‚ÄìBinomial and Gamma‚ÄìPoisson Models\n\n5.4.1 2.1 Beta‚ÄìBinomial Model (Review and Generalization)\nLet \\(y \\mid \\theta \\sim \\text{Binomial}(n,\\theta)\\) and \\(\\theta \\sim \\text{Beta}(\\alpha_0,\\beta_0)\\).\nThen the posterior is: \\[\n\\theta \\mid y \\sim \\text{Beta}(\\alpha_0 + y, \\beta_0 + n - y).\n\\]\nPosterior Mean: \\[\nE[\\theta \\mid y] = \\frac{\\alpha_0 + y}{\\alpha_0 + \\beta_0 + n}.\n\\]\nPredictive Probability for a Future Success: \\[\np(\\tilde{y}=1 \\mid y) = E[\\theta \\mid y].\n\\]\nInterpretation:\nEach observation updates the Beta prior by adding one success or failure to the corresponding shape parameter.\n\n\n\n5.4.2 2.2 Gamma‚ÄìPoisson Model (Counts)\nSuppose we model count data as \\(y_i \\sim \\text{Poisson}(\\lambda)\\), with prior \\(\\lambda \\sim \\text{Gamma}(a_0, b_0)\\)\n(where the Gamma density is parameterized as \\(p(\\lambda) \\propto \\lambda^{a_0-1} e^{-b_0\\lambda}\\)).\nPosterior: \\[\n\\lambda \\mid y_1,\\ldots,y_n \\sim \\text{Gamma}\\left(a_0 + \\sum_{i=1}^n y_i,\\; b_0 + n\\right).\n\\]\nPosterior Mean and Variance: \\[\nE[\\lambda \\mid y] = \\frac{a_0 + \\sum y_i}{b_0 + n}, \\quad\n\\text{Var}[\\lambda \\mid y] = \\frac{a_0 + \\sum y_i}{(b_0 + n)^2}.\n\\]\nPosterior Predictive: \\[\np(\\tilde{y} \\mid y) = \\int \\text{Poisson}(\\tilde{y} \\mid \\lambda)\\, p(\\lambda \\mid y)\\, d\\lambda,\n\\] which follows a Negative Binomial distribution.\nInterpretation:\nThe Gamma prior acts as if we had observed \\(a_0-1\\) pseudo-events over \\(b_0\\) pseudo-trials.\n\n\n\n5.4.3 2.3 R Example: Gamma‚ÄìPoisson Updating\n\n# Posterior update for Gamma-Poisson model\ny &lt;- c(3, 2, 4, 1, 0, 2, 3)\na0 &lt;- 2; b0 &lt;- 1   # prior Gamma(2,1)\nn &lt;- length(y)\n\na1 &lt;- a0 + sum(y)\nb1 &lt;- b0 + n\n\nlambda &lt;- seq(0, 10, length.out = 400)\nprior &lt;- dgamma(lambda, a0, b0)\nposterior &lt;- dgamma(lambda, a1, b1)\n\nplot(lambda, prior, type=\"l\", lwd=2, col=\"blue\", ylim=c(0, max(posterior)),\n     ylab=\"Density\", xlab=expression(lambda),\n     main=\"Gamma-Poisson Updating\")\nlines(lambda, posterior, col=\"red\", lwd=2)\nlegend(\"topright\",\n       legend=c(\"Prior Gamma(2,1)\", paste0(\"Posterior Gamma(\", a1, \",\", b1, \")\")),\n       col=c(\"blue\", \"red\"), lwd=2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Week 2 ‚Äî Conjugate Priors and Analytical Posteriors</span>"
    ]
  },
  {
    "objectID": "week03.html",
    "href": "week03.html",
    "title": "6¬† Week 3 ‚Äî Monte Carlo Integration and Simulation-Based Bayesian Inference",
    "section": "",
    "text": "6.1 Overview\nThis week introduces Monte Carlo methods, which allow us to approximate Bayesian quantities when analytical solutions are unavailable.\nWe explore how random sampling can be used to estimate expectations, posterior summaries, and probabilities.\nBy the end of this week, students will understand how Monte Carlo simulation forms the foundation for modern Bayesian computation such as MCMC.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Week 3 ‚Äî Monte Carlo Integration and Simulation-Based Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "week03.html#learning-goals",
    "href": "week03.html#learning-goals",
    "title": "6¬† Week 3 ‚Äî Monte Carlo Integration and Simulation-Based Bayesian Inference",
    "section": "6.2 Learning Goals",
    "text": "6.2 Learning Goals\nBy the end of Week 3, you should be able to:\n\nExplain the motivation for Monte Carlo methods in Bayesian inference.\n\nApproximate expectations, integrals, and posterior summaries using random sampling.\n\nImplement crude Monte Carlo and importance sampling in R.\n\nAssess the accuracy and variance of Monte Carlo estimators.\n\nInterpret Monte Carlo errors and convergence diagnostics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Week 3 ‚Äî Monte Carlo Integration and Simulation-Based Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "week03.html#lecture-1-motivation-and-fundamentals-of-monte-carlo",
    "href": "week03.html#lecture-1-motivation-and-fundamentals-of-monte-carlo",
    "title": "6¬† Week 3 ‚Äî Monte Carlo Integration and Simulation-Based Bayesian Inference",
    "section": "6.3 Lecture 1: Motivation and Fundamentals of Monte Carlo",
    "text": "6.3 Lecture 1: Motivation and Fundamentals of Monte Carlo\n\n6.3.1 1.1 The Problem\nBayesian inference often requires evaluating integrals such as: \\[\nE[h(\\theta) \\mid y] = \\int h(\\theta)\\, p(\\theta \\mid y)\\, d\\theta,\n\\] which are rarely available in closed form.\n\n\n6.3.2 1.2 Monte Carlo Idea\nIf we can sample \\(\\theta^{(1)}, \\ldots, \\theta^{(M)}\\) from the posterior \\(p(\\theta \\mid y)\\),\nthen we can approximate the expectation by: \\[\n\\hat{E}[h(\\theta)] = \\frac{1}{M} \\sum_{m=1}^M h(\\theta^{(m)}).\n\\] This is called the Monte Carlo estimator.\nBy the Law of Large Numbers, \\(\\hat{E}[h(\\theta)] \\to E[h(\\theta)]\\) as \\(M \\to \\infty\\). The Central Limit Theorem gives: \\[\n\\sqrt{M}\\,(\\hat{E} - E[h(\\theta)]) \\approx N(0, \\text{Var}[h(\\theta)]).\n\\]\n\n\n6.3.3 1.3 Monte Carlo Error\nWe can estimate the simulation error by:\n\\[\n\\text{SE}(\\hat{E}) \\approx \\sqrt{\\frac{\\text{Var}(h(\\theta))}{M}}.\n\\] Larger \\(M\\) gives more accurate approximations but increases computation time.\n\n\n6.3.4 1.4 Simple Example\nCompute \\(E[\\theta]\\) for \\(\\theta \\sim \\text{Beta}(2,5)\\) using Monte Carlo.\n\nset.seed(1)\nM &lt;- 1e5\ntheta &lt;- rbeta(M, 2, 5)\nmean(theta)          # Monte Carlo estimate\n\n[1] 0.2861808\n\nvar(theta) / M       # Monte Carlo variance\n\n[1] 2.56548e-07",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Week 3 ‚Äî Monte Carlo Integration and Simulation-Based Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "7¬† Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html",
    "href": "App_A-intro-R.html",
    "title": "8¬† Appendix: Introduction to R",
    "section": "",
    "text": "8.1 R\nFor conducting analyses with data sets of hundreds to thousands of observations, calculating by hand is not feasible and you will need a statistical software. R is one of those. R can also be thought of as a high-level programming language. In fact, R is one of the top languages to be used by data analysts and data scientists. There are a lot of analysis packages in R that are currently developed and maintained by researchers around the world to deal with different data problems. Most importantly, R is free! In this section, we will learn how to use R to conduct basic statistical analyses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#ide",
    "href": "App_A-intro-R.html#ide",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.2 IDE",
    "text": "8.2 IDE\n\n8.2.1 Rstudio\nRStudio is an integrated development environment (IDE) designed specifically for working with the R programming language. It provides a user-friendly interface that includes a source editor, console, environment pane, and tools for plotting, debugging, version control, and package management. RStudio supports both R and Python and is widely used for data analysis, statistical modeling, and reproducible research. It also integrates seamlessly with tools like R Markdown, Shiny, and Quarto, making it popular among data scientists, statisticians, and educators.\n\n\n8.2.2 Visual Studio Code (VS Code)\nVS Code is a versatile code editor that supports multiple programming languages, including R. With the R extension for VS Code, users can write and execute R code, access R‚Äôs console, and utilize features like syntax highlighting, code completion, and debugging. While not as specialized as RStudio for R development, VS Code offers a lightweight alternative with extensive customization options and support for various programming tasks.\n\n\n8.2.3 Positron\nPositron IDE is the next-generation integrated development environment developed by Posit, the company behind RStudio. Designed to be a modern, extensible, and language-agnostic IDE, Positron builds on the strengths of RStudio while supporting a broader range of languages and workflows, including R, Python, and Quarto.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#rstudio-layout",
    "href": "App_A-intro-R.html#rstudio-layout",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.3 RStudio Layout",
    "text": "8.3 RStudio Layout\nRStudio consists of several panes: - Source: Where you write scripts and markdown documents. - Console: Where you type and execute R commands. - Environment/History: Shows your variables and command history. - Files/Plots/Packages/Help/Viewer: For file management, viewing plots, managing packages, accessing help, and viewing web content.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-scripts",
    "href": "App_A-intro-R.html#r-scripts",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.4 R Scripts",
    "text": "8.4 R Scripts\nR scripts are plain text files containing R code. You can create a new script in RStudio by clicking File &gt; New File &gt; R Script.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-help",
    "href": "App_A-intro-R.html#r-help",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.5 R Help",
    "text": "8.5 R Help\nUse ?function_name or help(function_name) to access help for any R function. For example:\n?mean\nhelp(mean)",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-packages",
    "href": "App_A-intro-R.html#r-packages",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.6 R Packages",
    "text": "8.6 R Packages\nPackages extend R‚Äôs functionality. There are thousands of packages available in R ecosystem. You may install them from different sources.\n\n8.6.1 With Comprehensive R Archive Network (CRAN)\nCRAN is the primary repository for R packages. It contains thousands of packages that can be easily installed and updated.\nInstall a package with:\ninstall.packages(\"package_name\")\n\n\n8.6.2 With Bioconductor\nBioconductor is a repository for bioinformatics packages in R. It provides tools for the analysis and comprehension of high-throughput genomic data.\nInstall Bioconductor packages using the BiocManager package:\nBiocManager::install(\"package_name\")\n\n\n8.6.3 From GitHub\nMany of the authors of R packages host their work on GitHub. You can install these packages using the devtools package:\ndevtools::install_github(\"username/package_name\")\n\n\n8.6.4 Load a package\nOnce a package is installed, you need to load it into your R session to use its functions:\nlibrary(package_name)\nAlternatively, you may use a function in the package with package_name::function_name() without loading the entire package.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-markdown",
    "href": "App_A-intro-R.html#r-markdown",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.7 R Markdown",
    "text": "8.7 R Markdown\nR Markdown allows you to combine text, code, and output in a single document. Create a new R Markdown file in RStudio via File &gt; New File &gt; R Markdown....\nRecently, the posit team has developed a new version of the R Markdown called quarto document, with the file extension .qmd. It is still under rapid development.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#vectors",
    "href": "App_A-intro-R.html#vectors",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.8 Vectors",
    "text": "8.8 Vectors\nVectors are the most basic data structure in R.\n\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\n\nYou can perform operations on vectors:\n\nx * 2\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#data-sets",
    "href": "App_A-intro-R.html#data-sets",
    "title": "8¬† Appendix: Introduction to R",
    "section": "8.9 Data Sets",
    "text": "8.9 Data Sets\nData frames are used for storing data tables. Create a data frame:\n\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Score = c(90, 85))\ndf\n\n   Name Score\n1 Alice    90\n2   Bob    85\n\n\nYou can import data from files using read.csv() or read.table().\n\nThis appendix is adapted from Why R?.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R</span>"
    ]
  }
]