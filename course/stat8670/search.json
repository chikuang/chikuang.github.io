[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Description",
    "text": "Description\nTopics included are optimization, numerical integration, bootstrapping, cross-validation and Jackknife, density estimation, smoothing, and use of the statistical computer package of S-plus/R.\n\nPrerequisites\nMATH 4752/6752 ‚Äì Mathematical Statistics II, and the ability to program in a high-level language.\n\n\nInstructor\nChi-Kuang Yeh, I am an Assistant Professor in the Department of Mathematics and Statistics, Georgia State University.\n\nOffice: Suite 1407, 25 Park Place.\nEmail: cyeh@gsu.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#office-hour",
    "href": "index.html#office-hour",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Office Hour",
    "text": "Office Hour\n14:00‚Äì15:00 on Tuesday\n13:00 - 14:00 on Friday (online) Link Provided on iCollege",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#grade-distribution",
    "href": "index.html#grade-distribution",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\nAssignments: 40%\nExam 1: 15%\nExam 2: 15%\nProject: 30%",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assignment",
    "href": "index.html#assignment",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Assignment",
    "text": "Assignment\n\nAssignment 1: Due on September 12, 2025\nAssignment 2: Due on September 22, 2025\nAssignment 3: Due on October 22, 2025\nAssignment 4: TBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#midterm",
    "href": "index.html#midterm",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Midterm",
    "text": "Midterm\n\nMidterm 1 on October 8, 2025\nMidterm 2 on November 12, 2025",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#project",
    "href": "index.html#project",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Project",
    "text": "Project\n\nReport: Due Date TBA\nPresentation: Date TBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-and-corresponding-lectures",
    "href": "index.html#topics-and-corresponding-lectures",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Topics and Corresponding Lectures",
    "text": "Topics and Corresponding Lectures\nThose chapters are based on the lecture notes. This part will be updated frequently.\n\n\n\nStatus\nTopic\nLecture Covered\n\n\n\n\n‚úÖ Complete\nR Programming\n1‚Äì2\n\n\n‚úÖ Complete\nNumerical Approaches and Optimization in 1-D\n3‚Äì5\n\n\n‚úÖ Complete\nReview for Distribution\n6‚Äì7\n\n\n‚úÖ Complete\nRandom Variable Generation\n8‚Äì10\n\n\n‚úÖ Complete\nMonte Carlo and Integration\n11‚Äì15\n\n\n‚úÖ Complete\nExam 1\n13\n\n\n‚úÖ Complete\nResampling method: Cross-Validation\n16 -17\n\n\n‚è≥ In Progress\nResampling methods: Bootstrap and Jackknife\n18‚Äì\n\n\nüîú Planned\nExam 2\n23\n\n\nüîú Planned\nSmoothing\nTBA\n\n\nüîú Planned\nDensity estimation\nTBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-textbooks",
    "href": "index.html#recommended-textbooks",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Recommended Textbooks",
    "text": "Recommended Textbooks\n\nGivens, G.H. and Hoeting, J.A. (2012). Computational Statistics. Wiley, New York.\nRizzo, M.L. (2007) Statistical Computing with R. CRC Press, Roca Baton.\nHothorn, T. and Everitt, B.S. (2006). A Handbook of Statistical Analyses Using R. CRC Press, Boca Raton.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#side-readings",
    "href": "index.html#side-readings",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Side Readings",
    "text": "Side Readings\n\nWickham, H., √áetinkaya-Rundel, M. and Grolemund, G. (2023). R for Data Science. O‚ÄôReilly.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1¬† Data Structure and R Programming",
    "section": "",
    "text": "1.1 Data type\nData types, operators, variables\nTwo basic types of objects: (1) data & (2) functions\nday &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nweather &lt;- c(\"Raining\", \"Sunny\", NA, \"Windy\", \"Snowing\")\ndata.frame(rbind(day, weather))\n\n             X1      X2        X3       X4      X5\nday      Monday Tuesday Wednesday Thursday  Friday\nweather Raining   Sunny      &lt;NA&gt;    Windy Snowing",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-type",
    "href": "01-intro.html#data-type",
    "title": "1¬† Data Structure and R Programming",
    "section": "",
    "text": "Boolean/Logical: Yes or No, Head or Tail, True or False\nIntegers: Whole numbers \\(\\mathbb{Z}\\), e.g., 1, 2, 3, -1, -2, -3\nCharacters: Text strings, e.g., ‚ÄúHello‚Äù, ‚ÄúWorld.‚Äù\nFloats: Noninteger fractional numbers, e.g., \\(\\pi\\), \\(e\\).\nMissing data: NA in R, which stands for ‚ÄúNot Available.‚Äù It is used to represent missing or undefined values in a dataset.\n\n\n\nOther more complex types\n\n\n1.1.1 To change data type\nYou may change the data type using the following functions, but the chance is that some of the information will be missing. Do this with caution!\n\nx &lt;- pi\nprint(x)\n\n[1] 3.141593\n\nx_int &lt;- as.integer(x)\nprint(x_int)\n\n[1] 3\n\n\nSome of the conversion functions:\n\nas.integer(): Convert to integer.\nas.numeric(): Convert to numeric (float).\nas.character(): Convert to character.\nas.logical(): Convert to logical (boolean).\nas.Date(): Convert to date.\nas.factor(): Convert to factor (categorical variable).\nas.list(): Convert to list.\nas.matrix(): Convert to matrix.\nas.data.frame(): Convert to data frame.\nas.vector(): Convert to vector.\nas.complex(): Convert to complex number.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#operators",
    "href": "01-intro.html#operators",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.2 Operators",
    "text": "1.2 Operators\n\nUnary: With only one argument. E.g., -x (negation), !x (logical negation).\nBinary: With two arguments. E.g., x + y (addition), x - y (subtraction), x * y (multiplication), x / y (division).\n\n\n1.2.1 Comparison Operator\nComparing two objects. E.g., x == y (equal), x != y (not equal), x &lt; y (less than), x &gt; y (greater than), x &lt;= y (less than or equal to), x &gt;= y (greater than or equal to).\n\n\n1.2.2 Logical Operator\nLogical operators are used to combine or manipulate logical values (TRUE or FALSE). E.g., x & y (logical AND), x | y (logical OR), !x (logical NOT).\nWe shall note that the logical operators in R are vectorized, x | y and x || y are different. The former is vectorized, while the latter is not.\nx &lt;- c(TRUE, FALSE, FALSE)\ny &lt;- c(TRUE, FALSE, FALSE)\nx | y  # [1]  TRUE FALSE FALSE\nx || y # This will return an error",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#indexing",
    "href": "01-intro.html#indexing",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.3 Indexing",
    "text": "1.3 Indexing\nIndexing is a way to access or modify specific elements in a data structure. In R, indexing can be done using square brackets [] for vectors and matrices, or the $ operator for data frames. Note that the index starts from 0 in R, which is different from some other programming languages like Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#naming",
    "href": "01-intro.html#naming",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.4 Naming",
    "text": "1.4 Naming\nIn R, you can assign names to objects using the names() function. This is useful for making your code more readable and for accessing specific elements in a data structure.\nA good practice is to use _ (underscore) to separate words in variable names, e.g., my_variable. This makes the code more readable and easier to understand.\n\n# Assign names to a vector\ntemp &lt;- c(20, 30, 27, 31, 45)\nnames(temp) &lt;- c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\")\nprint(temp)\n\n  Mon  Tues   Wed Thurs   Fri \n   20    30    27    31    45 \n\n\nrownames(temp) &lt;- \"Day1\" # error\n\ntemp_mat &lt;- matrix(c(20, 30, 27, 31, 45), nrow = 1, ncol = 5)\ncolnames(temp_mat) &lt;- c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\")\nrownames(temp_mat) &lt;- \"Day1\" # error\nprint(temp_mat)\n\n     Mon Tues Wed Thurs Fri\nDay1  20   30  27    31  45",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#array-and-matrix",
    "href": "01-intro.html#array-and-matrix",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.5 Array and Matrix",
    "text": "1.5 Array and Matrix\nOne may define an array or a matrix in R using the array() or matrix() functions, respectively. An array is a multi-dimensional data structure, while a matrix is a two-dimensional array.\n\n# Create a 1-dimensional array\narray_1d &lt;- array(1:10, dim = 10)\narray_1d\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# Create a 2-dimensional array\narray_2d &lt;- array(1:12, dim = c(4, 3))\narray_2d\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n# Create a 3-dimensional array\narray_3d &lt;- array(1:24, dim = c(4, 3, 2))\narray_3d\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n# Create a matrix\nmy_matrix &lt;- matrix(1:12, nrow = 4, ncol = 3)\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nNote here, the matrix is a special case of an array, where the number of dimensions is exactly 2.\nis.matrix(array_2d)   # TRUE\nis.matrix(my_matrix)  # TRUE\n\nis.array(array_2d)    # TRUE\nis.array(my_matrix)   # TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#key-and-value-pair",
    "href": "01-intro.html#key-and-value-pair",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.6 Key and Value Pair",
    "text": "1.6 Key and Value Pair\nKey-Value Pair is a data structure that consists of a key and its corresponding value. In R, this can be implemented using named vectors, lists, or data frames. Usually, the most commonly used case is in the lists and data frames. The values can be extra by providing the corresonding key\n\nkey1 &lt;- \"Tues\"\nvalue1 &lt;- 32\nkey2 &lt;- \"Wed\"\nvalue2 &lt;- 28\n\nlist_temp &lt;- list()\nlist_temp[[ key1 ]] &lt;- value1\nlist_temp[[ key2 ]] &lt;- value2\n\nprint(list_temp)\n\n$Tues\n[1] 32\n\n$Wed\n[1] 28\n\n## Now providing a key - Tues\n### First way\nlist_temp[[\"Tues\"]]\n\n[1] 32\n\n### Second way\nlist_temp$Tues\n\n[1] 32",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-frame",
    "href": "01-intro.html#data-frame",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.7 Data Frame",
    "text": "1.7 Data Frame\nDataframe is a two-dimensional, tabular data structure in R that can hold different types of variables (numeric, character, factor, etc.) in each column. It is similar to a spreadsheet or SQL table.\n\niris &lt;- datasets::iris\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#apply-function",
    "href": "01-intro.html#apply-function",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.8 Apply function",
    "text": "1.8 Apply function\nThe apply() function is the basic model of the family of apply functions in R, which includes specific functions like lapply(), sapply(), tapply(), mapply(), vapply(), rapply(), bapply(), eapply(), and others. These functions are used to apply a function to elements of a data structure (like a vector, list, or data frame) in a (sometimes) more efficient and concise way than using loops.\n\nx &lt;- cbind(x1 = 3, x2 = c(4:1, 2:5))\ndimnames(x)[[1]] &lt;- letters[1:8]\nprint(x)\n\n  x1 x2\na  3  4\nb  3  3\nc  3  2\nd  3  1\ne  3  2\nf  3  3\ng  3  4\nh  3  5\n\napply(x, MARGIN = 2, mean) #apply the mean function to their \"columns\"\n\nx1 x2 \n 3  3 \n\ncol.sums &lt;- apply(x, MARGIN = 2, sum) #apply the sum function to their \"columns\"\nrow.sums &lt;- apply(x, MARGIN = 1, sum) #apply the sum function to their \"rows\"\nrbind(cbind(x, Rtot = row.sums), Ctot = c(col.sums, sum(col.sums)))\n\n     x1 x2 Rtot\na     3  4    7\nb     3  3    6\nc     3  2    5\nd     3  1    4\ne     3  2    5\nf     3  3    6\ng     3  4    7\nh     3  5    8\nCtot 24 24   48\n\n\nSome of the commonly used apply functions:\n\nlapply: Apply a Function over a List or Vector\nsapply: a user-friendly version and wrapper of lapply by default returning a vector, matrix\nvapply: similar to sapply, but has a pre-specified type of return value, so it can be safer (and sometimes faster) to use.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#tidyverse",
    "href": "01-intro.html#tidyverse",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.9 Tidyverse",
    "text": "1.9 Tidyverse\nThe tidyverse is a collection of open source packages for the R programming language introduced by Hadley Wickham and his team that ‚Äúshare an underlying design philosophy, grammar, and data structures‚Äù of tidy data. Characteristic features of tidyverse packages include extensive use of non-standard evaluation and encouraging piping.\n\n## Load all tidyverse packages\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.2\n‚úî ggplot2   4.0.0     ‚úî tibble    3.3.0\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.1.0     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Or load specific packages in the tidy family\nlibrary(dplyr) # Data manipulation\nlibrary(ggplot2) # Data visualization\nlibrary(readr) # Data import\nlibrary(tibble) # Tidy data frames\nlibrary(tidyr) # Data tidying\n# ...",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#pipe",
    "href": "01-intro.html#pipe",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.10 Pipe",
    "text": "1.10 Pipe\nPipe operator |&gt; (native after R version 4.0) or %&gt;$ (from magrittr package) is a powerful tool in R that allows you to chain together multiple operations in a clear and concise way. It takes the output of one function and passes it as the first argument to the next function.\nFor example, we can write\n\nset.seed(777)\nx &lt;- rnorm(5)\n\n## Without using pipe\nprint(round(mean(x), 2))\n\n[1] 0.37\n\n## Using pipe\nx |&gt; \n  mean() |&gt; # applying the mean function\n  round(2) |&gt; #round to 2nd decimal place\n  print()\n\n[1] 0.37\n\n\nWe can see that, without using the pipe, if we are applying multiple functions to the same object, we may have hard time to track. This can make the code less readable and harder to maintain. On the other hand, using pipe, we can clearly see the sequence of operations being applied to the data, making it easier to understand and modify.\n\n1.10.1 Some rules\n|&gt; should always have a space before it and should typically be the last thing on a line. This simplifies adding new steps, reorganizing existing ones, and modifying elements within each step.\nNote that all of the packages in the tidyverse family support the pipe operator (except ggplot2!), so you can use it with any of them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#questions-in-class",
    "href": "01-intro.html#questions-in-class",
    "title": "1¬† Data Structure and R Programming",
    "section": "1.11 Questions in class",
    "text": "1.11 Questions in class\n\n1.11.1 Lecture 1, August 25, 2025\nQ1. If I know Python already, why learn R?\nReply: My general take are 1). R is more specialized for statistical analysis and data visualization, while Python is a more general-purpose programming language. 2). R has a rich ecosystem of packages and libraries specifically designed for statistical computing, making it a popular choice among statisticians and data scientists. 3). R‚Äôs syntax and data structures are often more intuitive for statistical tasks, which can lead to faster development and easier collaboration with other statisticians. 4). Also, the tidyverse ecosystem including ggplot and others are a big plus when dealing with big dataframes. 5). They are not meant to replace each other, but work as a complement.\nQ2. Why my installation of R sometimes failed on a Windows machine?\nReply: There are many reasons. One of the most common reasons is that you may need to manually add the path to the environment variable.\n\n\n1.11.2 Lecture 2, August 27, 2025\nQ1. What‚Äôs the difference of using apply v.s. looping in R?\nReply: The apply functions are often faster and more efficient than looping, especially for large datasets, because they have done some vectorization under the hood. Also, it has much higher readibility and better conciseness. However, depends on the task, you may want to do the benchmarking to see the performance difference.\nQ2. How to use pipe with two or more variables?\nReply: There are several ways to do this.\n\nWithin the tidyverse family: One way is to use the dplyr package, which provides a set of functions that work well with the pipe operator. For example, you can use the mutate() function to create a new variable based on two existing variables. For example, you can do\n\n\nlibrary(dplyr)\nlibrary(magrittr)  # for %$%\nlibrary(purrr)     # for pmap / exec if needed\n\nmy_df &lt;- tibble(x = 1:5, y = 6:10)\nf  &lt;- function(a, b) a + 2*b\n\nmy_df %&gt;%\n  mutate(z = f(x, y))\n\n# A tibble: 5 √ó 3\n      x     y     z\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     6    13\n2     2     7    16\n3     3     8    19\n4     4     9    22\n5     5    10    25\n\n\n\nUsing base R, you may do something like the following through the magrittr package‚Äôs exposition pipe %$%:\n\n\nlibrary(magrittr)\n# method 1\nmy_df %$% f(x, y) \n\n[1] 13 16 19 22 25\n\n# or use . as a placeholder\n# method 2\nmy_df %&gt;% { f(.$x, .$y) }\n\n[1] 13 16 19 22 25\n\n\n\nSome of the materials are adapted from CMU Stat36-350.\nA comprehensive reference for all the tidyverse tools is R for Data Science.\nA comprehensive reference for ggplot2 is ggplot2: Elegant Graphics for Data Analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "02-optimization.html",
    "href": "02-optimization.html",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "",
    "text": "2.1 Theory versus Computation\nThe optimization plays an important role in statistical computing, especially in the context of maximum likelihood estimation (MLE) and other statistical inference methods. This chapter will cover various optimization techniques used in statistical computing.\nThere is a general principle that will be repeated in this chapter that Kenneth Lange calls optimization transfer in his 1999 paper. The basic idea applies to the problem of maximizing a function \\(f\\).\nNote 1: steps 2&3 are repeated until convergence.\nNote 2: maximizing \\(f\\) is equivalent to minimizing \\(-f\\).\nNote 3: the surrogate function \\(g\\) should be chosen such that it is easier to optimize than \\(f\\).\nFor instance, for a linear regression \\[\\begin{equation}\n  y = X\\boldsymbol{\\beta} + \\varepsilon. \\label{eq:linmod}\n\\end{equation}\\]\nFrom regression class, we know that the (ordinary) least-squares estimation (OLE) for \\(\\boldsymbol{\\beta}\\) is given by \\(\\hat{\\boldsymbol{\\beta}}=(X^\\top X)^{-1} X^\\top y\\). It is convenient as the solution is in the closed-form! However, in the most case, the closed-form solutions will not be available.\nFor GLMs or non-linear regression, we need to do this iterativelly!\nOne confusing aspect of statistical computing is that often there is a disconnect between what is printed in a statistical computing textbook and what should be implemented on the computer.\nSome potential issues includ:\nA &lt;- matrix(\n  c(1, 2, 3,\n    4, 5, 6,\n    7, 8, 9),\n  nrow = 3, ncol = 3, byrow = TRUE)\nB &lt;- A\nB[3, 3] &lt;- B[3, 3] + 1E-5\n\nqr(A)$rank\n\n[1] 2\n\nqr(B)$rank\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#theory-versus-computation",
    "href": "02-optimization.html#theory-versus-computation",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "",
    "text": "In textbooks, simpler to present solutions as convenient mathematical formulas whenever possible, in order to communicate basic ideas and to provide some insight.\n\nHowever, directly translating these formulas into computer code is usually not advisable because there are many problematic aspects of computers that are simply not relevant when writing things down on paper.\n\n\n\n\nMemory overflow: The computer has a limited amount of memory, and it is possible to run out of memory when working with large datasets or complex models.\nNumerical Precision: Sometimes, due to the cut precision of floating-point arithmetic, calculations that are mathematically equivalent can yield different results on a computer.\n\nExample 1: round \\(1/3\\) to two decimal places, we get \\(0.33\\). Then, \\(3 \\cdot (1/3)\\) is exactly \\(1\\), but \\(3 \\cdot 0.33\\) is \\(0.99\\).\nExample 2: \\(1 - 0.99999999\\) is \\(0.00000001\\) (=1E-8), but if we round \\(0.99999999\\) to two decimal places, we get \\(1.00\\), and then \\(1 - 1.00\\) is \\(0\\). If we round \\(0.00000001\\) to two decimal places, we get \\(0.00\\).\nExample 3: \\(\\pi\\)\n\n(Lienar) Dependence: The detection of linear dependence in matrix computations is influenced by machine precision. Since computers operate with finite precision, situations often arise where true linear dependence exists, but the computer cannot distinguish it from independence.\n\nExample: Consider the matrix \\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\] The 3rd column is a linear combination of the first two columns (i.e., col3 = col1 + col2). However, due to machine precision limitations, the computer might not recognize this exact linear dependence, leading to numerical instability in computations involving this matrix. With a small distortion, we have \\[\nB = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 + 10^{-5} \\\\\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#matrix-inversion",
    "href": "02-optimization.html#matrix-inversion",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.2 Matrix Inversion",
    "text": "2.2 Matrix Inversion\nIn many statistical analyses, such as linear regression and specify the distribution (such as normal distribution), matrix inversion plays a central role.\n\n2.2.1 Example 1: Normal distribution\nWe know that, a normal density with the parameters mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is \\[\nf\\left(x \\mid \\mu, \\sigma^2\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left\\{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2\\right\\}\n\\] or we may work on the multivariate normal distribution case which is a bit more involved.\n\n\\(\\boldsymbol{X} = (X_1,\\dots, X_d)\\) is said to be a multivariate normal distribution if and only if it is a linear comibnation of independent and identically distributed standard normals: \\[\n\\boldsymbol{X} = \\boldsymbol{CZ} + \\mu,\\quad \\boldsymbol{Z}=(Z_1,\\dots,Z_d),\\quad Z_i \\stackrel{iid}{\\sim} N(0,1).\n\\]\n\nThe property of the multivariate normal are:\n\nmean vector: \\(E(\\boldsymbol{X}) = \\mu\\)\nvariance: \\(Var(\\boldsymbol{X}) = \\boldsymbol{CZC}^\\top = \\boldsymbol{C} var(\\boldsymbol{Z})\\boldsymbol{C}^\\top:=  \\boldsymbol{\\Sigma}\\)\n\nNotation: \\(\\boldsymbol{X} \\sim N(\\mu, \\boldsymbol{\\Sigma})\\).\nPDF: \\[\nf(\\boldsymbol{x} \\mid \\mu, \\Sigma)=(2 \\pi)^{-d / 2} \\cdot \\exp \\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\prime} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-\\frac{1}{2} \\log |\\boldsymbol{\\Sigma}|\\right\\}.\n\\] Some of the potential ways to do this is to take logarithm of the PDF (Think about why).\n\n\n2.2.2 Example 2: Linear regression\nRecall the linear regression model . The OLE for \\(\\boldsymbol{\\beta}\\) is given by \\(\\hat{\\boldsymbol{\\beta}}=(X^\\top X)^{-1} X^\\top y\\).\nWe can solve this using the R command\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nwhere solve() is the R function for matrix inversion. However, it is not a desired way (think about why).\nA better way is to go back to the formula, and look at \\[\nX^\\top X\\boldsymbol{\\beta}= X^\\top y,\n\\] and solve this using the R command\nsolve( crossprod(X), crossprod(X, y) ) \n# this is the same as \n# solve(t(X) %*% X, t(X) %*% y)\nHere, we avoid explicitly calculating the inverse of \\(X^\\top X\\). Instead, we use gaussian elimination to solve the system of equations, which is generally more numerically stable and efficient.\n\n2.2.2.1 Speed comparison\nset.seed(2025-09-03)\nX &lt;- matrix(rnorm(5000 * 100), 5000, 100)\ny &lt;- rnorm(5000)\nlibrary(microbenchmark)\nmicrobenchmark(solve(t(X) %*% X) %*% t(X) %*% y)\nUnit: milliseconds\n                             expr      min       lq\n solve(t(X) %*% X) %*% t(X) %*% y 28.83505 30.16593\n     mean   median       uq      max neval\n 31.96782 30.79489 32.63315 111.0151   100\nWarning message:\nIn microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y) :\n  less accurate nanosecond times to avoid potential integer overflows\nmicrobenchmark(solve(t(X) %*% X) %*% t(X) %*% y,\n               solve(crossprod(X), crossprod(X, y)))\nUnit: milliseconds\n                                 expr      min       lq\n     solve(t(X) %*% X) %*% t(X) %*% y 28.90135 30.11608\n solve(crossprod(X), crossprod(X, y)) 25.05859 25.27480\n     mean   median       uq      max neval\n 31.78686 31.38513 32.66482 53.03354   100\n 26.15771 25.81678 26.89188 29.12045   100\n\n\n\n2.2.3 Take home message:\nThe take home here is that the issues arise from the finite precision of computer arithmetic and the limited memory available on computers. When implementing statistical methods on a computer, it is crucial to consider these limitations and choose algorithms and implementations that are robust to numerical issues.\n\n\n2.2.4 Multi-collinearity\nThe above approach may break down when there is any multi-colinearity in the \\(\\boldsymbol{X}\\) matrix. For example, we can tack on a column to \\(\\boldsymbol{X}\\) that is very similar (but not identical) to the first column of \\(\\boldsymbol{X}\\).\n\nset.seed(7777)\nN &lt;- 3000\nK &lt;- 100\ny &lt;- rnorm(N)\nX &lt;- matrix(rnorm(N * K), N, K)\nW &lt;- cbind(X, X[, 1] + rnorm(N, sd = 1E-15))\n\nsolve(crossprod(W), crossprod(W, y))\n\nError in `solve.default()`:\n! system is computationally singular: reciprocal condition number = 1.36748e-32\nThe algorithm does not work because the cross product matrix \\(W^\\top W\\) is singular. In practice, matrices like these can come up a lot in data analysis and it would be useful to have a way to deal with it automatically.\nR takes a different approach to solving for the unknown coefficients in a linear model. R uses the QR decomposition, which is not as fast, but has the added benefit of being able to automatically detect and handle colinear columns in the matrix.\nHere, we use the fact that X can be decomposed as \\(\\boldsymbol{X}=QR\\), where \\(Q\\) is an orthonormal matrix and \\(R\\) is an upper triangular matrix. Given that, we can rewrite \\(X^\\top X \\boldsymbol{\\beta}= X^\\top y\\) as \\[\\begin{align*}\nR^\\top Q^\\top Q R \\boldsymbol{\\beta}&= R^\\top Q^\\top y\\\\\nR^\\top I R \\boldsymbol{\\beta}&= R^\\top Q^\\top y\\\\\nR^\\top R \\boldsymbol{\\beta}&= R^\\top Q^\\top y,\n\\end{align*}\\] this leads to \\(R\\boldsymbol{\\beta}= Q^\\top y\\). Now we can perform the Gaussian elimination to do it. Because \\(R\\) is an upper triangular matrix, the computational speed is much faster. Here, we avoid to compute the cross product \\(X^\\top X\\), which is numerical unstable if it is not standardized properly\nWe can see in R code that even with our singular matrix \\(W\\) above, the QR decomposition continues without error.\n\nQw &lt;- qr(W)\nstr(Qw)\n\nList of 4\n $ qr   : num [1:3000, 1:101] 54.43933 0.00123 -0.02004 -0.00671 -0.00178 ...\n $ rank : int 100\n $ qraux: num [1:101] 1.01 1.01 1.01 1 1 ...\n $ pivot: int [1:101] 1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"class\")= chr \"qr\"\n\n\nNote that the output of qr() computes the rank of \\(W\\) to be 100, not 101 as the last column is collinear to the 1st column. From there, we can get \\(\\hat{\\boldsymbol{\\beta}}\\) if we want using qr.coef(),\n\nbetahat &lt;- qr.coef(Qw, y)\nhead(betahat, 3)\n\n[1]  0.024314718  0.000916951 -0.005980588\n\ntail(betahat, 3)\n\n[1]  0.01545039 -0.01010440          NA\n\n\nQ: Why there is an NA?\n\n\n2.2.5 Trade-off\nThere isn‚Äôt always elegance and flourish. When we take the robust approach, we accept that it comes at a cost.\n\nlibrary(ggplot2)\nlibrary(microbenchmark)\nm &lt;- microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y,\n                    solve(crossprod(X), crossprod(X, y)),\n                    qr.coef(qr(X), y))\n\nWarning in microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, solve(crossprod(X),\n: less accurate nanosecond times to avoid potential integer overflows\n\nautoplot(m)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\n‚Ñπ Please use tidy evaluation idioms with `aes()`.\n‚Ñπ See also `vignette(\"ggplot2-in-packages\")` for more information.\n‚Ñπ The deprecated feature was likely used in the microbenchmark package.\n  Please report the issue at\n  &lt;https://github.com/joshuaulrich/microbenchmark/issues/&gt;.\n\n\n\n\n\n\n\n\n\nCompared with the approaches discussed above, this method performs similarly to the naive approach but is much more stable and reliable.\nIn practice, we rarely call functions such as qr() or qr.coef() directly, since higher-level functions like lm() handle these computations automatically. However, in certain specialized and performance-critical settings, it can be advantageous to use alternative matrix decompositions to compute regression coefficients, especially when the computation must be repeated many times in a loop (i.e., Vectorization)\n\n\n2.2.6 Multivariate Normal revisit\nComputing the multivariate normal (MVN) density is a common task, for example, when fitting spatial models or Gaussian process models. Because maximum likelihood estimation(MLE) and likelihood ratio tests (LRT) often require evaluating the likelihood many times, efficiency is crucial.\nAfter taking the log of the MVN density, we have\n\\[\n\\ell(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\Sigma) := \\log \\left\\{ f(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\Sigma) \\right\\}\n= -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}).\n\\] On the right hand side, the first term is a constant, the second term is linear, and the last term is quadratic, which requires much more computational power.\n\n2.2.6.1 A Naive Implementation\nWe first center the data \\(\\boldsymbol{z}:=\\boldsymbol{x}- \\mu\\). Then we have \\(\\boldsymbol{z}^\\top \\Sigma^{-1} \\boldsymbol{z}\\). This simiplified the question for a bit.\nHere, much like the linear regression example above, the key bottleneck is the inversion of the \\(p\\)-dimensional covariance matrix \\(\\Sigma\\). If we take \\(\\boldsymbol{z}\\) to be a \\(p\\times 1\\) column vector, then a literal translation of the mathematics into R code might look something like this,\nt(z) %*% solve(Sigma) %*% z\nTo illustrate, let‚Äôs simulate some data and compute the quadratic form the naive way:\n\nset.seed(2025-09-03)\n\n# Generate data\nz &lt;- matrix(rnorm(200 * 100), 200, 100)\nS &lt;- cov(z)\n\n# Naive quadratic form\nquad.naive &lt;- function(z, S) {\n  Sinv &lt;- solve(S)\n  rowSums((z %*% Sinv) * z)\n}\n\nlibrary(dplyr)\nquad.naive(z, S) %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  70.67   93.61   99.94  100.54  107.31  126.73 \n\n\n\n\n2.2.6.2 A Better Way: Cholesky Decomposition\nBecause the covariance matrix is symmetric and positive definite, we can exploit its Cholesky decomposition. That is, we write \\(\\Sigma = R^\\top R\\), where \\(R\\) is a upper triangular matrix. Then, \\[\n\\boldsymbol{z}^\\top \\Sigma^{-1} \\boldsymbol{z}= \\boldsymbol{z}^\\top (R^\\top R)^{-1} \\boldsymbol{z}= \\boldsymbol{z}^\\top R^{-1}R^{-\\top} \\boldsymbol{z}= (R^{-\\top}\\boldsymbol{z})^\\top (R^{-\\top} \\boldsymbol{z}) := \\boldsymbol{v}^\\top \\boldsymbol{v}.\n\\] Note that \\(\\boldsymbol{v}\\in \\mathbb R^p\\) is the solution to the linear system \\(R^\\top \\boldsymbol{v}= \\boldsymbol{z}\\). Because \\(R\\) is upper triangular, we can solve this system efficiently using back substitution. Also, we can solve this without doing the inversion.\nOnce we have \\(\\boldsymbol{v}\\) we can compute its quadratic form \\(\\boldsymbol{v}^\\top \\boldsymbol{v}\\) by the crossprod() function.\n\nquad.chol &lt;- function(z, S) {\n  R &lt;- chol(S)\n  v &lt;- backsolve(R, t(z), transpose = TRUE)\n  colSums(v * v)\n}\n\nquad.chol(z, S) %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  70.67   93.61   99.94  100.54  107.31  126.73 \n\n\n\n\n2.2.6.3 By product\nAnother benefit of the Cholesky decomposition is that it gives us a simple way to compute the log-determinant of \\(\\Sigma\\). The log-determinant of \\(\\Sigma\\) is simply two times the sum of the log of the diagonal elements of R. (Why?)\n\n\n2.2.6.4 Performance comparison\n\nlibrary(microbenchmark)\nlibrary(ggplot2)\nm2 &lt;- microbenchmark(\n  naive = quad.naive(z, S),\n  chol  = quad.chol(z, S)\n)\nautoplot(m2)\n\n\n\n\n\n\n\n\nQ: Why one is faster than the other?\n\n\n2.2.6.5 Take home message 2\nThe naive algorithm simply inverts the covariance matrix. The Cholesky-based approach, on the other hand, exploits the fact that covariance matrices are symmetric and positive definite. This results in an implementation that is both faster and numerically more stable‚Äîexactly the kind of optimization that makes a difference in real-world statistical computing.\nThus, a knowledge of statistics and numerical analysis can often lead to better algorithms, often invaluable!",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#nonlinear-functions",
    "href": "02-optimization.html#nonlinear-functions",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.3 Nonlinear functions",
    "text": "2.3 Nonlinear functions\nOn the top, we have linear functions, such as \\(y=f(x) = ax + b\\) or in the linear regression \\(y=X\\beta +\\epsilon\\). It is a small class of the functions, and may be relatively limited.\nE.g., what if we have a quadratic relationship? Then \\(y=f(x) = ax^2 + bx + c\\).\nSuch nonlinear relationship is very common, , such as \\(f(x) = a\\sin(bx + c)\\) or \\(f(x) = a\\exp(bx) + c\\), and they may not have a closed-form solution like in the linear regression case.\nFrom now on, we will be talking about the numerical approaches to solve these problems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#type-of-optimization-algorithms",
    "href": "02-optimization.html#type-of-optimization-algorithms",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.4 Type of Optimization Algorithms",
    "text": "2.4 Type of Optimization Algorithms\nThere are in general two types of the optimization algorithms: (1). deterministic and (2). metaheuristic. Deterministic and metaheuristic algorithms represent two distinct paradigms in optimization.\n*. Deterministic methods: such as gradient descent, produce the same solution for a given input and follow a predictable path toward an optimum.\n*. In contrast, metaheuristic approaches: incorporate randomness and do not guarantee the best possible solution. However, they are often more effective at avoiding local optima and exploring complex search spaces.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#deterministic-algorithms",
    "href": "02-optimization.html#deterministic-algorithms",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.5 Deterministic Algorithms",
    "text": "2.5 Deterministic Algorithms\nNumerical approximation, what you learned in the mathematical optimization course. Some of the algorithms include:\n\nGradient Descent\nNewton‚Äôs Method\nConjugate Gradient Method\nQuasi-Newton Methods (e.g., BFGS)\nInterior Point Methods\n\nThey often rely on the Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions.\n\n2.5.1 Root finding\nThe root finding is probably the first numerical approach you learned in the numerical analysis course. Consider a function \\(f: \\mathbb R\\to \\mathbb R\\). The point \\(x\\in \\mathbb R\\) is called a root of \\(f\\) if \\(f(x) = 0\\).\nQ: Why do we care about the root finding?\nThis idea has broad applications. While finding the values of x such that f(x) = 0 is useful in many settings, a more general task is to determine the values of x for which f(x) = y. The same techniques used to find the roots of a function can be applied here by rewriting the problem as \\[\n\\tilde{f}(x) := f(x) - y = 0.\n\\] In this way, new function \\(\\tilde{f}(x)\\) has a root at the solution to, \\(f(x)=y\\), original equation.\nFor linear function, it is trivial. For quadratic function, we can use the quadratic formula, i.e., \\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\n\\] However, for more complex functions, we need to use numerical methods to solve it iteratively. Below, we are going to go over some numerical algorithms.\n\n\n2.5.2 One-dimensional case\nWe first look at the one-dimensional case. The function we want to optimize is\n\\[f(x) = x^3 - x + 1\\]\n\n\n2.5.3 Bisection method\nBisection method is just like a binary search.\n\nStep 1. Selection two points \\(a,b\\in \\chi \\subseteq \\mathbb R\\), where \\(\\chi\\) is the domain of \\(f\\). Make sure that \\(a\\) and \\(b\\) have opposite signs, i.e., \\(f(a)f(b) &lt; 0\\).\nStep 2. Compute the midpoint \\(c = (a+b)/2\\).\nStep 3. Evaluate and check the sign of \\(f(c)\\). If \\(f(c)\\) has the same sign as \\(f(a)\\), then set \\(a=c\\). Otherwise, set \\(b=c\\).\nStep 4. Iterate Steps 2 and 3 until the interval \\([a,b]\\) is sufficiently small.\n\nThe intuition here is that we are shirking the search space \\(\\chi\\) by half in each iteration.\nQ: Why this algorithm work and what are the assumptions? 1. We require the function to be continuous 2. We require the function to have opposite signs at the two endpoints \\(a,b\\in\\chi\\subseteq \\mathbb R\\). 3. We do not require the differentiability!\nQ: But what‚Äôs the cost?\nQ: Can this work for every function?\n\n2.5.3.1 Example\nSuppose the design region is\n\na &lt;- 1\nb &lt;- 4\ncurve(0.5*x^3 - 0.5*x - 18, from = a, to = b, xlab = \"x\", ylab = \"f(x)\")\nfun_obj &lt;- function(x) 0.5*x^3 - 0.5*x - 18\n\nmy_bisec &lt;- function(fun_obj, a, b, tol = 1E-2, ind_draw = FALSE) {\n  if (fun_obj(a) * fun_obj(b) &gt; 0) {\n    stop(\"f(a) and f(b) must have opposite signs!\")\n  }\n  iter &lt;- 0\n  while ((b - a) / 2 &gt; tol) {\n    c &lt;- (a + b) / 2\n    \n    if (ind_draw == TRUE) {\n    # Draw vertical line\n    abline(v = c, col = \"red\", lty = 2)\n    # Label the iteration above the x-axis\n    text(c, par(\"usr\")[3] + 2, labels = iter + 1, col = \"blue\", pos = 3, cex = 0.8)\n    }\n\n    \n    if (fun_obj(c) == 0) {\n      return(c)\n    } else if (fun_obj(a) * fun_obj(c) &lt; 0) {\n      b &lt;- c\n    } else {\n      a &lt;- c\n    }\n    iter &lt;- iter + 1\n  }\n  val_x &lt;- (a + b) / 2\n  val_fx &lt;- fun_obj(val_x)\n  return(list(root = val_x, f_root = val_fx, iter = iter))\n}\n\n# Run it\nres_plot &lt;- my_bisec(fun_obj, a, b, ind_draw = TRUE)\n\n\n\n\n\n\n\nres_plot\n\n$root\n[1] 3.408203\n\n$f_root\n[1] 0.09048409\n\n$iter\n[1] 8\n\nres &lt;- my_bisec(fun_obj, a, b)\nplot(function(x) fun_obj(x), from = a, to = b)\nabline(h = 0, col = \"blue\", lty = 2)\ntitle(main = paste0(\"Bisection Method with \", res$iter, \" iterations\"))\nabline(v = res$root, col = \"red\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5, \n     labels = paste0(\"Root ~= \", round(res$root, 3)), \n     col = \"red\", pos = 3, cex = 0.9, font = 2)\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Newton-Raphson method\nThe Newton-Raphson method (or simply Newton‚Äôs method) is an iterative numerical method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\nHere, we assume that the function \\(f\\) is differentiable. The idea here is to use the Taylor expansion of the function. Suppose we are search a small neighbour of the solution \\(x \\in \\mathbb R\\), say \\(x_j \\in \\mathbb R\\) is a small number. Then Then we first order Taylor series to approximate \\(f(x_j+h)\\) around \\(x_j\\) is \\[\nf(x)\\approx f(x_j) +  f^\\prime(x_j) (x-x_j),\n\\] where \\(f^\\prime(x) := \\partial_x f(x)\\) is the first derivative of \\(f(x)\\). So the root of this approximation can be improved by updating its place to where \\(f(x_{j+1}) = 0\\).\nSo if \\(f(x_j+h)\\) is the root, then we have \\[ 0 = f(x_j) + f^\\prime(x_j) h \\implies h = -\\frac{f(x_j)}{f^\\prime(x_j)}.\\]\nThen, we can come back to \\(x_{j+1}= x_j+h\\), and plug the value of \\(h\\) in from above, we have \\[\nx_{j+1} = x_j - \\frac{f(x_j)}{f^\\prime(x_j)}.\n\\]\nThe algorithm is given as below:\n\nLet \\(f:\\mathbb R\\to\\mathbb R\\) be differentiable.\nStep 0: Choose a function \\(f(x)\\): This is the function for which you want to find a root (i.e., solve \\(f(x) = 0\\)).\nStep 1: Calculate the derivative \\(f^\\prime(x)\\): You will need it to apply the formula.\nStep 2: Make an initial guess \\(x_0\\): Select a starting point \\(x_0\\) near the expected root.\nStep 3: Update the estimate: Use the Newton‚Äôs method formula to compute the next estimate \\(x_1\\) using \\(x_0\\) by \\[x_{j+1} = x_j - \\frac{f(x_j)}{f^\\prime(x_j)}.\\]\nStep 4: Repeat Steps 2 and 3 until the values converge to a root or reach a desired tolerance.\n\n\n## Function and derivative\nf  &lt;- function(x) 0.5*x^3 - 0.5*x - 18\ndf &lt;- function(x) 1.5*x^2 - 0.5\n\n## Newton‚ÄìRaphson with iterate tracking\nnewton_raphson &lt;- function(f, df, x0, tol = 1e-5, \n                           maxit = 100, eps = 1e-5) {\n  x &lt;- x0\n  xs &lt;- x0      # store iterates (x0, x1, x2, ...)\n  for (k in 1:maxit) {\n    fx  &lt;- f(x)\n    dfx &lt;- df(x)\n    x_new &lt;- x - fx/dfx\n    xs &lt;- c(xs, x_new)\n    if (abs(x_new - x) &lt; tol || abs(fx) &lt; tol) {\n      return(list(root = x_new, iter = k, path = xs))\n    }\n    x &lt;- x_new\n  }\n  list(root = x, iter = maxit, path = xs)\n}\n\n## Starting point\n\nIf we start at -1 which is far away from\n\nx0 &lt;- -1\nres &lt;- newton_raphson(f, df, x0)\na &lt;- -2; b &lt;- 5\nplot(function(x) f(x), from = a, to = b, \n     xlab = \"x\", ylab = \"f(x)\",\n     main = paste(\"Newton-Raphson (Iterations:\", res$iter, \")\"))\nabline(h = 0, col = \"blue\", lty = 2)\n\n## Draw vertical lines for each iterate with labels 0,1,2,...\nfor (i in seq_along(res$path)) {\n  xi &lt;- res$path[i]\n  abline(v = xi, col = \"red\", lty = 2)\n  text(xi, par(\"usr\")[3] + 2, labels = i - 1, col = \"blue\", pos = 3, cex = 0.9)\n}\n\n## Final root marker + label\nabline(v = res$root, col = \"darkgreen\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5,\n     labels = paste0(\"Root ~= \", round(res$root, 5),\n                     \" ; f(root) ~= \", signif(f(res$root), 3)),\n     col = \"darkgreen\", pos = 3, cex = 0.95, font = 2)\n\n\n\n\n\n\n\nres\n\n$root\n[1] 3.402848\n\n$iter\n[1] 9\n\n$path\n [1] -1.000000 17.000000 11.387991  7.704327  5.368534  4.042133  3.500619\n [8]  3.405629  3.402850  3.402848\n\n\nIf we start at 3 which is near to the point\n\nx0 &lt;- 3\nres &lt;- newton_raphson(f, df, x0)\n## Plot range that shows the iterates and root\na &lt;- -2; b &lt;- 5\nplot(function(x) f(x), from = a, to = b, \n     xlab = \"x\", ylab = \"f(x)\",\n     main = paste(\"Newton-Raphson (Iterations:\", res$iter, \")\"))\nabline(h = 0, col = \"blue\", lty = 2)\n\n## Draw vertical lines for each iterate with labels 0,1,2,...\nfor (i in seq_along(res$path)) {\n  xi &lt;- res$path[i]\n  abline(v = xi, col = \"red\", lty = 2)\n  text(xi, par(\"usr\")[3] + 2, labels = i - 1, col = \"blue\", pos = 3, cex = 0.9)\n}\n\n## Final root marker + label\nabline(v = res$root, col = \"darkgreen\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5,\n     labels = paste0(\"Root ~= \", round(res$root, 5),\n                     \" ; f(root) ~= \", signif(f(res$root), 3)),\n     col = \"darkgreen\", pos = 3, cex = 0.95, font = 2)\n\n\n\n\n\n\n\nres\n\n$root\n[1] 3.402848\n\n$iter\n[1] 4\n\n$path\n[1] 3.000000 3.461538 3.403866 3.402848 3.402848\n\n\nRemarks:\n\nAssumptions: \\(f\\) is differentiable in a neighborhood of the root \\(r\\).\nFailure cases: if \\(f^\\prime(x_j)=0\\) (or is very small), the update is ill-defined/unstable; if the initial guess is far, the method can diverge or jump to a different root.\nPractical checks: stop when \\(|f(x_j)|\\le \\delta\\) or \\(|x_{j+1}-x_j| \\le \\delta\\) is below tolerance \\(\\delta\\).\n\n\n\n2.5.5 Second Method\nThe secant method can be thought of as a finite-difference approximation of Newton‚Äôs method, so it is considered a quasi-Newton method. It is simialr to Newton‚Äôs method, but it does not require the computation of the derivative of the function. Instead, it approximates the derivative using two previous points.\nIn the second method, we require the first two points, say \\(x_0, x_1 \\in \\mathbb R\\). Then, we can approximate the derivative of \\(f\\) at \\(x_1\\) using the finite difference formula. Instead of calculate the derivative \\(f^\\prime(x_1)\\), we approximate it as using the secant line. In calculate, we know that, \\(f^\\prime(x_1) \\approx \\frac{f(x_1)-f(x_0)}{x_1-x_0}\\), if \\(x_1\\) and \\(x_0\\) are close. Then, we can plug this approximation into the Newton‚Äôs update formula to get \\[x_j  = x_{j-1}  - f(x_{j-1}) \\frac{x_{j-1}-x_{j-2}}{f(x_{j-1}) - f(x_{j-2})} = \\frac{x_{j-2} f\\left(x_{j-1}\\right)-x_{j-1} f\\left(x_{j-2}\\right)}{f\\left(x_{j-1}\\right)-f\\left(x_{j-2}\\right)} .\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#hill-climbing",
    "href": "02-optimization.html#hill-climbing",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.6 Hill climbing",
    "text": "2.6 Hill climbing\nIn numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. The Newton method and secant method can be thought as questions in hill climbing.\nThe algorithm starts with an arbitrary solution to a problem, then iteratively makes small changes to the solution, each time moving to a neighboring solution that is better than the current one. The process continues until no neighboring solution is better than the current solution, at which point the algorithm terminates.\nIn the world of optimization, finding the best solution to complex problems can be challenging, especially when the solution space is vast and filled with local optima.\n\n2.6.1 In R\nuniroot(), optim() , nlm(), and mle() functions. Note that you may approximate the derivative/gradient.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#converegence",
    "href": "02-optimization.html#converegence",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.7 Converegence",
    "text": "2.7 Converegence\nIn order to compare the efficiency of the set of algorithms, one may compare their abilities for finding the optimals. However, what if, say, two algorithms both can find optimals, which one is better? The convergence rate comes in. Convergence rate is a measure of how quickly an iterative algorithm approaches its limit or optimal solution, which mean, how fast the algorithm converges to the optimals.\nIn the previous lecture(s), we saw that we can use R functions such microbenchmark::microbenchmark(), to measure the performance. However, it may takes a long time and a lot of computational resources. For such cases, we may use the theoretical convergence rate to compare the efficiency of the algorithms.\nThe convergence rate is often classified into several categories. It acts like the sequence \\(\\{x_j\\}\\) we learned in grade schools. Here, \\(\\{x_j\\}\\) is a sequence of estimates generated by the algorithm at each iterations, and \\(x^*\\) is the true solution or optimal value we are trying to reach. The error at iteration \\(n\\) is defined as \\(e_j = d(x_j,x^*)\\), where the typical metric here is the absolute distance \\(d(x_j,x^*)=|x_j-x^*|\\) (note, in spaces, different metric to define the distance). The convergence rate describes how quickly this error sequence \\(\\{e_j\\}\\) decreases as \\(j\\) increases. For\n\\[\n\\lim_{j \\to \\infty} \\frac{\\left|x_{j+1}-x^* \\right|}{\\left|x_j-x^* \\right|^q }=\\mu.\n\\]\n\n2.7.1 Linear Convergence\nIf order \\(q = 1\\) and \\(0 &lt; \\mu &lt; 1\\), the sequence \\(\\{x_j\\}\\in\\mathbb R^d\\) converges to \\(x^*\\) linearly. That is, \\(x_j\\to x^*\\) as \\(j\\to\\infty\\) in \\(\\mathbb R^d\\) if there existence a constant \\(\\mu\\) such that \\[\n  \\frac{\\left\\|x_{n+1}-x_{\\infty}\\right\\|}{\\left\\|x_n-x_{\\infty}\\right\\|} \\le \\mu,\\quad \\text{ as } \\quad n\\to\\infty.\n\\] This means that the error decreases proportionally to its current value, leading to a steady but relatively slow convergence.\n\n\n2.7.2 Superlinear Convergence\nSuppose \\(\\{x_n\\}\\) converges to \\(x^*\\), if order \\(q = 1\\) and \\(\\mu = 0\\), the sequence \\(\\{x_n\\}\\) converges to \\(x^*\\) superlinearly. That is, \\(x_n\\) is said to be converges to \\(x^*\\) as \\(n\\to\\infty\\) superlinearly if\n\\[\n\\lim _{n \\to \\infty} \\frac{\\left\\|x_{n+1}-x_{\\infty}\\right\\|}{\\left\\|x_n-x_{\\infty}\\right\\|}=0.\n\\] It is clearly that the superlinear is a stronger condition than the linear convergence, such that \\(\\mu=0\\).\n\n\n2.7.3 Quadratic Convergence\nIf order \\(q = 2\\) and \\(\\mu &gt; 0\\), the sequence \\(\\{x_n\\}\\) converges to \\(x^*\\) quadratically. That is, \\(x_n\\) is said to be converges to \\(x^*\\) as \\(n\\to\\infty\\) quadratically if\n\\[\n\\frac{\\left\\|x_{n+1}-x_{\\infty}\\right\\|}{\\left\\|x_n-x_{\\infty}\\right\\|^2} \\le \\mu, \\quad \\text{ as } \\quad n\\to\\infty.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#heuristic-algorithms",
    "href": "02-optimization.html#heuristic-algorithms",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.8 Heuristic Algorithms",
    "text": "2.8 Heuristic Algorithms\nMany of the heuristic algorithms are inspired by the nature, such as the genetic algorithm (GA) and particle swarm optimization (PSO). These algorithms are often used for complex optimization problems where traditional methods may struggle to find a solution. Some of the popular heuristic algorithms include:\n\nGenetic Algorithm (GA)\nParticle Swarm Optimization (PSO)\nSimulated Annealing (SA)\nAnt Colony Optimization (ACO)\n\n\n2.8.1 Simulating Annealing\nSimulated annealing (SA) is a stochastic technique for approximating the global optimum of a given function.\nInspired by the physical process of annealing in metallurgy, Simulated Annealing is a probabilistic technique used for solving both combinatorial and continuous optimization problems.\nWhat is Simulated Annealing?\nSimulated Annealing is an optimization algorithm designed to search for an optimal or near-optimal solution in a large solution space. The name and concept are derived from the process of annealing in metallurgy, where a material is heated and then slowly cooled to remove defects and achieve a stable crystalline structure. In Simulated Annealing, the ‚Äúheat‚Äù corresponds to the degree of randomness in the search process, which decreases over time (cooling schedule) to refine the solution. The method is widely used in combinatorial optimization, where problems often have numerous local optima that standard techniques like gradient descent might get stuck in. Simulated Annealing excels in escaping these local minima by introducing controlled randomness in its search, allowing for a more thorough exploration of the solution space.\nSome terminology:\n\nTemperature: controls how likely the algorithm is to accept worse solutions as it explores the search space.\n\n\nStep 1 (Initilization): Begin with an initial solution \\(S_Œø\\) and an initial temperature \\(T_Œø\\).\nStep 2 (Neighborhood Search): At step \\(j\\), a new solution \\(S^\\prime\\) is generated by making a small change (or perturbation) to \\(S_j\\).\nStep 3 (Evaluation): evaluate the objective function \\(f(S^\\prime)\\) Step 3.1: If \\(f^(S^\\prime)\\) is better than \\(f(S_j)\\), we accept it and take it as \\(S_{j+1}\\). Step 3.2: If \\(f(S^\\prime)\\) is worse than \\(f(S_j)\\), we may still accept it with a certain probability \\(P(S_{j+1}=S^\\prime)=\\exp(-\\Delta E/T_j)\\), where \\(E\\) is the energy \\(f(S^\\prime)-f(S_j)\\).\nStep 4 Cooling Schedule: Decrease the temperature according to a cooling schedule, e.g., \\(T_{j+1} = \\alpha T_j\\) where \\(\\alpha \\in (0,1)\\) is a cooling rate.\nStep 5 (Evaluation): Repeat Steps 2 and 3 for a certain number of iterations or until convergence criteria are met.\n\nExample:\nFigure 1 in my paper\nAdvantages:\n\nGlobal optimization\nFlexibility\nIntuitive\nDerivative?\n\nLimitations:\n\nParameter semsitivity\nComputational time\nSlow convergence\n\n\n2.8.1.1 R implementation\nAn paper about an implementation in R by Husmann et al. and another package called GenSA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#genetic-algorthm",
    "href": "02-optimization.html#genetic-algorthm",
    "title": "2¬† Numerical Approaches and Optimization in 1-D",
    "section": "2.9 Genetic Algorthm",
    "text": "2.9 Genetic Algorthm\nGenetic Algorithm (GA) is a metaheuristic optimization technique inspired by the process of natural evolution/selection.\nGA are based on an analogy with the genetic structure and behavior of chromosomes of the population.\nSTEP 1: Start with an initial generation \\(G_0\\) of potential solutions (individuals). Each individual is represented by a chromosome, which is typically encoded as a binary string, real-valued vector, or other suitable representation. Evaluate the objective function on those points.\nStep 2: Generate the next generation \\(G_{j+1}\\) from the current generation \\(G_j\\) using genetic operators: a). Selection: Retain the individual that is considered as good b). Crossover: Create children variables from the parents c). Mutation\nStep 3: Repeat Step 2 until the algorithm converges or reaches a stopping criterion.\n\n2.9.1 Particle Swarm Optimization\nParticle Swarm Optimization (PSO) was proposed by Kennedy and Eberhart in 1995. It is inspried by the movement of the species in nature, such as fishes or birds.\nThe algorithm is based on a population, not a single current point.\nAt iteration \\(n\\) of the algorithm, a particle has a velocity \\(v(n)\\) that depends on the follows.\n\nThe location of the best objective function value that it has encountered, \\(s(n)\\).\nThe location of the best objective function value among its neighbors, \\(g(n)\\).\nThe previous velocity \\(v(n ‚Äì 1)\\).\n\nThe position of a particle x(n) updates according to its velocity: \\[x(n+1)=x(n)+v(n),\\] adjusted to stay within the bounds. The velocity of a particle updates approximately according to this equation:\n\\[v(n+1) = W(n)v(n)+r(1)(s(n)‚àíx(n))+r(2)(g(n)‚àíx(n)).\\]\nHere, \\(r(1),r(2) \\in [0,1]\\) are random scalar values, and \\(W(n)\\) is an inertia factor that adjusts during the iterations. The full algorithm uses randomly varying neighborhoods and includes modifications when it encounters an improving point.\nNote: There are A LOT of variations of the PSO and other swarm-based algorithms used in the literature.\nIn R, there is a PSO implementation in the pso package. The associated manual may be found here.\n\nExamples are borrowed from the following sources:\n\nPeng, R.D. Advanced Statistical Computing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Numerical Approaches and Optimization in 1-D</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html",
    "href": "03-generating-rv.html",
    "title": "3¬† Generating Random Variables",
    "section": "",
    "text": "3.1 Overview\nOne of the fundamental tools required in computational statistics is the ability to simulate random variables (rvs) from specified probability distributions.\nIn the simplest case, to simulate drawing an observation at random from a finite population, a method of generating rvs from the discrete uniform distribution is required. Therefore, a suitable generator of uniform pseudo-random numbers is essential.\nMethods for generating random variates from other probability distributions all depend on the uniform random number generator (RNG).\nIn the Appendices, we have seen that how to use the built-in R functions to generate RVs from some common distributions, such as runif(), rnorm(), rbinom(), etc. In this Section, we will go over some of the common methods to generate RVs from a costume distributions.\nExample Theorem",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html#overview",
    "href": "03-generating-rv.html#overview",
    "title": "3¬† Generating Random Variables",
    "section": "",
    "text": "If we already have a finite population of size \\(N\\) with values \\(x_1, x_2, \\ldots, x_N\\) in hand, we can sample from this population with and without replacement.\n\nset.seed(777)\nsample(c(0,1), size = 10, replace = TRUE)  # with replacement\n\n [1] 1 0 1 1 1 1 0 1 1 1\n\n# Lottery ticket\nsample(1:999999, size = 5, replace = FALSE)\n\n[1] 567561 203418 450070 287692 435311\n\nsample(toupper(letters))\n\n [1] \"H\" \"N\" \"J\" \"Y\" \"B\" \"U\" \"F\" \"P\" \"Z\" \"V\" \"W\" \"I\" \"L\" \"A\" \"G\" \"Q\" \"X\" \"D\" \"M\"\n[20] \"R\" \"C\" \"T\" \"O\" \"K\" \"E\" \"S\"\n\n\n\n\n\n\nTable¬†3.1: Common probability distributions and their corresponding R functions for cumulative distribution function (CDF) and random number generation (borrowed from Table 3.1 in reference [2]).\n\n\n\n\n\nDistribution\ncdf\nGenerator\nParameters\n\n\n\n\nbeta\npbeta\nrbeta\nshape1, shape2\n\n\nbinomial\npbinom\nrbinom\nsize, prob\n\n\nchi-squared\npchisq\nrchisq\ndf\n\n\nexponential\npexp\nrexp\nrate\n\n\nF\npf\nrf\ndf1, df2\n\n\ngamma\npgamma\nrgamma\nshape, rate or scale\n\n\ngeometric\npgeom\nrgeom\nprob\n\n\nlognormal\nplnorm\nrlnorm\nmeanlog, sdlog\n\n\nnegative binomial\npnbinom\nrnbinom\nsize, prob\n\n\nnormal\npnorm\nrnorm\nmean, sd\n\n\nPoisson\nppois\nrpois\nlambda\n\n\nStudent‚Äôs t\npt\nrt\ndf\n\n\nuniform\npunif\nrunif\nmin, max",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html#inverse-transformation-method",
    "href": "03-generating-rv.html#inverse-transformation-method",
    "title": "3¬† Generating Random Variables",
    "section": "3.2 Inverse Transformation Method",
    "text": "3.2 Inverse Transformation Method\nThe first method to simulate rvs is the inverse transformation method (ITM).\n\nIf \\(X\\sim F_X\\) is a continuous rv, then the rv \\(U = F_X(x) \\sim \\operatorname{Unif}(0,1)\\).\n\nITM of generating rvs applies the probability integral transformation. Define the inverse transformation \\[ F^{‚àí1}_X(u) = \\inf\\{x : F_X(x) = u\\},\\quad  0 &lt; u &lt; 1.\\] Then, if \\(U \\sim \\operatorname{Unif}(0,1)\\), the rv \\(X = F^{‚àí1}_X(U)\\) has the distribution \\(F_X\\). This can be shown as, for all \\(x \\in \\mathbb{R}\\)\n\\[\n\\begin{aligned}\nP\\left(F_X^{-1}(U) \\leq x\\right) & =P\\left(\\inf \\left\\{t: F_X(t)=U\\right\\} \\leq x\\right) \\\\\n& =P\\left(U \\leq F_X(x)\\right) \\\\\n& =F_U\\left(F_X(x)\\right)=F_X(x),\n\\end{aligned}\n\\] Hence, \\(F_X^{-1}(U)\\) and \\(X\\) have the same distribution. So, in order to generate rv \\(X\\), we can simulate \\(U\\sim \\operatorname{Unif}(0,1)\\) first, then apply the inverse \\(F_X^{-1}(u)\\).\n\n\n\n\n\n\nNoteProcedure with inverse transformation method\n\n\n\nGiven a distribution function \\(F_X(\\cdot)\\), we can simulate/generate a rv \\(X\\) using the ITM in three steps:\n\nDerive the inverse function \\(F_X^{-1}(u)\\).\nWrite a (R) command or function to compute \\(F_X^{-1}(u)\\).\nFor each random variate required:\n\n\n\nGenerate a random \\(u\\sim \\operatorname{Unif}(0,1)\\).\nObtain \\(x = F_X^{-1}(u)\\).\n\n\n\n\n3.2.1 Continuous case\nWhen the distribution function \\(F_X(\\cdot)\\) is continuous, the ITM is straightforward to implement.\n\nSuppose we want to use the ITM to simulate \\(N=1000\\) rvs from the density \\(f_X(x)=3x^2,\\quad x\\in(0,1)\\).\n\nThe cdf is \\(F_X(x)=x^3\\), so the inverse function is \\(F_X^{-1}(u)=u^{1/3}\\).\nSimulate \\(N=1000\\) rvs from \\(u\\sim\\operatorname{Unif}(0,1)\\) and apply the inverse function to obtain the 1000 \\(x\\) values.\n\n\nset.seed(777)\nN &lt;- 1000\nuVec &lt;- runif(N)\nxVec &lt;- uVec^(1/3)\n\ndf &lt;- data.frame(x = xVec)\n\n# Density histogram with theoretical density overlay\nggplot(df, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30,\n                 fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = function(x) 3*x^2,\n                color = \"red\", size = 1) +\n  labs(title = expression(f(x) == 3*x^2),\n       y = \"Density\", x = \"x\") + theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSuppose \\(X\\sim \\exp(\\lambda)\\) where \\(\\lambda\\) is the rate parameter. Then \\(F_X(x) = 1 - e^{-\\lambda x}\\), so the inverse function is \\(F_X^{-1}(u) = -\\frac{1}{\\lambda}\\log(1-u)\\). The other fact, is the \\(U\\) and \\(1-U\\) have the same distribution, so we can use either of them, i.e., \\(x= -\\frac{1}{\\lambda}\\log(u)\\) or \\(x= -\\frac{1}{\\lambda}\\log(1-u)\\).\n\nset.seed(777)\nN &lt;- 1000\nlambda &lt;- 0.7\nuVec &lt;- runif(N)\nxVec_1 &lt;- - (1/lambda) * log(uVec)\nxVec_2 &lt;- - (1/lambda) * log(1-uVec)\n\n# Put data into long format for ggplot\ndf &lt;- data.frame(\n  value = c(xVec_1, xVec_2),\n  method = rep(c(\"log(U)\", \"log(1-U)\"), each = N)\n)\n\n# Theoretical density function\nexp_density &lt;- function(x) lambda * exp(-lambda * x)\n\n# Plot\nggplot(df, aes(x = value, fill = method)) +\n  geom_histogram(aes(y = ..density..), bins = 40,\n                 position = \"identity\", alpha = 0.4, color = \"black\") +\n  stat_function(fun = exp_density, color = \"red\", size = 1) +\n  labs(title = bquote(\"Exponential(\" ~ lambda == .(lambda) ~ \")\"),\n       x = \"x\", y = \"Density\") \n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Discrete case\nAlthough it is slightly more complicated than the continuous case, the ITM can also be applied to discrete distributions. Why?\nFirst, in the discrete case, the cdf \\(F_X(x)\\) is NOT continuous, instead, a step function, so the inverse function \\(F_X^{-1}(u)\\) is not unique.\nHere, if we order the random variable \\[\\cdots &lt; x_{(i-1)} &lt; x_{(i)} &lt; x_{(i+1)}&lt; cdots,\\] then the inverse transformation is \\(F_X^{-1}(u)=x_i\\), where \\(F_X(x_{(i-1)}) &lt; u \\leq F_X(x_{(i)})\\).\nThen the procedure is:\n\n\n\n\n\n\nNoteProcedure with ITM for discrete case\n\n\n\n\nDerive the cdf \\(F_X(x)\\) and tabulate the values of \\(x_i\\) and \\(F_X(x_i)\\).\nWrite a (R) command or function to compute \\(F_X^{-1}(u)\\).\nFor each random variate required:\n\n\n\nGenerate a random \\(u\\sim\\operatorname{Unif}(0,1)\\).\nFind \\(x_i\\) such that \\(F_X(x_{(i-1)}) &lt; u \\leq F_X(x_{(i)})\\) and set \\(x = x_i\\).\n\n\n\n\nIn this example, \\(F_X(0) = f_X(0) = 1 - p\\) and \\(F_X(1) = 1\\).\nThus, \\[\nF_X^{-1}(u) =\n\\begin{cases}\n1, & u &gt; 0.6,\\\\\n0, & u \\leq 0.6.\n\\end{cases}\n\\]\nThe generator should therefore deliver the numerical value of the logical expression \\(u &gt; 0.6\\).\n\nset.seed(777)\nn &lt;- 1000\np &lt;- 0.4\nu &lt;- runif(n)\nx &lt;- as.integer(u &gt; 0.6)  # (u &gt; 0.6) is a logical vector\n\n(m_x &lt;- mean(x));  (v_x &lt;- var(x))\n\n[1] 0.381\n\n\n[1] 0.2360751\n\n\nCompare the sample statistics with the theoretical moments. The sample mean of a generated sample should be approximately \\(p = 0.4\\) and the sample variance should be approximately \\(p(1 - p) = 0.24\\), versus our simulated values 0.381 and 0.2360751.\n\n\nIn this example, we will use ITM to simulate \\(X\\sim \\operatorname{Geom}(1/4)\\).\nLet \\(q:=1-p\\). The pmf is \\(f(x) = p q^x\\), \\(x = 0,1,2,\\ldots\\). At the points of discontinuity \\(x = 0,1,2,\\ldots\\), the cdf is \\[\nF(x) = 1 - q^{x+1}.\n\\] For each sample element we need to generate a \\(u\\sim \\operatorname{Unif}(0,1)\\) and solve \\[\n1 - q^x &lt; u \\leq 1 - q^{x+1}.\n\\]\nWhich is equivalent to \\(x &lt; \\frac{\\log(1 - u)}{\\log(q)} \\leq x+1.\\) The solution is \\[\nx + 1 = \\left\\lceil \\frac{\\log(1 - u)}{\\log(q)} \\right\\rceil,\n\\] where \\(\\lceil \\cdot \\rceil\\) denotes the ceiling function (and \\(\\lfloor \\cdot \\rfloor\\) is the floor function). Hence, we have,\n\nset.seed(999)\nn &lt;- 1000\np &lt;- 0.25\nu &lt;- runif(n)\nk1 &lt;- ceiling(log(1-u) / log(1-p)) - 1\n\nNote again that \\(U\\) and \\(1 - U\\) have the same distribution. Also, the probability that \\(\\log(1 - u)/\\log(1 - p)\\) equals an integer is zero. Thus, we can simplify it to\n\nk2 &lt;- floor(log(u) / log(1-p))\ndf &lt;- data.frame(\n  value = c(k1, k2),\n  group = rep(c(\"k1\", \"k2\"), each = length(k1))\n)\n\n# Plot both histograms side by side\nggplot(df, aes(x = value, fill = group)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 40, color = \"black\") +\n  labs(title = \"Histograms of k1 and k2\", x = \"Value\", y = \"Count\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\nThe geometric distribution was particularly easy to simulate by the inverse transform method because it was easy to solve the inequality \\(F(x-1) &lt; u \\leq F(x)\\) rather than compare each \\(u\\) to all the possible values \\(F(x)\\). \nThe same method applied to the Poisson distribution is more complicated because we do not have an explicit formula for the value of \\(x\\) such that \\[\nF(x-1) &lt; u \\leq F(x).\n\\]\nThe R function generates random Poisson samples. The basic method to generate a Poisson (\\(\\lambda\\)) variate is to generate and store the cdf via the recursive formula \\[\nf(x+1) = \\frac{\\lambda f(x)}{x+1},\n\\qquad\nF(x+1) = F(x) + f(x+1).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html#acceptance-rejection-method",
    "href": "03-generating-rv.html#acceptance-rejection-method",
    "title": "3¬† Generating Random Variables",
    "section": "3.3 Acceptance-Rejection Method",
    "text": "3.3 Acceptance-Rejection Method\nIn the previous section, we have seen that the ITM is straightforward to implement when the inverse cdf is available in closed form. However, for many distributions, the inverse cdf is not available in closed form or is difficult to compute. In those cases, we need to have other strategies!\nThe acceptance-rejection method (ARM) is a general method for generating rvs from a distribution with pdf \\(f_X(x)\\), when the inverse cdf is not available in closed form or is difficult to compute.\n\nSuppose \\(X\\) and \\(Y\\) are rvs with pdfs/pmds \\(f_X(x)\\) and \\(g_Y(y)\\), respectively. Further we suppose there is a constant \\(k\\) such that \\[\n\\frac{f_X(t)}{g_Y(t)} \\leq k,\n\\] for all \\(t\\) such that \\(f_X(t) &gt; 0\\).\nThen we can simulate \\(X\\) using the following procedure:\n\nFind a rv \\(Y\\) with density \\(g_Y(\\cdot)\\) satisfying \\(f_X(t)/g_Y(t) \\le k,\\) for all \\(t\\) such that \\(f(t) &gt; 0\\).\nFor each rv, required:\n\n\n\nGenerate a random \\(y\\) from the distribution with density \\(g_Y\\).\nGenerate a random \\(u\\sim\\operatorname{Unif}(0,1)\\).\nIf \\(u &lt; f_X(y)/(k g_Y(y))\\), accept \\(y\\) and set \\(x = y\\); o.w. reject \\(y\\) and jump back to (i)\n\n\nWhy it work?Note that in Step 2c, \\[\nP(\\text{accept} \\mid Y)\n= P\\!\\left(U &lt; \\frac{f(Y)}{k g(Y)} \\,\\Big|\\, Y\\right)\n= \\frac{f_X(Y)}{k g_X(Y)}.\n\\]\nThe total probability of acceptance for any iteration is therefore \\[\n\\sum_y P(\\text{accept} \\mid y) P(Y = y)\n= \\sum_y \\frac{f(y)}{k g(y)} g(y)\n= \\frac{1}{k},\n\\] and the number of iterations until acceptance has the geometric distribution with mean \\(k\\). That means, in order to sample \\(X\\), in average, we need \\(k\\) iterations.\nNote: The choice of \\(Y\\) and \\(k\\) is crucial for the efficiency of the ARM. A poor choice can lead to a large \\(k\\), resulting in many rejections and inefficiency. We want \\(Y\\) to be easy to simulate, and \\(k\\) to be as small as possible.\nDoes this have anything to do with \\(X\\)?\nTo see that the accepted sample has the same distribution as \\(X\\), apply Bayes‚Äô Theorem. In the discrete case, for each \\(\\ell\\) such that \\(f(\\ell) &gt; 0\\), \\[\nP(\\ell \\mid \\text{accepted})\n= \\frac{P(\\text{accepted} \\mid \\ell) g(\\ell)}{P(\\text{accepted})}\n= \\frac{\\big[f(\\ell)/(k g(\\ell))\\big] g(\\ell)}{1/k}\n= f(\\ell).\n\\]\n\nThis example illustrates the acceptance‚Äìrejection method for the beta distribution.\nQ: On average, how many random numbers must be simulated to generate \\(N=1000\\) samples from the \\(\\operatorname{Beta}(\\alpha=2,\\beta=2)\\) distribution by ARM?\nA: Depends on the upper bound \\(k\\) of \\(f_X(t)/g_Y(t)\\), which depends on the choice of the function \\(g_Y(\\cdot)\\).\nRecall that the \\(\\operatorname{Beta}(2,2)\\) density is \\[\nf(t) = 6t(1-t), \\quad 0 &lt; t &lt; 1.\n\\] Let \\(g(\\cdot)\\) be the Uniform(0,1) density. Then \\[\n\\frac{f(t)}{g(t)} = \\frac{6t(1-t)}{(1)} = 6t(1-t) \\leq k \\quad \\text{for all } 0 &lt; t &lt; 1.\n\\] It is easy to see that \\(k = 6\\). A random \\(x\\) from \\(g(x)\\) is accepted if \\[\n\\frac{f(x)}{kg(x)} = \\frac{6x(1-x)}{6(1)} = x(1-x) &gt; u.\n\\]\nOn average, \\(kN = 6\\cdot 1000 =6000\\) iterations (12000 random numbers as we need \\(X\\) and \\(Y\\)) will be required for \\(N=1000\\). In the following simulation, the counter \\(\\operatorname{iter}\\) for iterations is not necessary, but included to record how many iterations were actually needed to generate the 1000 beta rvs.\n\nset.seed(7777)\nN &lt;- 1000\nell_accept &lt;- 0       # counter for accepted\niter &lt;- 0       # iterations\ny &lt;- rep(0, N)\n\nwhile (ell_accept &lt; N) {\n  u &lt;- runif(1)\n  iter &lt;- iter + 1\n  x &lt;- runif(1)   # random variate from g\n  if (x * (1-x) &gt; u) {\n    # we accept x\n    ell_accept &lt;- ell_accept + 1\n    y[ell_accept] &lt;- x\n  }\n}\n\niter\n\n[1] 5972\n\n\nIn this simulation, 5972 iterations ( 1.1944^{4} random numbers) were required to generate the 1000 beta samples.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html#using-known-probability-distribution-theory",
    "href": "03-generating-rv.html#using-known-probability-distribution-theory",
    "title": "3¬† Generating Random Variables",
    "section": "3.4 Using known probability distribution theory",
    "text": "3.4 Using known probability distribution theory\nMany types of transformations other than the probability inverse transformation can be applied to simulate random variables. Some examples are\n1). If \\(Z \\sim N(0,1)\\), then \\(V = Z^2 \\sim \\chi^2(1)\\).\n2). If \\(Z_1,\\ldots,Z_n \\sim N(0,1)\\) are independent, then \\[\n  U = \\sum_{i=1}^n Z_i^2 \\sim \\chi^2(n).\n  \\]\n3). If \\(U \\sim \\chi^2(m)\\) and \\(V \\sim \\chi^2(n)\\) are independent, then \\[\n  F = \\frac{U/m}{V/n}\n  \\] has the \\(F\\) distribution with \\((m,n)\\) degrees of freedom.\n4). If \\(Z \\sim N(0,1)\\) and \\(V \\sim \\chi^2(n)\\) are independent, then \\[\n  T = \\frac{Z}{\\sqrt{V/n}}\n  \\] has the Student \\(t\\) distribution with \\(n\\) degrees of freedom.\n5). If \\(U,V \\sim \\text{Unif}(0,1)\\) are independent, then \\[\n  Z_1 = \\sqrt{-2 \\log U}\\, \\cos(2\\pi V),\n  \\qquad\n  Z_2 = \\sqrt{-2 \\log U}\\, \\sin(2\\pi V)\n  \\] are independent standard normal variables.\n6). If \\(U \\sim \\text{Gamma}(r,\\lambda)\\) and \\(V \\sim \\text{Gamma}(s,\\lambda)\\) are independent, then \\[\n  X = \\frac{U}{U+V}\n  \\] has the \\(\\text{Beta}(r,s)\\) distribution.\n7). If \\(U,V \\sim \\text{Unif}(0,1)\\) are independent, then \\[\n  X = \\left\\lfloor 1 + \\frac{\\log(V)}{\\log\\big(1 - (1-\\theta)U\\big)} \\right\\rfloor.\n  \\] has logarithmic distribution with parameter \\(\\theta\\).\n\nUsing the distribution theory, we recall the relationship between beta and gamma distributions provides another beta generator.\nIf \\(U \\sim \\mathrm{Gamma}(r,\\lambda)\\) and \\(V \\sim \\mathrm{Gamma}(s,\\lambda)\\) are independent, then \\[\nX=\\frac{U}{U+V}\n\\] has the \\(\\mathrm{Beta}(r,s)\\) distribution. This transformation determines an algorithm for generating random \\(\\mathrm{Beta}(a,b)\\) variates.\n\nGenerate a random \\(u\\) from \\(\\mathrm{Gamma}(a,1)\\).\nGenerate a random \\(v\\) from \\(\\mathrm{Gamma}(b,1)\\).\nObtain \\(x=\\dfrac{u}{u+v}\\).\n\nThis method is applied below to generate a random \\(\\mathrm{Beta}(3,2)\\) sample.\n\nset.seed(777)\nn &lt;- 1000\na &lt;- 3\nb &lt;- 2\nu &lt;- rgamma(n, shape = a, rate = 1)\nv &lt;- rgamma(n, shape = b, rate = 1)\nx &lt;- u / (u + v)\n\nThe sample data can be compared with the Beta\\((3,2)\\) distribution using a quantile‚Äìquantile (QQ) plot. If the sampled distribution is Beta\\((3,2)\\), the QQ plot should be nearly linear.\n\nq &lt;- qbeta(ppoints(n), a, b)\nqqplot(q, x, cex = 0.25, xlab = \"Beta(3, 2)\", ylab = \"Sample\")\nabline(0, 1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html#sum-and-mixture",
    "href": "03-generating-rv.html#sum-and-mixture",
    "title": "3¬† Generating Random Variables",
    "section": "3.5 Sum and Mixture",
    "text": "3.5 Sum and Mixture\n\n3.5.1 Sum/Convolution\nLet \\(X_1, X_2, \\ldots, X_n \\overset{iid}{\\sim}F_X\\). Then we may consider the sum of the random variables \\(S_n:=\\sum_{i=1}^n X_i\\), with distribution \\(F_{S_n}\\), which can be referred as convoluton. We can simulate \\(S_n\\) by simulating \\(X_1, X_2, \\ldots, X_n\\) and summing them up. There are several common convolutions we have seen so far\n\nSum of \\(n\\) independent i.i.d. chi-square with degreee of freedom (df) 1 is chi-square with df \\(n\\).\nSum of \\(n\\) independent i.i.d. exponential with rate \\(\\lambda\\) is gamma with shape \\(n\\) and rate \\(\\lambda\\).\nSum of \\(n\\) independent i.i.d. geometric with parametric \\(p\\) is negative binomial with size \\(n\\) and parameter \\(p\\).\n\n\nIn order to simulate \\(\\chi^2\\) distribution with df k, we can simulate k independent standard normal rvs and sum their squares. Let‚Äôs simulate \\(n\\) independent \\(S \\sim \\chi^2(5)\\).\n\nFill an \\(n \\times k\\) matrix with \\(n k\\) realization of the random variables that follow \\(N(0,1)\\).\nSquare each entry in the matrix (1).\nCompute the row sums of the squared normals. Each row sum is one random observation from the \\(\\chi^2(k)\\) distribution.\n\n\nset.seed(777)\nk &lt;- 5\nn &lt;- 1000\nX &lt;- matrix(rnorm(n*k), nrow = n, ncol = k)^2\nX_row &lt;- rowSums(X)\nmean(X_row)\n\n[1] 5.065355\n\nmean(X_row^2)\n\n[1] 36.40181\n\n\n\n\n\n3.5.2 Mixture\nA mixture distribution is a probability distribution constructed as a weighted sum of other distributions. If \\(X\\) is a rv with a mixture distribution, then its pdf is given by \\[\nF_X(x) = \\sum_{i=1}^k \\alpha_i F_{X_i}(x),\n\\] where \\(\\alpha_i\\ge 0\\) and \\(\\sum_i \\alpha_i=1\\). We can just simply simulate each component \\(X_i\\) first, then multiply with their corresponding weights \\(\\alpha_i\\).\n\nSuppose \\(X\\) and \\(Y\\) is a mixture of two normal distributions, where \\(X\\sim N(\\mu_1,\\sigma_1^2)\\) with probability \\(\\alpha\\) and \\(Y\\sim N(\\mu_2,\\sigma_2^2)\\) with probability \\(1-\\alpha\\).\n\nn &lt;- 1000\nx1 &lt;- rnorm(n, 0, 1)\nx2 &lt;- rnorm(n, 3, 3)\ns_convolution &lt;- x1 + x2 #the convolution\n\nu &lt;- runif(n)\nk &lt;- as.integer(u &gt; 0.5) #vector of 0‚Äôs and 1‚Äôs\n\n## pay attention to it\nm_mixture &lt;- k * x1 + (1-k) * x2 #the mixture\ndf &lt;- data.frame(\n  value = c(s_convolution, m_mixture),\n  type = rep(c(\"convolution\", \"mixture\"), each = n)\n)\nggplot(df, aes(x = value, fill = type)) +\n  geom_histogram(alpha = 0.6, position = \"identity\", bins = 40, color = \"black\") +\n  labs(title = \"Histograms of convolution and mixture\", x = \"Value\", y = \"Count\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSimulate for mixture\n\n\n\nPay attention how a mixture is simulated. In particular, we first simulate a vector \\(U\\sim\\operatorname{Unif}(0,1)\\), then use it to select from which distribution/component we want to sample. It is not an (weighted) addition as in the convolution.\n\n\n\n\n\n\n\n\nNoteDifference between convolution and mixture\n\n\n\nConvolution is the sum of two independent rvs, while mixture is a weighted average of two rvs. Note: Mixture is non-normal! but the convolution is normal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "03-generating-rv.html#simulate-multivaraite-rvs",
    "href": "03-generating-rv.html#simulate-multivaraite-rvs",
    "title": "3¬† Generating Random Variables",
    "section": "3.6 Simulate Multivaraite RVs",
    "text": "3.6 Simulate Multivaraite RVs\nThis section presents generators for the multivariate normal distribution, multivariate normal mixtures, the Wishart distribution, and the uniform distribution on the sphere in \\(\\mathbb{R}^d\\).\n\n3.6.1 Multivariate normal distribution\nRecall that, from Definition 1 in Chapter 2, a \\(d\\)-dimensional random vector \\(X\\) is a multivariate normal distribution with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\), denoted by \\(X \\sim N_d(\\mu, \\Sigma)\\), if every linear combination of its components has a univariate normal distribution. That is, for every nonzero vector \\(a \\in \\mathbb{R}^d\\), the rv \\(a^T X\\) has a univariate normal distribution.\nTo write this in an explicit form, we have, \\(X = (X_1,\\dots,X_N)\\) follow a multivariate normal (MVN) distribution with mean vector \\(\\mu = (\\mu_1,\\dots,\\mu_N)\\) and covariance matrix \\(\\Sigma = (\\sigma_{ij})\\), if its joint density function is given by \\[\nf_X(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\right), \\quad x \\in \\mathbb{R}^p.\n\\]\nMore explicitly, we can write this as the \\[\\mu=(\\mu_1,\\dots,\\mu_p) = \\begin{pmatrix}\\mu_1\\\\\n\\vdots\\\\\n\\mu_p\\end{pmatrix}\\in \\mathbb{R}^p,\\] and \\[\\Sigma=(\\Sigma_{ij}) = (\\mathbb{C}ov(X_i,X_j))= \\left[\\begin{array}{cccc}\n\\sigma_{11} & \\sigma_{12} & \\ldots & \\sigma_{1 d} \\\\\n\\sigma_{21} & \\sigma_{22} & \\ldots & \\sigma_{2 d} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n\\sigma_{d 1} & \\sigma_{d 2} & \\ldots & \\sigma_{d d}\n\\end{array}\\right] \\in \\mathbb{R}^{d\\times d}.\n\\]\nA random vector \\(X\\sim N_d(\\mu,\\Sigma)\\) can be simulated by the following steps:\n\nSimulate \\(Z = (Z_1,\\ldots,Z_d)^T\\) where \\(Z_i \\overset{iid}{\\sim}N(0,1)\\).\nFind a vector \\(\\mu\\) and a matrix \\(A\\) and such that \\(\\Sigma = AA^T\\). The decomposition can be Cholesky decomposition, eigendecomposition, or singular value decomposition, etc.\n\nIn practice, we do not do this one at a time, we want to simulate \\(n\\) sample in as few steps as possible. The following procedure is more efficient. Typically, one applies the transformation to a data matrix and transforms the entire sample. Suppose that \\(Z = (Z_ij ) \\in \\mathbb{R}^{n\\times d}\\), where \\(Z_{ij}\\overset{iid}{\\sim}N(0,1)\\). Then the rows of Z are \\(n\\) random observations from the \\(d\\)-dimensional standard MVN distribution. The required transformation applied to the data matrix is \\[X = ZQ + J \\mu^\\top,\\] where \\(Q^\\top Q=\\Sigma\\), and \\(J\\) is a column vector of \\(n\\) ones. The rows of \\(X\\) are \\(n\\) random observations from the \\(d\\)-dimensional MVN distribution with mean vector \\(\\mu\\) and covariance matrix \\(\\Sigma\\).\n\nMethod for generating multivariate $n $ normal samples from the \\(N_d(\\mu,\\Sigma)\\) distribution.\nStep 1. Simulate \\(Z = (Z_{ij}) \\in \\mathbb{R}^{n\\times d}\\), where \\(Z_{ij} \\overset{iid}{\\sim}N(0,1)\\).\nStep 2. Compute the decomposition \\(\\Sigma = Q^\\top Q\\).\nStep 3. Compute \\(X = ZQ + J\\mu^\\top\\), where \\(J\\) is a column vector of \\(n\\) ones.\nStep 4. Return \\(X\\in \\mathbb{R}^{ n\\times d}\\), in which each of the \\(n\\) rows of \\(X\\) is a random observation from the \\(N_d(\\mu,\\Sigma)\\) distribution.\n\n\n3.6.1.1 A few things to be considered\n\nComputation of \\(J \\mu^\\top\\)\n\nRecall that \\(J\\) is a vector of 1‚Äôs (i.e., \\(J=(1,\\dots,1)\\in\\mathbb{R}^d\\) and \\(\\mu^\\top\\) is the transpose of \\(\\mu\\). So, \\(J \\mu^\\top\\) is a matrix with each row being \\(\\mu\\). In R, we can use matrix(mu, n, d, byrow = TRUE) to create this matrix. Also, for any \\(i\\) and \\(j\\), \\(\\mu_i=\\mu_j=c\\), where c is a constant, then we can write it as \\(c I_{n\\times d}\\), where \\(I_{n\\times d}\\) is a matrix of 1‚Äôs with dimension \\(n\\times d\\).\nZ &lt;- matrix(rnorm(n*d), nrow = n, ncol = d)\nX &lt;- Z %*% Q + matrix(mu, n, d, byrow = TRUE)\n\nDecomposition of \\(\\Sigma\\)\n\nThere are many different ways to decompose \\(\\Sigma\\). Recall that \\(\\Sigma\\) is a symmetric positive definite matrix, so we can use Cholesky decomposition, eigendecomposition, or singular value decomposition (SVD). In R, we can use chol(), eigen(), or svd() functions to compute the decomposition.\n\nCholeski decomposition: Choleski of a real symmetric positive-definite matrix is \\(X = Q^\\top Q\\), where \\(Q\\) is an upper triangular matrix.\nSpectral decomposition: The square root of the covariance is \\(Œ£^{1/2} = P^{1/2} \\Lambda P^{‚àí1}\\), where \\(\\Lambda\\) is the diagonal matrix with the eigenvalues of \\(\\Sigma\\) along the diagonal and \\(P\\) is the matrix whose columns are the eigenvectors of \\(\\Sigma\\) corresponding to the eigenvalues in \\(\\Lambda\\). This method can also be called the eigen-decomposition method. In the eigen-decomposition we have \\(P^{-1}= P^\\top\\) and therefore \\(Œ£^{1/2} = P \\Lambda^{1/2}P^\\top\\) . The matrix \\(Q = Œ£^{1/2}\\) is a factorization of \\(\\Sigma\\) such that \\(Q^\\top Q = \\Sigma\\).\nSingular Value Decomposition: The singular value decomposition (svd) generalizes the idea of eigenvectors to rectangular matrices. The svd of a matrix \\(X\\) is \\(X = U DV^\\top\\) , where D is a vector containing the singular values of \\(X\\), \\(U\\) is a matrix whose columns contain the left singular vectors of \\(X\\), and \\(V\\) is a matrix whose columns contain the right singular vectors of \\(X\\). The matrix \\(X\\) in this case is the population covariance matrix \\(\\Sigma\\), and \\(UV\\top = I\\). The svd of a symmetric positive definite matrix \\(\\Sigma\\) gives \\(U = V = P\\) and \\(Œ£^{1/2} = U D^{1/2}V^\\top\\) . Thus the svd method for this application is equivalent to the spectral decomposition method, but is less efficient because the svd method does not take advantage of the fact that the matrix \\(\\Sigma\\) is square symmetric.\n\n\n\n\n\n\n\nNotePerformance Comparison\n\n\n\n\nset.seed(777)\n\n\nn  &lt;- 100     # sample size per call\nd  &lt;- 30      # dimension\nN  &lt;- 1000    # number of distinct Sigmas to cycle through in Scenario A\nreps_A &lt;- 200 # microbenchmark repetitions for Scenario A\nreps_B &lt;- 500 # microbenchmark repetitions for Scenario B\nmu &lt;- numeric(d)\n\n## ====== MVN generators ======\nrmvn_eigen &lt;- function(n, mu, Sigma) {\n  ev &lt;- eigen(Sigma, symmetric = TRUE)\n  A  &lt;- ev$vectors %*% (sqrt(pmax(ev$values, 0)) * t(ev$vectors))\n  Z  &lt;- matrix(rnorm(n * length(mu)), n)\n  sweep(Z %*% A, 2, mu, `+`)\n}\n\nrmvn_svd &lt;- function(n, mu, Sigma) {\n  sv &lt;- svd(Sigma)\n  A  &lt;- sv$u %*% (sqrt(pmax(sv$d, 0)) * t(sv$v))\n  Z  &lt;- matrix(rnorm(n * length(mu)), n)\n  sweep(Z %*% A, 2, mu, `+`)\n}\n\nrmvn_chol &lt;- function(n, mu, Sigma) {\n  R  &lt;- chol(Sigma)\n  Z  &lt;- matrix(rnorm(n * length(mu)), n)\n  sweep(Z %*% R, 2, mu, `+`)\n}\n\n## ====== Utilities ======\nrand_cov &lt;- function(d) {\n  A &lt;- matrix(rnorm(d*d), d, d)\n  S &lt;- crossprod(A) / d\n  diag(S) &lt;- diag(S) + 1e-6\n  S\n}\n\ndrop_outliers &lt;- function(df) {\n  df %&gt;%\n    group_by(expr) %&gt;%\n    mutate(\n      q1 = quantile(time, 0.25),\n      q3 = quantile(time, 0.75),\n      iqr = q3 - q1,\n      lower = q1 - 1.5 * iqr,\n      upper = q3 + 1.5 * iqr\n    ) %&gt;%\n    filter(time &gt;= lower & time &lt;= upper) %&gt;%\n    ungroup()\n}\n\n## Precompute N random SPD matrices (same pool for all methods)\nSigma_list &lt;- replicate(N, rand_cov(d), simplify = FALSE)\n\n## ====== Scenario A: varying Sigma each call (factorize every time) ======\ni &lt;- 0\nnext_Sigma &lt;- function() { i &lt;&lt;- if (i == N) 1 else i + 1; Sigma_list[[i]] }\n\nbench_A &lt;- microbenchmark(\n  rmvn_eigen       = rmvn_eigen(n, mu, next_Sigma()),\n  rmvn_svd         = rmvn_svd(n,   mu, next_Sigma()),\n  rmvn_chol        = rmvn_chol(n,  mu, next_Sigma()),\n  MASS_mvrnorm     = MASS::mvrnorm(n, mu, next_Sigma()),\n  mvtnorm_rmvnorm  = mvtnorm::rmvnorm(n, mean = mu, sigma = next_Sigma()),\n  times = reps_A\n)\n\nsum_A &lt;- as.data.frame(bench_A) %&gt;%\n  group_by(expr) %&gt;%\n  summarize(median_ms = median(time)/1e6,\n            iqr_ms    = IQR(time)/1e6,\n            .groups = \"drop\") %&gt;%\n  arrange(median_ms) %&gt;%\n  mutate(scenario = \"A: varying Œ£\")\n\nprint(sum_A)\n\n# A tibble: 5 √ó 4\n  expr            median_ms iqr_ms scenario    \n  &lt;fct&gt;               &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 rmvn_chol           0.129 0.0113 A: varying Œ£\n2 MASS_mvrnorm        0.194 0.0153 A: varying Œ£\n3 rmvn_eigen          0.198 0.0171 A: varying Œ£\n4 rmvn_svd            0.245 0.0195 A: varying Œ£\n5 mvtnorm_rmvnorm     0.266 0.0215 A: varying Œ£\n\npA &lt;- drop_outliers(as.data.frame(bench_A)) %&gt;%\n  mutate(ms = time/1e6) %&gt;%\n  ggplot(aes(x = reorder(expr, ms, FUN = median), y = ms)) +\n  geom_boxplot() +\n  stat_summary(fun = median, geom = \"point\", shape = 21, size = 2, stroke = 0.6) +\n  coord_flip() +\n  labs(title = \"MVN generators ‚Äî Scenario A (varying Œ£, outliers removed)\",\n       x = NULL, y = \"Time (ms) per call)\") +\n  theme_minimal(base_size = 12)\n\nprint(pA)\n\n\n\n\n\n\n\n## ====== Scenario B: fixed Sigma, reuse factorization when possible ======\nSigma0 &lt;- Sigma_list[[1]]\n\nrmvn_chol_fixed &lt;- local({\n  R &lt;- chol(Sigma0)\n  function(n, mu) {\n    Z &lt;- matrix(rnorm(n * length(mu)), n)\n    sweep(Z %*% R, 2, mu, `+`)\n  }\n})\n\nrmvn_eigen_fixed &lt;- local({\n  ev &lt;- eigen(Sigma0, symmetric = TRUE)\n  A  &lt;- ev$vectors %*% (sqrt(pmax(ev$values, 0)) * t(ev$vectors))\n  function(n, mu) {\n    Z &lt;- matrix(rnorm(n * length(mu)), n)\n    sweep(Z %*% A, 2, mu, `+`)\n  }\n})\n\nrmvn_svd_fixed &lt;- local({\n  sv &lt;- svd(Sigma0)\n  A  &lt;- sv$u %*% (sqrt(pmax(sv$d, 0)) * t(sv$v))\n  function(n, mu) {\n    Z &lt;- matrix(rnorm(n * length(mu)), n)\n    sweep(Z %*% A, 2, mu, `+`)\n  }\n})\n\nbench_B &lt;- microbenchmark(\n  rmvn_chol_fixed   = rmvn_chol_fixed(n, mu),\n  rmvn_eigen_fixed  = rmvn_eigen_fixed(n, mu),\n  rmvn_svd_fixed    = rmvn_svd_fixed(n, mu),\n  MASS_mvrnorm      = MASS::mvrnorm(n, mu, Sigma0),                   # factorizes internally\n  mvtnorm_rmvnorm   = mvtnorm::rmvnorm(n, mean = mu, sigma = Sigma0), # Cholesky internally\n  times = reps_B\n)\n\nsum_B &lt;- as.data.frame(bench_B) %&gt;%\n  group_by(expr) %&gt;%\n  summarize(median_ms = median(time)/1e6,\n            iqr_ms    = IQR(time)/1e6,\n            .groups = \"drop\") %&gt;%\n  arrange(median_ms) %&gt;%\n  mutate(scenario = \"B: fixed Œ£\")\n\nprint(sum_B)\n\n# A tibble: 5 √ó 4\n  expr             median_ms  iqr_ms scenario  \n  &lt;fct&gt;                &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;     \n1 rmvn_chol_fixed      0.117 0.00887 B: fixed Œ£\n2 rmvn_eigen_fixed     0.117 0.00828 B: fixed Œ£\n3 rmvn_svd_fixed       0.117 0.00965 B: fixed Œ£\n4 MASS_mvrnorm         0.182 0.0130  B: fixed Œ£\n5 mvtnorm_rmvnorm      0.249 0.0171  B: fixed Œ£\n\npB &lt;- drop_outliers(as.data.frame(bench_B)) %&gt;%\n  mutate(ms = time/1e6) %&gt;%\n  ggplot(aes(x = reorder(expr, ms, FUN = median), y = ms)) +\n  geom_boxplot() +\n  stat_summary(fun = median, geom = \"point\", shape = 21, size = 2, stroke = 0.6) +\n  coord_flip() +\n  labs(title = \"MVN generators ‚Äî Scenario B (fixed Œ£; outliers removed)\",\n       x = NULL, y = \"Time (ms) per call)\") +\n  theme_minimal(base_size = 12)\n\nprint(pB)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.2 Mixture of Multivariate Normal\nA mixture of multivariate normal distributions is a convex combination of multivariate normal distributions. That is, the density function of a mixture of \\(k\\) multivariate normal distributions is given by \\[\n\\alpha N_p(\\mu_1,\\Sigma_1) + (1-\\alpha) N_p(\\mu_2,\\Sigma_2), \\quad 0 &lt; \\alpha &lt; 1.\n\\] Similar to the univariate case, different choice of \\(\\alpha\\) will lead to different degree of departure from the normal.\n\nGiven a \\(\\alpha\\), to simulatecallout-note a random sample from \\(\\alpha N_d(\\mu_1, \\Sigma_1) + (1 ‚àí p) N_d(\\mu_2, \\Sigma_2)\\)\nThere are two ways.\nWay 1:\n\nGenerate \\(U \\sim unif(0,1) \\in \\mathbb{R}^n\\).\n(Component-wise) If \\(U_i \\le \\alpha\\), generate X from \\(X_i\\sim N_d(\\mu_1, \\Sigma_1)\\), o.w. generate \\(X_i from \\sim N_d(\\mu_2, \\Sigma_2)\\).\n\nEquivalent:\nWay 2:\n\nGenerate \\(N \\sim ‚àº \\operatorname{Bern}(\\alpha)\\).\n(Component-wise) If \\(N_i = 1\\) generate \\(X\\) from \\(X_i \\sim N_d(\\mu_1, \\Sigma_1)\\), o.w. generate \\(X\\) from \\(N_d(\\mu_2, \\Sigma_2)\\).\n\n\n\n\nset.seed(777)\nloc.mix.0 &lt;- function(n, alpha, mu1, mu2, Sigma) {\n  # generate sample from BVN location mixture\n  X &lt;- matrix(0, n, 2)\n  for (i in 1:n) {\n    k &lt;- rbinom(1, size = 1, prob = alpha)\n    if (k) {\n      X[i, ] &lt;- mvrnorm(1, mu = mu1, Sigma)\n    } else {\n      X[i, ] &lt;- mvrnorm(1, mu = mu2, Sigma)\n    }\n  }\n  return(X)\n}\n\n\nloc.mix &lt;- function(n, alpha, mu1, mu2, Sigma) {\n  # generate sample from BVN location mixture\n  n1 &lt;- rbinom(1, size = n, prob = alpha)\n  n2 &lt;- n - n1\n  x1 &lt;- mvrnorm(n1, mu = mu1, Sigma)\n  x2 &lt;- mvrnorm(n2, mu = mu2, Sigma)\n  X &lt;- rbind(x1, x2) # combine the samples\n  return(X[sample(1:n), ]) # mix them\n}\n\n\n\n\n\n\n\n\nNoteMixture of multivariate normal\n\n\n\nOne interesting example is when \\(\\alpha = 1 ‚àí 1/2 (1 ‚àí \\sqrt{3}/3 )\\approx .\n0.7887\\), provides an example of a skewed distribution with normal kurtosis (level of the peak).\n\nset.seed(777)\n\n\nn &lt;- 1000\ncomp1 &lt;- MASS::mvrnorm(n, mu = c(0, 0), Sigma = diag(2))\ncomp2 &lt;- MASS::mvrnorm(n, mu = c(3, 2), Sigma = matrix(c(1, 0.5, 0.5, 1), 2))\n\n# Proper mixture membership per observation\nalpha &lt;- 1 - 1/2*(1 - sqrt(3)/3 )   # P(Comp1)\nmemb &lt;- rbinom(n, 1, alpha)         # 1 = Comp1, 0 = Comp2\n\n# Build the mixture matrix row-wise (avoid ifelse on matrices)\nmix &lt;- matrix(NA_real_, nrow = n, ncol = 2)\nid1 &lt;- memb == 1\nid2 &lt;- !id1\nmix[id1, ] &lt;- comp1[sample(n, sum(id1), replace = TRUE), ]\nmix[id2, ] &lt;- comp2[sample(n, sum(id2), replace = TRUE), ]\n\n# Tidy data frames with labels\ndf1 &lt;- data.frame(x = comp1[,1], y = comp1[,2], group = \"Comp1\")\ndf2 &lt;- data.frame(x = comp2[,1], y = comp2[,2], group = \"Comp2\")\ndfm &lt;- data.frame(x = mix[,1],  y = mix[,2],  group = \"Mixture\")\ndf_all &lt;- rbind(df1, df2, dfm)\n\n# Option A: single panel, colored by group\np1 &lt;- ggplot(df_all, aes(x, y, color = group)) +\n  geom_point(alpha = 0.35, size = 1) +\n  stat_ellipse(level = 0.95, linewidth = 0.7) +\n  coord_equal() +\n  theme_minimal(base_size = 12) +\n  labs(title = \"Two Gaussian Components and Their Mixture\")\np1 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHigher dimensional case\n\n\n\nIt is difficult to visualize data in \\(R^d\\), for \\(d\\ge 4\\), so we display only the histograms of the marginal distributions. All of the one-dimensional marginal distributions are univariate normal location mixtures.\n\nset.seed(777)\nlibrary(MASS)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# ---- generate x exactly like your call ----\nx &lt;- loc.mix(1000, .5, rep(0, 4), 2:5, Sigma = diag(4))\ncolnames(x) &lt;- paste0(\"X\", 1:4)\n\n# ---- ggplot histograms ----\nr &lt;- range(x) * 1.2  # global x-limits like your base R code\ndf_long &lt;- as.data.frame(x) |&gt;\n  pivot_longer(everything(), names_to = \"Variable\", values_to = \"Value\")\n\nggplot(df_long, aes(x = Value)) +\n  geom_histogram(aes(y = ..density..),\n                 breaks = seq(-5, 10, 0.5),\n                 fill = \"red\", color = \"white\") +\n  facet_wrap(~ Variable, ncol = 2) +\n  coord_cartesian(xlim = r, ylim = c(0, 0.3)) +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Histograms of a 4D Gaussian Location Mixture\",\n    subtitle = expression(paste(\"p = 0.5,  \", mu[1], \" = (0,0,0,0),  \",\n                                mu[2], \" = (2,3,4,5),  \",\n                                Sigma, \" = I\")),\n    x = \"Value\", y = \"Density\"\n  )\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.3 Uniform Distribution on a Sphere\nThe uniform distribution on the surface of a \\(d\\)-sphere in \\(\\mathbb{R}^d\\) is the distribution that assigns equal probability to equal areas on the surface of the sphere.\nThe d-sphere is the set of all points \\(x \\in \\mathbb{R}^d\\) such that \\(\\| x \\| = \\sqrt{x^\\top x} = 1\\). Random vectors uniformly distributed on the \\(d\\)-sphere have equally likely directions. A method of generating this distribution uses a property of the\nmultivariate normal distribution (see [97, 160]). If X1, . . . , Xd are iid N (0, 1), then U = (U1, . . . , Ud) is uniformly distributed on the unit sphere in Rd, where\n\\[\nU_j=\\frac{X_j}{\\left(X_1^2+\\cdots+X_d^2\\right)^{1 / 2}}, \\quad j=1, \\ldots, d \\tag{*}\n\\]\nAlgorithm to generate uniform variates on the \\(d\\)-Sphere\n\nFor each variate \\(u_i,~ i = 1,\\dots , n\\) repeat\n\n\n\nGenerate a random sample \\(x_{i_1}, \\dots , x_{i_d}\\) from \\(N (0, 1)\\).\nCompute the Euclidean norm \\(\\|x_i\\| = (x^2_{i1} + \\cdots + x^2_{id})^{1/2}\\).\nSet \\(u_{ij} = x_{ij} / \\|xi\\|, j = 1, \\dots , d\\).\nDeliver \\(u_i = (u_{i1}, \\dots , u_{id})\\).\n\nTo implement these steps efficiently in R for a sample size n,\n\nGenerate nd univariate normals in \\(n \\times d\\) matrix \\(M\\). The \\(i\\)th row of M corresponds to the ith random vector \\(u_i\\).\nCompute the denominator of (*) for each row, storing the \\(n\\) norms in vector \\(L\\).\nDivide each number M[i,j] by the norm L[i], to get the matrix U, where \\(U[i,] = u_i = (u_{i1}, \\dots, u_{id})\\).\nDeliver matrix \\(U\\) containing \\(n\\) random observations in rows.\n\n\nThis example provides a function to generate random variates uniformly distributed on the unit \\(d\\)-sphere.\n\nrunif.sphere &lt;- function(n, d) {\n  # return a random sample uniformly distributed\n  # on the unit sphere in R ^d\n  M &lt;- matrix(rnorm(n * d), nrow = n, ncol = d)\n  L &lt;- apply(M,\n    MARGIN = 1,\n    FUN = function(x) {\n      sqrt(sum(x * x))\n    }\n  )\n  D &lt;- diag(1 / L)\n  U &lt;- D %*% M\n  U\n}\n\n#generate a sample in d=2 and plot\nX_d2 &lt;- runif.sphere(200, 2)\ndf &lt;- data.frame(x1 = X_d2[,1], x2 = X_d2[,2])\n\nggplot(df, aes(x = x1, y = x2)) +\n  geom_point(color = \"steelblue\", size = 2, alpha = 0.7) +\n  coord_equal() +                     \n  labs(\n    x = expression(x[1]),\n    y = expression(x[2]),\n    title = \"Uniform Random Points on the Unit Circle\"\n  ) +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), inherit.aes = FALSE,\n              color = \"black\", linetype = \"dashed\")\n\nWarning in ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), inherit.aes = FALSE, : All aesthetics have length 1, but the data has 200 rows.\n‚Ñπ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\nReference used:\n\nRizzo, M.L. (2007) Statistical Computing with R. CRC Press, Roca Baton.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Generating Random Variables</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html",
    "href": "04-monte-carlo.html",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "",
    "text": "4.1 What is Monte Carlo Simulation?\nMonte Carlo (MC) integration is a simulation-based method for approximating integrals using random sampling.\nIn numerical integration, methods such as the trapezoidal rule use a deterministic approach. MC integration, on the other hand, employs a non-deterministic approach: each realization provides a different outcome.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#what-is-monte-carlo-simulation",
    "href": "04-monte-carlo.html#what-is-monte-carlo-simulation",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "",
    "text": "4.1.1 History\nMonte Carlo (MC) is a casino in Monaco, famous for its gambling and games of chance. The term ‚ÄúMonte Carlo‚Äù was coined by physicists Stanislaw Ulam in the 1940s while working on nuclear weapon projects at Los Almos.\n\n\n\nMonte Carlo Casino, Picture borrowed from Wikipedia\n\n\nMonte Carlo methods are mainly used in three distinct problem classes:\n\noptimization\nnumerical integration\ngenerating draws from a probability distribution.\n\nThey can also be used to model phenomena with significant uncertainty in inputs, such as calculating the risk of a nuclear power plant failure. Monte Carlo methods are often implemented using computer simulations, and they can provide approximate solutions to problems that are otherwise intractable or too complex to analyze mathematically.\n\n\n4.1.2 Key Steps of MC methods\n\nMonte Carlo methods vary, but tend to follow a particular pattern:\n\nDefine a domain of possible inputs.\nGenerate inputs randomly from a probability distribution over the domain.\nPerform a deterministic computation of the outputs.\nAggregate the results.\n\n\n\nFor example, consider a quadrant (circular sector) inscribed in a unit square. Given that the ratio of their areas is \\(\\pi/4\\), the value of \\(\\pi\\) can be approximated using the Monte Carlo method:\n\nDraw a square, then inscribe a quadrant within it.\nUniformly scatter a given number of points over the square.\nCount the number of points inside the quadrant, i.e.¬†having a distance from the origin of less than 1.\nThe ratio of the inside-count and the total-sample-count is an estimate of the ratio of the two areas, \\(\\pi/4\\). Multiplying this ratio by 4 gives an estimate of \\(\\pi\\).\n\n\n\n\nPicture borrowed from Wikipedia",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#basic-monte-carlo-integration",
    "href": "04-monte-carlo.html#basic-monte-carlo-integration",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "4.2 Basic Monte Carlo Integration",
    "text": "4.2 Basic Monte Carlo Integration\nTo approximate the integral of a function \\(f(x)\\) over the interval \\([a, b]\\), we can use the following formula:\nConsider the problem of estimating \\(\\theta = \\int_0^1 g(x)dx\\). If \\(X_1,\\dots , X_m\\sim\\operatorname{Unif}(0,1)\\), then the MC estimator is given by:\n\\[\\hat{\\theta}=\\bar{g}_m(X)=\\frac{1}{m}\\sum_{i=1}^m g(X_i)\\] converges to \\(\\mathbb{E}[g(X)]\\) as \\(m\\to\\infty\\) with probability 1, by Strong law of Large Number (SLLN). The simple MC estimator is unbiased, i.e., \\(\\bar{g}_m(X)\\).\n\nCompute a MC estimate \\[\n\\theta = \\int_0^1 \\exp(-x)dx,\n\\] and compare the estimate with the theoretical value\n\nset.seed(777)\nn &lt;- 1E3\nx &lt;- runif(n)\n# simulated estimator\ntheta_hat &lt;- exp(-x) |&gt; mean()\n\n# theoretical value\ntheta_true &lt;- 1 - exp(-1)\n\n# put them in a tibble\n(results &lt;- tibble(\n  Method = c(\"Simulated\", \"Theoretical\"),\n  Value  = c(theta_hat, theta_true)))\n\n# A tibble: 2 √ó 2\n  Method      Value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Simulated   0.641\n2 Theoretical 0.632\n\n\n\nTo simulate \\(\\int_a^b g(t)dt\\), use change of variable so the limit becomes from \\(0\\) to \\(1\\). This can be done through a linear transformation of the variable \\(t\\): \\(y:=\\frac{t-a}{b-a}\\). Then, \\(t=a+(b-a)y\\) and \\(dt=(b-a)dy\\). Thus, we have \\[\n\\int_a^b g(t)dt = (b-a)\\int_0^1 g(a+(b-a)y)dy.\n\\] Alternatively, instead of using \\(\\operatorname{Unif}(0,1)\\), we can replace it with other densities with supports between \\(a\\) and \\(b\\). One instance is, \\[\n\\int_a^b g(t)dt = \\int_a^b \\frac{g(t)}{f}f dt = (b-a) \\int_a^b  \\frac{g(t)}{b-a} dt.\n\\] This is a \\(b-a\\) times the expectation of \\(g(X)\\) where \\(X\\sim \\operatorname{Unif}(a,b)\\). Therefore, this integral can be estimated by averaging through the function \\(g(\\cdot)\\) over the interval from \\(a\\) to \\(b\\) multiply by \\(b-a\\).\n\nCompute a MC estimate of \\[\n\\theta = \\int_2^4 \\exp(-x)dx,\n\\] and compare the estimate with the exact value of the integral.\n\nlibrary(tibble)\n\nset.seed(777)\nm &lt;- 1E3\nx &lt;- runif(m, min = 2, max = 4)\n\n# simulated estimator\ntheta_hat &lt;- exp(-x) |&gt; mean() * (4 - 2)\n\n# theoretical value\ntheta_true &lt;- exp(-2) - exp(-4)\n\n# put into tibble\n(results &lt;- tibble(\n  Method = c(\"Simulated\", \"Theoretical\"),\n  Value  = c(theta_hat, theta_true)\n))\n\n# A tibble: 2 √ó 2\n  Method      Value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Simulated   0.120\n2 Theoretical 0.117\n\n\n\n\nTo summarize, the simple Monte Carlo estimator of the integral \\(\\theta = \\int_a^b g(x)dx\\) is computed as follows.\n\nGenerate \\(X_1, \\dots , X_m\\overset{iid}{\\sim}\\operatorname{Unif}(a,b)\\),\nCompute \\(\\bar{g}(X) = \\frac{1}{m} g(X_i)\\).\n\\(\\hat{\\theta}= (b ‚àí a)\\bar{g}(X)\\).\n\n\n\nCompute a MC estimate of a standard normal cdf\n\\[\n\\Phi(x)=\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}} e^{-t^2 / 2} d t\n\\]\n\nNote, we cannot apply the algorithm above directly because the limits of integration cover an unbounded interval. However, we can break this problem into two cases: (i) \\(x \\ge 0\\) and (ii) \\(x &lt; 0\\), and use the symmetry of the normal density to handle the second case. Then the problem is to estimate \\(\\theta =\\int_0^x \\theta = \\int_0^x \\exp(‚àít^2/2) dt\\) for \\(x &gt; 0\\). This can be done by generating random \\(\\operatorname{Unif}(0,x)\\) numbers, but it would mean changing the parameters of the uniform distribution for each different value of the cdf required. Suppose that we prefer an algorithm that always samples from \\(\\operatorname{Unif}(0,1)\\). This can be accomplished by a change of variables. Making the substitution \\(y = t/x\\), we have \\(dt = x dy\\) and\n\n\\[\n\\theta=\\int_0^1 x e^{-(x y)^2 / 2} d y\n\\] Thus, \\(\\theta = \\mathbb{E}_Y[x\\exp(-(xY)^2/2)]\\), where the rv \\(Y\\sim \\operatorname{Unif}(0,1)\\). Generate iid \\(\\operatorname{Unif}(0,1)\\) random numbers \\(u_1,\\dots,u_m\\), and compute \\[\\hat{\\theta}=\\overline{g_m(u)}=\\frac{1}{m} \\sum_{i=1}^m x e^{-\\left(u_i x\\right)^2 / 2}.\n\\]\nThe sample mean \\(\\hat{\\theta}\\) converges to \\(\\mathbb{E}\\hat{\\theta}= \\theta\\) as \\(m\\to \\infty\\). If \\(x &gt; 0\\), the estimate of \\(\\Phi(x) = 1/2 + \\hat{\\theta}/\\sqrt{2\\pi}\\). If \\(x &lt; 0\\) compute \\(\\Phi(x) = 1 ‚àí \\Phi(‚àíx)\\).\n\nx &lt;- seq(.1, 2.5, length = 10)\nm &lt;- 10000\nu &lt;- runif(m)\ncdf &lt;- numeric(length(x))\nfor (i in 1:length(x)) {\n  g &lt;- x[i] * exp(-(u * x[i])^2 / 2)\n  cdf[i] &lt;- mean(g) / sqrt(2 * pi) + 0.5\n}\nPhi &lt;- pnorm(x)\nprint(round(rbind(x, cdf, Phi), 3))\n\n    [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\nx   0.10 0.367 0.633 0.900 1.167 1.433 1.700 1.967 2.233 2.500\ncdf 0.54 0.643 0.737 0.816 0.878 0.924 0.956 0.976 0.987 0.994\nPhi 0.54 0.643 0.737 0.816 0.878 0.924 0.955 0.975 0.987 0.994\n\n\nNotice that it would have been simpler to generate random Uniform(0, x) random variables and skip the transformation. In fact, the integrand of the previous example is itself a density function, and we can generate random variables from this density. This provides a more direct approach to estimating the integral.\n\n\nLet \\(I(\\cdot)\\) be the indicator function, and \\(Z\\sim N(0,1)\\). Then for any constant \\(x\\) we have \\(\\mathbb{E}[I(Z ‚â§ x)] = P (Z ‚â§ x) =\\Phi(x)\\), the standard normal cdf evaluated at \\(x\\).\nGenerate a random sample \\(z_1, \\dots , z_m\\sim N(0,1)\\). Then the theoretical mean and sample mean are \\[\\hat{\\theta}= \\frac{1}{m} \\sum_{i=1}^m I(z_i \\le x),\\] and \\[\\mathbb{E}[\\hat{\\theta}] = P(Z \\le x) = \\Phi(x).\\]\n\nset.seed(777)\nx &lt;- seq(0.1, 2.5, length = 10)\nm &lt;- 1E4\nz &lt;- rnorm(m)\ndim(x) &lt;- length(x)\np &lt;- apply(x, MARGIN = 1,\n  FUN = function(x, z) {mean(z &lt; x)}, z = z)\nPhi &lt;- pnorm(x)\n\nrbind(x, p, Phi) |&gt; round(3)\n\n     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10]\nx   0.100 0.367 0.633 0.900 1.167 1.433 1.700 1.967 2.233 2.500\np   0.544 0.645 0.740 0.818 0.881 0.926 0.957 0.976 0.986 0.993\nPhi 0.540 0.643 0.737 0.816 0.878 0.924 0.955 0.975 0.987 0.994\n\n\nIn this example, compared with the previous example, it appears that we have better agreement with pnorm() in the upper tail, but worse agreement near the center.\n\n\n\n\n\n\n\nNote\n\n\n\nSummarizing, if \\(f (x)\\) is a probability density function supported on a set A, (that is, \\(f (x) \\ge 0\\) for all \\(x\\in\\mathbb{R}\\) and \\(\\int_A f (x) = 1\\)), to estimate the integral \\[\\theta = \\int_A g(x)f (x)dx,\\] generate a random sample \\(x_1,\\dots,x_m\\) from the distribution \\(f (x)\\), and compute the sample mean \\[\\hat{\\theta}= \\frac{1}{m}\\sum_{i=1}^mg(x_i).\\] Then \\(\\hat{\\theta}\\overset{p}{\\to}\\theta\\) by the weak law of large numbers (WLLN).\n\n\nThe standard error of \\(\\hat{\\theta}= \\frac{1}{m}\\sum_{i=1}^m g(x_i)\\).\nRecall that, \\(\\mathbb{V}ar({\\hat{\\theta}})=\\sigma^2/m\\) where \\(\\sigma^2 = \\mathbb{V}ar\\{g(X)\\}\\). When the distribution of \\(X\\) is unknown, we substitute for \\(F_X\\) the empirical distribution \\(F_m\\) of the sample \\(x_1, \\dots , x_m\\). The variance of \\(\\hat{\\theta}\\) can be estimated by \\[\n\\frac{\\hat{\\sigma}^2}{m} = \\frac{1}{m^2}\\sum_{i=1}^m [g(x_i) - \\bar{g}(x)]^2.\n\\]\nNote that \\[\\frac{1}{m}\\sum_{i=1}^m[g(x_i)-\\bar{g}(x_i)]^2,\\] is the plug-in estimate of \\(\\mathbb{V}ar\\{g(X)\\}\\). That is, it is the variance of \\(U\\) , where \\(U\\) is uniformly distributed on the set of replicates \\(\\{g(x_i)\\}\\). The corresponding estimated standard error of \\(\\hat{\\theta}\\) is \\[\n\\widehat{se}(\\hat{\\theta}) = \\frac{\\hat{\\sigma}}{\\sqrt{m}} = \\frac{1}{m}\\left\\{\\sum_{i=1}^m [ g(x_i)- \\bar{g}(x)]^2\\right\\}^{1/2}.\n\\] The CLT implies \\[\n\\frac{\\hat{\\theta}-E[\\hat{\\theta}]}{\\sqrt{\\mathbb{V}ar(\\hat{\\theta})}} \\overset{D}{\\to} N(0,1),\n\\] as \\(m\\to\\infty\\). Hence, if \\(m\\) is sufficiently large, \\(\\hat{\\theta}\\) is approximately normal with mean \\(\\theta\\). The large-sample, approximately normal distribution of \\(\\hat{\\theta}\\) can be applied to put confidence limits or error bounds on the MC estimate of the integral, and check for convergence\n\nCompute the 95% confidence interval for \\(\\Phi(2)\\) and \\(\\Phi(2.5)\\).\n\nset.seed(777)\nx &lt;- 2\nm &lt;- 10000\nz &lt;- rnorm(m)\ng &lt;- (z &lt; x) #the indicator function\nv &lt;- mean((g - mean(g))^2) / m\ncdf &lt;- mean(g)\nc(cdf, v)\n\n[1] 9.771000e-01 2.237559e-06\n\nc(cdf - 1.96 * sqrt(v), cdf + 1.96 * sqrt(v))\n\n[1] 0.9741681 0.9800319\n\n\nThe interpretation is:\nthe probability \\(P (I(Z &lt; x) = 1)\\) is \\(\\Phi(2)\\approx 0.977\\). Here \\(g(X)\\) has the distribution of the sample proportion of 1‚Äôs in \\(m = 10000\\) Bernoulli trials with \\(p\\approx 0.977\\), and the variance of \\(g(X)\\) is therefore \\((0.977)(1 ‚àí 0.977)/10000 =2.223 \\times 10^{-6}\\). The MC estimate \\(2.228\\times 10^{-6}\\) of variance is quite close to this value.\nQ: What about \\(\\Phi(2.5)\\)?",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#variance-and-efficiency",
    "href": "04-monte-carlo.html#variance-and-efficiency",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "4.3 Variance and Efficiency",
    "text": "4.3 Variance and Efficiency\nWe have seen that a MC approach to estimating the integral \\(\\int_a^b g(x)dx\\) is to represent the integral as the expected value of a function of a uniform random variable. That is, suppose \\(X\\sim \\operatorname{Unif}(0,1)\\), then \\(f(x) = (b-a)^{-1}\\) for \\(x\\in[a,b]\\) and 0 otherwise, and \\[\n\\begin{aligned}\n\\theta & =\\int_a^b g(x) d x \\\\\n& =(b-a) \\int_a^b g(x) \\frac{1}{b-a} d x=(b-a) E[g(X)]\n\\end{aligned}\n\\]\nRecall that, from Algorithm 2, the sample-mean MC estimator of the integral \\(\\theta\\) is computed as follows.\n\nGenerate \\(X_1, \\dots , X_m\\overset{iid}{\\sim}\\operatorname{Unif}(a,b)\\),\nCompute \\(\\bar{g}(X) = g(X_i)/m\\)\n\\(\\hat{\\theta}= (b ‚àí a)\\bar{g}(X)\\).\n\nThe sample mean \\(\\bar{g}(X)\\) has expected value \\(g(X)=\\theta/(b-a)\\) and variance \\(\\mathbb{V}ar\\{\\bar{g}(X)\\}=\\mathbb{V}ar\\{g(X)\\}/m\\). By CLT, \\(\\bar{g}(X)\\) is approximately normal for large \\(m\\). Therefore, the variance of the MC estimator \\(\\hat{\\theta}\\) is approximately normal with mean \\(\\theta\\) and variance \\(\\mathbb{V}ar\\{g(X)\\}/m\\).\n\n\n\n\n\n\nNoteExpectation, Variance and Distribution of theta hat\n\n\n\nThe expectation and the variance of the MC estimator \\(\\hat{\\theta}\\) are given by \\[\\begin{align*}\n\\mathbb{E}[\\hat{\\theta}] & = \\theta, \\\\\n\\mathbb{V}ar(\\hat{\\theta}) &= (b-a)^2 \\mathbb{V}ar(\\overline{g}(X))=\\frac{(b-a)^2}{m} \\mathbb{V}ar\\{g(X)\\} .\n\\end{align*}\\] Further more, by CLT, for large \\(m\\), \\(\\overline{g}(X)\\) is approximately normally distributed, and with the mean and variance given above.\n\n\n\n4.3.1 Hit-or-Miss Approach\nThe ‚Äúhit-or-miss‚Äù approach to MC integration also uses a sample mean to estimate the integral, but the sample mean is taken over a different sample and therefore this estimator has a different variance than the one we have above.\n\n\n\nHit-or-Miss MC, image borrowed from UBC\n\n\nSuppose \\(f(x)\\) is the density of a random variable \\(X\\). The ‚Äúhit-or-miss‚Äù approach to estimating \\(F(x) = \\int_{-\\infty}^x f(t)\\,dt\\) is as follows:\n\n\nGenerate a random sample \\(X_1,\\dots,X_m \\sim F_X(x)\\).\nFor each observation \\(X_i\\), compute \\[\ng(X_i) = I(X_i \\leq x) =\n\\begin{cases}\n1, & X_i \\leq x, \\\\\n0, & X_i &gt; x.\n\\end{cases}\n\\]\nCompute \\[\n\\hat{F}(x) = \\frac{1}{m} \\sum_{i=1}^m I(X_i \\leq x).\n\\]\n\n\nNote that the random variable \\(Y = g(X) \\sim \\text{Bern}(1, p)\\), where the success probability is\n\\[\np := P(X \\leq x) = F(x).\n\\]\nThe transformed sample \\(Y_1, \\ldots, Y_m\\) are the outcomes of \\(m\\) independent, identically distributed Bernoulli trials.\nThe estimator \\(\\hat{F}(x)\\) is the sample proportion \\[\n\\hat{p} = \\frac{y}{m},\n\\]\nwhere \\(y\\) is the total number of successes observed in \\(m\\) trials. Hence,\n\\[\n\\mathbb{E}[\\hat{F}(x)] = p = F(x), \\qquad\n\\mathrm{Var}(\\hat{F}(x)) = \\frac{p(1-p)}{m} = \\frac{F(x)\\big(1-F(x)\\big)}{m}.\n\\]\nThe variance of \\(\\hat{F}(x)\\) can be estimated by \\[\n\\frac{\\hat{p}(1-\\hat{p})}{m} = \\frac{\\hat{F}(x)\\big(1-\\hat{F}(x)\\big)}{m}.\n\\]\nThe maximum variance occurs when \\(F(x) = \\tfrac{1}{2}\\), so a conservative estimate of the variance of \\(\\hat{F}(x)\\) is \\[\n\\frac{1}{4m}.\n\\]\n\n\n4.3.2 Efficiency\nSuppose we have two (unbiased) estimators \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) for Œ∏. We say \\(\\hat{\\theta}_1\\) is statistically more efficient than \\(\\hat{\\theta}_2\\) if \\[\\mathbb{V}ar(\\hat{\\theta}_1) &lt; \\mathbb{V}ar(\\hat{\\theta}_2).\\] What is the variance of \\(\\hat{\\theta}_i\\) is unknown or hard to be calculated?\ncan substitute it by sample estimate of the variance for each estimator.\nNote, the variance can always be reduced by increasing the number of replicates, so computational efficiency is also relevant.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#variance-reduction",
    "href": "04-monte-carlo.html#variance-reduction",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "4.4 Variance Reduction",
    "text": "4.4 Variance Reduction\nWe have seen that MC integration may be used to estimate \\(\\theta:=\\mathbb{E}[g(X)]\\). However, how to have the efficient estimator for \\(\\theta\\), and if we have a several ways \\(\\hat{\\theta}_1,\\dots,\\hat{\\theta}_k\\) to estimate \\(\\theta\\), which one is better, or, more efficient? Here, we try to introduce some variance reduction techniques.\n\nLet \\(\\hat{\\theta}_1\\) and \\(\\hat{\\theta}_2\\) be two estimators of \\(\\theta\\) where \\(\\mathbb{V}ar(\\hat{\\theta}_1) &gt; \\mathbb{V}ar(\\hat{\\theta}_2)\\). The relative efficiency gain of \\(\\hat{\\theta}_2\\) instead of using \\(\\hat{\\theta}_1\\) is defined as \\[\n\\operatorname{Eff}(\\hat{\\theta}_1, \\hat{\\theta}_2) = \\frac{\\mathbb{V}ar(\\hat{\\theta}_1)- \\mathbb{V}ar(\\hat{\\theta}_2)}{\\mathbb{V}ar(\\hat{\\theta}_1)}.\n\\]\n\nLet \\(X\\) be a random object and let \\(g(\\cdot)\\) be a (possibly vector‚Äìvalued) statistic of a sample from the distribution of \\(X\\). For \\(j=1,\\ldots,m\\), draw an i.i.d. replicate sample \\(X^{(j)}=\\{X^{(j)}_1,\\ldots,X^{(j)}_n\\}\\) and compute \\[\nY_j = g\\!\\big(X^{(j)}\\big).\n\\tag{6.5}\n\\] Then \\(Y_1,\\ldots,Y_m\\) are i.i.d. with common mean \\[\n\\theta \\;=\\; \\mathbb{E}\\big[g(X)\\big] \\;=\\; \\mathbb{E}[Y].\n\\]\nThe MC estimator of \\(\\theta\\) is the sample mean \\[\n\\hat{\\theta}\\;=\\; \\bar Y \\;=\\; \\frac{1}{m}\\sum_{j=1}^m Y_j.\n\\] By linearity of expectation, \\[\n\\mathbb{E}[\\hat{\\theta}] \\;=\\; \\theta,\n\\] so \\(\\hat{\\theta}\\) is unbiased. Its variance is \\[\n\\mathbb{V}ar(\\hat{\\theta})\n  \\;=\\; \\mathbb{V}ar(\\bar Y)\n  \\;=\\; \\frac{\\mathbb{V}ar\\{g(X)\\}}{m}.\n\\]\nHence the standard error decays as \\(m^{-1/2}\\). To reduce the standard error from \\(0.01\\) to \\(0.0001\\), one would need about \\(10000\\) times as many replicates. More generally, if \\(\\mathbb{V}ar\\{g(X)\\}=\\sigma^2\\) and we target standard error at most \\(\\varepsilon\\), then \\[\nm \\;\\ge\\; \\frac{\\sigma^2}{\\varepsilon^2}\n\\] replicates suffice (ignoring finite‚Äì\\(m\\) effects).\nThe remainder of this note gives a minimal simulation illustrating the \\(1/\\sqrt{m}\\) behavior and provides a template for later variance‚Äìreduction sections (e.g., control variates, antithetic sampling, importance sampling).\n\n4.4.1 Rule-of-thumb for target standard error\nGiven a target standard error \\(\\varepsilon\\), we can solve \\(\\sqrt{\\sigma^2/m} \\le \\varepsilon\\) for \\(m\\).\n\nsigma2_true &lt;- 2\nepsilon &lt;- c(0.1, 0.05, 0.01, 0.005, 0.001)\nrequired_m &lt;- ceiling(sigma2_true / (epsilon^2))\ndata.frame(se_target = epsilon, m_needed = required_m)\n\n  se_target m_needed\n1     0.100    2e+02\n2     0.050    8e+02\n3     0.010    2e+04\n4     0.005    8e+04\n5     0.001    2e+06\n\n\n\nRemark. Increasing \\(m\\) always reduces variance but can be costly. Variance‚Äìreduction techniques seek lower variance at the same \\(m\\) (or similar compute), rather than increasing \\(m\\) alone.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#antithetic-variables",
    "href": "04-monte-carlo.html#antithetic-variables",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "4.5 Antithetic Variables",
    "text": "4.5 Antithetic Variables\nAntithetic variables are a variance reduction technique used in MC simulation to improve the efficiency of estimators. The basic idea is to use pairs of negatively correlated random variables to reduce the variance of the estimator. Recall that the variance of two variables are \\[\n\\mathbb{V}ar(X+Y) = \\mathbb{V}ar(X) + \\mathbb{V}ar(Y) + 2\\mathbb{C}ov(X,Y),\n\\] where \\(\\mathbb{C}ov(X,Y) =\\mathbb{E}[XY] - \\mathbb{E}[X] \\mathbb{E}[Y]\\). The covariance can be written as \\(\\mathbb{C}ov(X,Y) = \\rho_{YX} \\sigma_X \\sigma_Y\\), where \\(\\rho_{YX}\\) is the correlation coefficient between \\(X\\) and \\(Y\\). If \\(X\\) and \\(Y\\) are negatively correlated, i.e., \\(\\rho_{YX} &lt; 0\\), then \\(\\mathbb{C}ov(X,Y) &lt; 0\\), which can lead to a reduction in the variance of the sum \\(X + Y\\).\n\nRandom Variables \\(X\\) and \\(Y\\) on the same probability space are said to be antithetic if they are negatively correlated, i.e., \\(\\mathbb{C}ov(X,Y) &lt; 0\\).\n\nNow, consier two random variable \\(U_1\\) and \\(U_2\\) that follows the same distribution. Then, if we take the average of the two random variables, we have \\[\n\\mathbb{V}ar\\left(\\frac{U_1 + U_2}{2}\\right) = \\frac{1}{4} \\left\\{\\mathbb{V}ar(U_1) + \\mathbb{V}ar(U_2) + 2\\mathbb{C}ov(U_1,U_2)\\right\\},\n\\] which will be smaller than when the two variables are independent (in that case, the last term is 0). Hence, the idea here it to consider the case where the RVs are negatively correlated.\nSuppose that \\(X_1, \\dots , X_n\\) are simulated via the ITM. For each of the m replicates we have generated \\(U_j \\sim\\operatorname{Unif}(0,1)\\), and the corresponding \\(X^{(j)} = F^{‚àí1}_X(U_j), j = 1, \\dots, n\\). From before, we know that if \\(U\\sim\\operatorname{Unif}(0,1)\\), then \\(1 ‚àí U\\) has the same distribution as \\(U\\), but \\(U\\) and \\(1 ‚àí U\\) are negatively correlated. Then\n\\[ Y_j = g(F^{‚àí1}_X(U^{(j)}_1 ), \\dots , F^{‚àí1}_X(U_n^{(j)})),\\] and\n\\[ Y_j^\\prime = g(F^{‚àí1}_X(1-U^{(j)}_1 ), . . . , F^{‚àí1}_X(1-U_n^{(j)}))\\] have the same distribution. So the question now if, when will \\(Y_j^{\\prime}\\) and \\(Y_j\\) be negatively correlated?\nWe will see that if \\(g(\\cdot)\\) is monotone, then \\(Y_j\\) and \\(Y_j^{\\prime}\\) are negatively correlated.\n\nDefine \\((x_1, . . . , x_n) \\le (y_1, \\dots , y_n)\\) if \\(x_j ‚â§ y_j, j = 1, \\dots, n\\). An \\(n\\)-variate function \\(g := g(X_1, \\dots, X_n)\\) is increasing if it is increasing in its coordinates. That is, \\(g\\) is increasing if \\(g(x_1,\\dots, x_n) \\le g(y_1, \\dots, y_n)\\) whenever \\((x_1, \\dots , x_n) \\le (y_1, . . . , y_n)\\). Similarly \\(g\\) is decreasing if it is decreasing in its coordinates. Then \\(g\\) is monotone if it is increasing or decreasing.\n\n\nLet \\(X\\) be a random varaible, and \\(f\\) and \\(g\\) are monotonic increasing functions. Then \\[\n\\mathbb{E}[f(X)g(X)] \\ge \\mathbb{E}[f(X)]\\mathbb{E}[g(X)].\n\\]\n\n\nIt is actually a corollary of the above theorem.\nLet \\(g := g(X_1, \\dots, X_n)\\) be monotonic, and \\(U_1, \\dots, U_n \\sim \\operatorname{Unif}(0,1)\\) be independent. Then\n\\[ Y_j^\\prime = g(F^{‚àí1}_X(1-U_1 ), . . . , F^{‚àí1}_X(1-U_n))\\] and \\[ Y_j = g(F^{‚àí1}_X(U_1 ), . . . , F^{‚àí1}_X(U_n))\\] are negatively correlated.\n\n\nRefer to the example 4, illustrating MC integration applied to estimate the standard normal cdf\n\\[\n\\Phi(x)=\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi}} e^{-t^2 / 2} d t\n\\]\nRepeat the estimation using antithetic variables, and find the approximate reduction in standard error. In this example, after change of variable, the target parameter is \\(\\theta = \\mathbb{E}_U[x\\exp\\{-(xU)^2/2\\}]\\) where \\(U\\sim\\operatorname{Unif}(0,1)\\).\nBy restricting the simualtion to the upper tail, the function \\(g\\) is monotone, so the hypothesis of Theorem 2 is satisfied. Now, we generate \\(u_1,\\dots,u_{m/2}\\overset{iid}{\\sim}\\operatorname{Unif}(0,1)\\), and compute the half of the replicates using \\[\nY_j=g^{(j)}(u)=x e^{-\\left(u_j x\\right)^2 / 2}, \\quad j=1, \\ldots, m / 2,\n\\] as before, but also compute \\[\nY_j^{\\prime}=x e^{-\\left(\\left(1-u_j\\right) x\\right)^2 / 2}, \\quad j=1, \\ldots, m / 2 .\n\\]\nThus, the sample mean is \\[\n\\begin{aligned}\n\\hat{\\theta}=\\overline{g_m(u)} & =\\frac{1}{m} \\sum_{j=1}^{m / 2}\\left(x e^{-\\left(u_j x\\right)^2 / 2}+x e^{-\\left(\\left(1-u_j\\right) x\\right)^2 / 2}\\right) \\\\\n& =\\frac{1}{m / 2} \\sum_{j=1}^{m / 2}\\left(\\frac{x e^{-\\left(u_j x\\right)^2 / 2}+x e^{-\\left(\\left(1-u_j\\right) x\\right)^2 / 2}}{2}\\right)\\\\\n& \\to \\mathbb{E}[\\hat{\\theta}] = \\theta, \\quad \\text{as } m\\to\\infty.\n\\end{aligned}\n\\] If \\(x\\ge 0\\), the estimate is \\(\\Phi(x) = 0.5 + \\hat{\\theta}/ \\sqrt{2\\pi}\\), where as if \\(x&lt; 0\\), the estimate is \\(\\Phi(x) = 1-\\Phi(-x)\\).\n\nMC.Phi &lt;- function(x, R = 10000, antithetic = TRUE) {\n  u &lt;- runif(R / 2)\n  if (!antithetic) {\n    v &lt;- runif(R / 2)\n  } else {\n    v &lt;- 1 - u\n  }\n  u &lt;- c(u, v)\n  cdf &lt;- numeric(length(x))\n  for (i in 1:length(x)) {\n    g &lt;- x[i] * exp(-(u * x[i])^2 / 2)\n    cdf[i] &lt;- mean(g) / sqrt(2 * pi) + 0.5\n  }\n  cdf\n}\n\nx &lt;- seq(.1, 2.5, length=5)\nPhi &lt;- pnorm(x)\nset.seed(123)\nMC_raw &lt;- MC.Phi(x, anti = FALSE)\nset.seed(123)\nMC_anti &lt;- MC.Phi(x)\nprint(round(rbind(x, MC_raw, MC_anti, Phi), 5))\n\n           [,1]    [,2]    [,3]    [,4]    [,5]\nx       0.10000 0.70000 1.30000 1.90000 2.50000\nMC_raw  0.53983 0.75825 0.90418 0.97311 0.99594\nMC_anti 0.53983 0.75805 0.90325 0.97132 0.99370\nPhi     0.53983 0.75804 0.90320 0.97128 0.99379\n\n## For variance\nm &lt;- 1000\nMC_raw &lt;- MC_anti &lt;- numeric(m)\nx &lt;- 1.95\nfor (i in 1:m) {\n  MC_raw[i] &lt;- MC.Phi(x, R = 1000, anti = FALSE)\n  MC_anti[i] &lt;- MC.Phi(x, R = 1000)\n}\nsd(MC_raw) |&gt; round(6) \n\n[1] 0.006875\n\nsd(MC_anti) |&gt; round(6)\n\n[1] 0.000439\n\neff &lt;- (var(MC_raw) - var(MC_anti))/var(MC_raw) \neff |&gt; round(6)\n\n[1] 0.995917\n\n\nConclusion: The antithetic variable approach achieved approximately 0.9959 reduction in variance at x = 1.95.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#control-variates",
    "href": "04-monte-carlo.html#control-variates",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "4.6 Control Variates",
    "text": "4.6 Control Variates\nControl variates is another variance reduction technique used in MC simulation to improve the efficiency of estimators. The basic idea is to use a known variable that is correlated with the variable of interest to reduce the variance of the estimator.\nSuppose there exists a function \\(f(\\cdot)\\) such that \\(\\mu=\\mathbb{E}[f(X)]\\) is known, and \\(f(X)\\) is correlated with \\(g(X)\\). Then, we have, for a constant \\(c\\in\\mathbb{R}\\), \\(\\hat{\\theta}_c = g(X) + c\\{f(X) - \\mu\\}\\) is also an unbiased estimator of \\(\\theta\\). The variance of \\(\\hat{\\theta}_c\\) is \\[\n\\mathbb{V}ar(\\hat{\\theta}_c) = \\mathbb{V}ar\\{g(X)\\} + c^2\\mathbb{V}ar\\{f(X)\\} + 2c\\mathbb{C}ov\\{g(X), f(X)\\}.\n\\] This is a quadratic function in \\(c\\), and it is minimized at \\[\nc^*=-\\frac{\\mathbb{C}ov\\{g(X), f(X))\\}}{\\mathbb{V}ar\\{f(X)\\}}.\n\\] By plugging in this optimizer \\(c=c^\\ast\\), the resulting variance is \\[\n\\mathbb{V}ar(\\hat{\\theta}_{c^\\ast}) = \\mathbb{V}ar\\{g(X)\\} - \\frac{[\\mathbb{C}ov\\{g(X), f(X)\\}]^2}{\\mathbb{V}ar\\{f(X)\\}}.\n\\] This RV \\(f(X)\\) is called a control variate for the estimator of \\(g(X)\\). We can see that the \\(\\mathbb{V}ar\\{g(X)\\}\\) is reduced by the second term \\([\\mathbb{C}ov\\{g(X), f(X)\\}]^2/\\mathbb{V}ar\\{f(X)\\}\\), and the percentage of variance reduction is \\[\n100 \\frac{[\\mathbb{C}ov(g(X), f(X))]^2}{\\mathbb{V}ar(g(X)) \\mathbb{V}ar(f(X))}=100[\\mathbb{C}orr(g(X), f(X))]^2 .\n\\] Thus, if \\(g(X)\\) and \\(f(X)\\) is strongly correlated, the variance will be reduced, and if they are uncorrelated, there is no reduction in variance.\nTO estimate \\(c^*\\), we need to estimate \\(\\mathbb{C}ov\\{g(X),f(X)\\}\\) and \\(\\mathbb{V}ar\\{f(X)\\}\\), which may be estimated using MC too, in the preliminary steps.\n\nUse control variate approach to compute \\[\n\\theta = \\mathbb{E}[\\exp(U)] =\\int_0^1\\exp(u)du,\\quad \\text{where } U\\sim\\operatorname{Unif}(0,1).\n\\] We can easily find the analytical solution \\(\\theta=\\exp(1) - \\exp(0) = e - 1 = 1.718282\\), but how to use the control variate approach to estimate \\(\\theta\\)?\nWe can apply the simple MC approach where the variance is \\(\\mathbb{V}ar\\{g(U)\\}/m\\), where \\[\n\\mathbb{V}ar(g(U))=\\mathbb{V}ar\\left(e^U\\right)=\\mathbb{E}\\left[e^{2 U}\\right]-\\theta^2=\\frac{e^2-1}{2}-(e-1)^2 \\approx 0.2420\n\\]\nAs for control variate, a natural choice is \\(U\\sim\\operatorname{Unif}(0,1)\\). Then the mean is \\(\\mu=\\mathbb{E}[U]=1/2\\), variance is \\(\\mathbb{V}ar(U) = 1/12\\) and the covariance is \\(\\mathbb{C}ov(e^U, U)= \\mathbb{E}[Ue^U]-\\mathbb{E}[U]\\mathbb{E}[\\exp(U)]= 1-(e-1)/2 \\approx 0.1409\\). Thus, the optimal \\(c^* = -\\mathbb{C}ov(e^U,U)/\\mathbb{V}ar(U) =-12+6(e-1)^2 \\approx -1.6903\\).\nThen, the controlled estimator is \\(\\hat{\\theta}_{c^\\ast}=\\exp(U)-1.6903(U-0.5)\\). With \\(m\\) replications, we have \\[\n\\begin{aligned}\nm\\mathbb{V}ar(\\hat{\\theta}_{c^\\ast})\n&=\\operatorname{Var}\\left(e^U\\right)-\\frac{\\left[\\operatorname{Cov}\\left(e^U, U\\right)\\right]^2}{\\operatorname{Var}(U)} \\\\ &=\\frac{e^2-1}{2}-(e-1)^2-12\\left(1-\\frac{e-1}{2}\\right) \\\\\n& \\doteq 0.2420356-12(0.1408591)^2 \\\\\n& =0.003940175 .\n\\end{aligned}\n\\]\nThus, the reduction in variance using the control variate compared with the simple Monte Carlo estimate is \\[100 \\frac{1-0.003940175}{0.2429355} = 98.3781 \\%\\].\n\nset.seed(777)\nm &lt;- 10000\na &lt;- - 12 + 6 * (exp(1) - 1)\nU &lt;- runif(m)\nT1_MC &lt;- exp(U) #simple MC\nT2_cv &lt;- exp(U) + a * (U - 1/2) #controlled\nmean(T1_MC) |&gt; round(6)\n\n[1] 1.715071\n\nmean(T2_cv) |&gt; round(6)\n\n[1] 1.717789\n\neff &lt;- (var(T1_MC) - var(T2_cv)) / var(T1_MC)\neff |&gt; round(4)\n\n[1] 0.9835\n\n\nThis show that the reduction is 0.9835% in variance using the control variate approach, which is the same as our analysis above",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "04-monte-carlo.html#other-methods",
    "href": "04-monte-carlo.html#other-methods",
    "title": "4¬† Monte Carlo Simulation and Variance Reduction",
    "section": "4.7 Other methods",
    "text": "4.7 Other methods\nThere are other variance reduction techniques, including\n\nImportance Sampling\nStratified Sampling\nStratified Importance Sampling\n\nDue to the limited amount of time we have, we will not go over those methods here. Interested readers may refer to the references below.\n\nReference used:\n\nChapter 6 of Rizzo, M.L. (2007). Statistical Computing with R. CRC Press, Roca Baton.\nMonte Carlo Method page on Wikiepdia, Link.\nWasserman, L. (2003). All of Statistics: A Concise Course in Statistical Inference.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Monte Carlo Simulation and Variance Reduction</span>"
    ]
  },
  {
    "objectID": "05-cv.html",
    "href": "05-cv.html",
    "title": "5¬† Cross Validation",
    "section": "",
    "text": "5.1 Introduction\nThis chapter covers resampling technique called cross-validation.\nCross-validation (CV) is a statistical method used to estimate the skill of machine learning (ML) models. It is commonly used in applied ML to compare and select a model for a given predictive modeling problem because it is easy to understand, easy to implement, and results in skill estimates that generally have a lower bias than other methods.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#resampling-methods",
    "href": "05-cv.html#resampling-methods",
    "title": "5¬† Cross Validation",
    "section": "5.2 Resampling methods",
    "text": "5.2 Resampling methods\nResampling methods is a way to use the observed data to generate hypothetical samples. It treats an observed sample as a finite population, and random samples are generated/resampled from it to estimate population characteristics and make inferences about the sampled population. It is useful when:\n\nDo not know the underlying distribution of a population\nThe formula may be difficult to be calculated.\n\nSome commonly used resampling methods include:\n\nBootstrap: Bootstrap methods are often used when the distribution of the target population is not specified; the sample is the only information available.\nJackknife: The jackknife is a resampling technique used to estimate the bias and variance of a statistic. It is like a leave-one-out (LOO) cross-validation.\nCross-validation: Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\nPermutation tests: Permutation tests are a type of non-parametric statistical test that involves rearranging the data points to test a hypothesis. They are used to determine whether the observed effect is statistically significant by comparing it to the distribution of effects obtained through random permutations of the data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#machine-learning-models",
    "href": "05-cv.html#machine-learning-models",
    "title": "5¬† Cross Validation",
    "section": "5.3 Machine Learning Models",
    "text": "5.3 Machine Learning Models\n\nLeft: What machine learning can do\nRight: Model/Methods",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#training-and-testingvalidating-sets",
    "href": "05-cv.html#training-and-testingvalidating-sets",
    "title": "5¬† Cross Validation",
    "section": "5.4 Training and testing/validating sets",
    "text": "5.4 Training and testing/validating sets\n\n\n\n\n\n\n\n\n\nQuestions\nHow do we choose between different models \\(f_1,\\dots,f_m\\)?",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#setup",
    "href": "05-cv.html#setup",
    "title": "5¬† Cross Validation",
    "section": "5.5 Setup",
    "text": "5.5 Setup\nSuppose we have a supervised learning model \\(f(X)\\to Y\\).\nDenote the training set by \\(\\mathcal{T}=\\{(X_i,Y_i)\\}_{i=1}^{N_{train}}\\).\nHow to choose between the models \\(f_1,\\dots,f_m\\)?\nIdeal: \\((X,Y)\\sim F_{X,Y}\\).\nDefine the generalization error (Population error) as \\[\n  \\mathrm{Err}(f) = \\mathbb{E}_{X,Y}[\\{Y-f(X)\\}^2]\n\\]\nChoose \\(f\\) by \\[\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\mathrm{Err}(f)\n\\]\nBut, \\((X,Y)\\sim F_{X,Y}\\) is usally unknown!\nQuestion:\nWhat to do if we do not know about \\(F_{X,Y}\\)?\nLet \\(V:=\\{(X_i,Y_i)\\}_{i=1}^{N_{Val}}\\) be the validating/testing set.\n\\[\n  \\mathrm{Err}(f) = E[(Y-f(X))^2] \\approx \\frac{1}{N_{Val}}\\sum_{i=1}^{N_{Val}}(Y_i-f(X_i))^2 =: \\mathrm{err}_{Val}(f).\n\\]\nWhen \\(N_{Val}\\to\\infty\\), \\(\\mathrm{err}_{Val}(f)\\to \\mathrm{Err}(f)\\).\nWe can then do \\[\n  \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\mathrm{err}_{Val}(f)\n\\] If \\(N_{val}\\) is large, we can have good estimate of \\(\\mathrm{Err}(f)\\).\nBut actually, we may only have small validating set.\nProblems with simple Train-Test Split:\n\nSplitting 50-50 wastes data that could improve the model.\nSplitting 80-20 may leave too little test data for reliable evaluation.\n\nWhat we can do? Cross-validation!\nCross-validation uses all data efficiently for training and testing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#what-is-cross-validation",
    "href": "05-cv.html#what-is-cross-validation",
    "title": "5¬† Cross Validation",
    "section": "5.6 What is Cross-Validation?",
    "text": "5.6 What is Cross-Validation?\nSuppose there are 5 folds (\\(K=5\\)).\n\nPut together, we have\n\\[\n  \\mathrm{err}_{cv}(f) = \\frac{1}{N}\\sum_{k=1}^5 \\sum_{i\\in S_k}(y_i-\\hat{f}^{[s_k^C]}(x_i))^2.\n\\]\nThen cross-validation is to find \\[\n  f^* = \\arg\\min_{f\\in\\{f_1,\\dots,f_m\\}} \\mathrm{err}_{cv}(f).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#k-fold-cross-validation",
    "href": "05-cv.html#k-fold-cross-validation",
    "title": "5¬† Cross Validation",
    "section": "5.7 K-Fold Cross-Validation",
    "text": "5.7 K-Fold Cross-Validation\nLet \\(\\mathcal{D}=\\{X_i,Y_i\\}_{i=1}^N\\) be our data.\n\n\nSplit the data into K approximately equal sizes parts/fold \\(K\\)\nFor each \\(k=1,2,\\dots,K\\), repeat the following steps:\n\nLeave the \\(k\\)th fold \\(S_k\\) from the data \\(\\mathcal{D}\\), and denote the remaining data as \\(S_k^C\\). We fit the model to \\(S_k^C\\) and denote the corresponding model we obtained by \\(\\hat{f}^{[S_k^C]}\\)\nCalculate the total prediction error on the fitted model \\(\\hat{f}^{[S_k^C]}\\) on the left-out fold \\(S_k\\) \\[\n  \\mathrm{err}_{cv,k}(f) = \\sum_{i\\in S_k} L(Y_i, \\hat{f}^{[S_k^C]}(X_i)).\n\\]\n\nThe CV estimate of prediction error is \\[\n  \\mathrm{err}_{cv}(f) = \\frac{1}{N}\\sum_{k=1}^K \\mathrm{err}_{cv,k}(f).\n\\]\n\n\nSo if we have \\(M\\) models, \\(f_1,f_2,\\dots,f_M\\), we can use cross-validation to select the best model by computing the cross-validation error for each model \\(\\mathrm{err}_{cv}(f_1), \\mathrm{err}_{cv}(f_2),\\dots \\mathrm{err}_{cv}(f_M)\\)\nThen the best model is \\[\n  f^*=\\arg\\min_{f\\in\\{f_1,\\dots,f_M\\}} \\mathrm{err}_{cv}(f).\n\\]\nThere are two many use of the K-fold CV\n\nTune hyperparameters\n\n\nSuppose we have a family of models:\n\\(y=\\beta_0+\\sum_{j=1}^L\\beta_kx^k+\\varepsilon\\), \\(L\\) is the hyperparameter here.\nM1. \\(y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\)\nM2. \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\varepsilon\\)\nM3. \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\beta_3 x^3+ \\varepsilon\\)\nWe may use K-fold CV to choose the best \\(L\\).\n\n\nTo better evaluate the performance of a model\n\nThe number of folds depends on the data size.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#discussion",
    "href": "05-cv.html#discussion",
    "title": "5¬† Cross Validation",
    "section": "5.8 Discussion",
    "text": "5.8 Discussion\nQuestion:\nWhat would you consider when choosing \\(K\\)?**\nAnswer: The Bias-Variance Decomposition!\n\\[\\begin{align*}\n\\text{Generalization error} &~=~ \\text{variance} &~+~& \\text{bias} &~+~& \\text{irreducible error} \\\\\n\\mathbb{E}_\\mathcal{T}[(y-f(x;\\mathcal{T}))^2] &~=~ \\mathbb{V}ar(x) &~+~& \\mathbb{B}\\mathrm{ias}^2(x) &~+~& \\varepsilon^2.\n\\end{align*}\\]\nSee Section 7.3, Equation (7.9) Hastie et al.¬†(2009).\n\nPop-up quiz:\nQ: What are the characteristic a good model \\(f\\) should have?\n\nLow bias, high variance\nHigh bias, low variance\nLow bias, low variance\nHigh bias, high variance\nNone of above\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n. \n\nBias (Systematic error): The difference between predicted values and the true target.\nVariance (Sensitivity to data changes): How much predictions change with new data.\nGoal: Minimize both bias and variance.\n\n\n\n\nPicture borrowed from 1.\n\n\n\nWith new data \\((X,Y)\\) from the same distribution:",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#choice-of-fold-k",
    "href": "05-cv.html#choice-of-fold-k",
    "title": "5¬† Cross Validation",
    "section": "5.9 Choice of Fold K",
    "text": "5.9 Choice of Fold K\nBias-Variance Tradeoff in CV:\nThe choice of K is a tradeoff between bias and variance.\n\n\n\n\n\n\n\nQ: What values of \\(K, 2 \\leq K \\leq N\\) should we use?\n\nLarge \\(K\\): high variance, but small bias.\nSmall \\(K\\): low variance, but high bias.\n\n\n\n\nBias decreases as \\(K\\) increases.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#example-stock-market",
    "href": "05-cv.html#example-stock-market",
    "title": "5¬† Cross Validation",
    "section": "5.10 Example: Stock market",
    "text": "5.10 Example: Stock market\nWe look at the dataset in Smarket package in R.\nIt contains the daily percentage returns for the S&P 500 stock index between 2001 and 2005.\n\\(N = 1250\\)\n\nlibrary(ISLR2)\nlibrary(kableExtra)\n\nattach(Smarket)\n\n# Create the table and scale it to fit the page\nhead(Smarket) %&gt;%\n  kable(\"latex\", caption =\" First Few Rows of Smarket Dataset\") %&gt;%\n  kable_styling(full_width = FALSE, position = \"center\", font_size = 8)",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#leave-one-out-cross-validation-loocv",
    "href": "05-cv.html#leave-one-out-cross-validation-loocv",
    "title": "5¬† Cross Validation",
    "section": "5.11 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "5.11 Leave-One-Out Cross-Validation (LOOCV)\n\nWhen \\(K=N\\), the size of the training data, it is leave-one-out cross validation.\nInstead of creating two subsets of comparable size, a single observation \\((x_i, y_i)\\) is used for the validation set and the remaining observations make up the training set.\nRepeat this for each observation and get the average.\n\n\n\n\nIllustration for Leave one out CV.\n\n\n\n\n\n5 fold CV.\n\n\n\n\n\nResult of the 5-fold CV with 10 runs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#take-home-messages",
    "href": "05-cv.html#take-home-messages",
    "title": "5¬† Cross Validation",
    "section": "5.12 Take Home Messages",
    "text": "5.12 Take Home Messages\nWhat is CV?\n\nA method to estimate prediction error using all data efficiently.\n\nWhy K-fold?:\n\nBalances bias and variance effectively.\n\nLOOCV:\n\nSpecial case with K = N, unbiased but expensive.\n\nPractical Tips:\n\nK = 5 or K = 10 is common and (usually) works well.\nUse CV to tune hyperparameters and compare models.\n\nNote:\n\nCross-validation can be applied in various contexts!\n\n\nEach panel reproduces your model fit:\n\nLinear: \\(y = \\beta_0 + \\beta_1 x\\)\nQuadratic: \\(y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\)\nExponential: \\(\\log y = \\beta_0 + \\beta_1 x\\)\nLog‚ÄìLog: \\(\\log y = \\beta_0 + \\beta_1 \\log x\\)\n\nOnce the model is estimated, we want to assess the fit. CV can be used to estimate the prediction errors.\n\nlibrary(DAAG); attach(ironslag)\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# --- raw data ---\n  df &lt;- data.frame(chemical, magnetic)\na  &lt;- seq(10, 40, 0.1)\n\n# --- fits (using coefficients explicitly) ---\nL1 &lt;- lm(magnetic ~ chemical, data = df)\nyhat1 &lt;- L1$coef[1] + L1$coef[2] * a\n\nL2 &lt;- lm(magnetic ~ chemical + I(chemical^2), data = df)\nyhat2 &lt;- L2$coef[1] + L2$coef[2] * a + L2$coef[3] * a^2\n\nL3 &lt;- lm(log(magnetic) ~ chemical, data = df)\nlogyhat3 &lt;- L3$coef[1] + L3$coef[2] * a\nyhat3 &lt;- exp(logyhat3)\n\nL4 &lt;- lm(log(magnetic) ~ log(chemical), data = df)\nlogyhat4 &lt;- L4$coef[1] + L4$coef[2] * log(a)\n\n# --- assemble data for plotting ---\nfits &lt;- data.frame(\n  a = a,\n  yhat1 = yhat1,\n  yhat2 = yhat2,\n  yhat3 = yhat3,\n  loga  = log(a),\n  logyhat4 = logyhat4\n)\n\n# --- plots ---\np1 &lt;- ggplot(df, aes(x = chemical, y = magnetic)) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = a, y = yhat1), linewidth = 1.1, color = \"steelblue\") +\n  ggtitle(\"Linear\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\np2 &lt;- ggplot(df, aes(x = chemical, y = magnetic)) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = a, y = yhat2), linewidth = 1.1, color = \"darkgreen\") +\n  ggtitle(\"Quadratic\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\np3 &lt;- ggplot(df, aes(x = chemical, y = magnetic)) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = a, y = yhat3), linewidth = 1.1, color = \"firebrick\") +\n  ggtitle(\"Exponential\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\np4 &lt;- ggplot(df, aes(x = log(chemical), y = log(magnetic))) +\n  geom_point(shape = 16) +\n  geom_line(data = fits, aes(x = loga, y = logyhat4), linewidth = 1.1, color = \"purple\") +\n  ggtitle(\"Log‚ÄìLog\") +\n  theme_bw(base_size = 13) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# --- combine in 2√ó2 grid ---\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nattach(ironslag)\n\nThe following objects are masked from ironslag (pos = 3):\n\n    chemical, magnetic\n\nn &lt;- length(magnetic) # in DAAG ironslag\ne1 &lt;- e2 &lt;- e3 &lt;- e4 &lt;- numeric(n)\n# for n-fold cross validation\n# fit models on leave-one-out samples\nfor (k in 1:n) {\n  y &lt;- magnetic[-k]\n  x &lt;- chemical[-k]\n  J1 &lt;- lm(y ~ x)\n  yhat1 &lt;- J1$coef[1] + J1$coef[2] * chemical[k]\n  e1[k] &lt;- magnetic[k] - yhat1\n  \n  J2 &lt;- lm(y ~ x + I(x^2))\n  yhat2 &lt;- J2$coef[1] + J2$coef[2] * chemical[k] +\n    J2$coef[3] * chemical[k]^2\n  e2[k] &lt;- magnetic[k] - yhat2\n  \n  J3 &lt;- lm(log(y) ~ x)\n  logyhat3 &lt;- J3$coef[1] + J3$coef[2] * chemical[k]\n  yhat3 &lt;- exp(logyhat3)\n  e3[k] &lt;- magnetic[k] - yhat3\n  \n  J4 &lt;- lm(log(y) ~ log(x))\n  logyhat4 &lt;- J4$coef[1] + J4$coef[2] * log(chemical[k])\n  yhat4 &lt;- exp(logyhat4)\n  e4[k] &lt;- magnetic[k] - yhat4\n}\n\n# compute MSEs\nmse &lt;- c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))\n\n# put into a tidy data frame\nresults &lt;- data.frame(\n  Model = c(\"Linear\", \"Quadratic\", \"Exponential\", \"Log‚ÄìLog\"),\n  MSE = round(mse, 4)\n)\n\n# display as a clean table\nknitr::kable(results, \n             caption = \"Model Comparison of Mean Squared Errors\", align = c(\"l\", \"c\"))\n\n\nModel Comparison of Mean Squared Errors\n\n\nModel\nMSE\n\n\n\n\nLinear\n19.5564\n\n\nQuadratic\n17.8525\n\n\nExponential\n18.4419\n\n\nLog‚ÄìLog\n20.4542\n\n\n\n\nL2\n\n\nCall:\nlm(formula = magnetic ~ chemical + I(chemical^2), data = df)\n\nCoefficients:\n  (Intercept)       chemical  I(chemical^2)  \n     24.49262       -1.39334        0.05452  \n\n\nSo the best fitted model is \\[\n\\hat{Y} =\n24.493 +\n-1.393\\,X +\n0.055\\,X^2\n\\]\n\npar(mfrow = c(2, 2)) #layout for graphs\nplot(L2$fit, L2$res) #residuals vs fitted values\nabline(0, 0) #reference line\nqqnorm(L2$res) #normal probability plot\nqqline(L2$res) #reference line\npar(mfrow = c(1, 1)) #restore display\n\n\n\n\n\n\n\n\n\n\n\nNote: This lecture is based on the book by Hastie et al.¬†(2009), and James et al.¬†(2013).\nSection 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). The Elements of Statistical Learning. Springer, 2nd edition.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "05-cv.html#footnotes",
    "href": "05-cv.html#footnotes",
    "title": "5¬† Cross Validation",
    "section": "",
    "text": "https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/‚Ü©Ô∏é",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "06-resample.html",
    "href": "06-resample.html",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "",
    "text": "6.1 Introduction\nThis chapter continues from the previous chapter, introducing more resampling methods; jackknife and bootstrap techniques.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#resampling-methods",
    "href": "06-resample.html#resampling-methods",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.2 Resampling methods",
    "text": "6.2 Resampling methods\nResampling methods is a way to use the observed data to generate hypothetical samples. It treats an observed sample as a finite population, and random samples are generated/resampled from it to estimate population characteristics and make inferences about the sampled population. It is useful when:\n\nDo not know the underlying distribution of a population\nThe formula may be difficult to be calculated.\n\nSome commonly used resampling methods include:\n\nCross-validation: Cross-validation is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.\nBootstrap: Bootstrap methods are often used when the distribution of the target population is not specified; the sample is the only information available.\nJackknife: The jackknife is a resampling technique used to estimate the bias and variance of a statistic. It is like a leave-one-out (LOO) cross-validation.\nPermutation tests: Permutation tests are a type of non-parametric statistical test that involves rearranging the data points to test a hypothesis. They are used to determine whether the observed effect is statistically significant by comparing it to the distribution of effects obtained through random permutations of the data.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#estimators",
    "href": "06-resample.html#estimators",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.3 Estimators",
    "text": "6.3 Estimators\n\n6.3.1 Bias-Variance Tradeoff of an estimator\nIn statistics, the bias-variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters. If we look at the Mean square error (MSE) of an estimator \\(\\hat \\theta\\) for a parameter \\(\\theta\\):\n\nSuppose we have a parameter \\(\\theta\\) and an estimator \\(\\hat \\theta\\). The mean square error (MSE) of the estimator is defined as \\[\\mathrm{MSE}_{\\theta}(\\hat \\theta)=\\mathbb E[(\\hat \\theta-\\theta )^2 ]= \\mathbb{V}ar(\\hat \\theta) + [\\mathbb b(\\hat \\theta)]^2,\\] where \\(\\mathbb b\\) is the bias of the estimator.\n\n\n\n\n\n\n\nNoteDerivation\n\n\n\nWe start with the definition of the mean squared error (MSE): \\[\n\\mathrm{MSE}_\\theta(\\hat \\theta)\n= \\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big].\n\\]\nAdd and subtract \\(\\mathbb E[\\hat \\theta]\\) inside the square: \\[\n\\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big]\n= \\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta] + \\mathbb E[\\hat \\theta] - \\theta)^2\\big].\n\\]\nExpand the square: \\[\n\\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])^2\\big]\n+ 2\\,\\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])(\\mathbb E[\\hat \\theta] - \\theta)\\big]\n+ (\\mathbb E[\\hat \\theta] - \\theta)^2.\n\\]\nThe middle term vanishes because \\[\n\\mathbb E\\big[\\hat \\theta- \\mathbb E[\\hat \\theta]\\big] = 0.\n\\]\nHence, \\[\n\\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big]\n= \\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])^2\\big]\n+ (\\mathbb E[\\hat \\theta] - \\theta)^2.\n\\]\nRecognizing that \\[\n\\mathbb{V}ar(\\hat \\theta) = \\mathbb E\\big[(\\hat \\theta- \\mathbb E[\\hat \\theta])^2\\big]\n\\quad \\text{and} \\quad\nb(\\hat \\theta) = \\mathbb E[\\hat \\theta] - \\theta,\n\\] we obtain the bias‚Äìvariance decomposition: \\[\n\\boxed{\n\\mathbb E\\big[(\\hat \\theta- \\theta)^2\\big]\n= \\mathbb{V}ar(\\hat \\theta) + [b(\\hat \\theta)]^2.}\n\\]\n\n\nQuestions: Why do we care about the bias? Can we always find an unbiased estimator? What does it mean?\nFor most of the course, we focus on the unbiased estimator. This may often be obtained from using the LLN. Then, such as in the MC Chapter, we can compare the (relative) efficiency between the estimators, and discuss about the variance reduction.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#bootstrap",
    "href": "06-resample.html#bootstrap",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.4 Bootstrap",
    "text": "6.4 Bootstrap\nBootstrap estimates of a sampling distribution are analogous to the idea of density estimation. We construct a histogram of a sample to obtain an estimate of the shape of the density function. The histogram is not the density, but in a nonparametric problem, can be viewed as a reasonable estimate of the density. We have methods to generate random samples from completely specified densities; bootstrap generates random samples from the empirical distribution of the sample.\nSuppose we have an observed random sample \\(x=(x_1,\\dots,x_n)\\) from a distribution \\(F\\). If \\(X^\\ast\\) is selected at random from \\(x\\), then \\[P(X^\\ast=x_i)=1/n,\\quad i=1,\\dots,n.\\]\nResampling generates a random sample \\(X^\\ast_1,\\dots,X_n^\\ast\\) by sampling with replacement from the observed sample \\(x\\). Then the RVs \\(X_i^\\ast\\) are i.i.d. and uniformly distributed on the observed data points \\(\\{x_1,\\dots,x_n\\}\\).\nThe empirical distribution function (ecdf) \\(F_n(x)\\) is an estimator of \\(F(x)\\). It can be shown that \\(F_n(x)\\) is a sufficient statistic for \\(F(x)\\); that is, all the information about \\(F(x)\\) that is contained in the sample is also contained in \\(F_n(x)\\). Moreover, \\(F_n(x)\\) is itself the distribution function of a random variable; namely the random variable that is uniformly distributed on the set \\(\\{x_1, \\dots , x_n\\}\\). Hence the empirical cdf \\(F_n\\) is the cdf of \\(X^\\ast\\). Thus in bootstrap, there are two approximations. The ecdf \\(F_n\\) is an approximation to the cdf \\(F_X\\). The ecdf \\(F_m^\\ast\\) of the bootstrap replicates is an approximation to the ecdf \\(F_n\\). Resampling from the sample \\(x\\) is equivalent to generating random samples from the distribution \\(F_n(x)\\). The two approximations can be represented by the diagram\n\\[\n\\begin{aligned}\nF & \\rightarrow X \\rightarrow F_n \\\\\nF_n & \\rightarrow X^* \\rightarrow F_n^* .\n\\end{aligned}\n\\]\nTo generate a bootstrap random sample by resampling \\(x\\), generate \\(n\\) random integers \\(\\{i_1,\\dots, i_n\\}\\) uniformly distributed on \\(\\{1,\\dots , n\\}\\) and select the bootstrap sample \\(x^\\ast = (x_{i_1} ,\\dots , x_{i_n} )\\).\n\nLet \\(\\theta\\) be the parameter of interest ( \\(\\theta\\) could be a vector), and \\(\\hat \\theta\\) be an estimator. Then the bootstrap estimate of the distribution of \\(\\hat \\theta\\) is obtained as follows.\n\n\nFor each bootstrap replicate, indexed \\(b = 1, \\dots, B\\):\n\nGenerate sample \\(x^{\\ast (b)} = x_1^\\ast,\\dots,x_n^\\ast\\) by sampling with replacement from the observed sample \\(x_1,\\dots,x_n\\).\nCompute the \\(b\\)th replicate \\(\\hat \\theta^{(b)}\\) from the \\(b\\)th bootstrap sample.\n\nThe bootstrap estimate of \\(F_{\\hat \\theta}(\\cdot)\\) is the empirical distribution of the replicates \\(\\hat \\theta^{(1)},\\dots ,\\hat \\theta^{(B)}\\).\n\n\n\nSuppose that we have observed the sample \\[x = \\{2, 2, 1, 1, 5, 4, 4, 3, 1, 2\\}.\\]\nResampling from \\(x\\) we select \\(1, 2, 3, 4\\), or \\(5\\) with probabilities \\(0.3, 0.3, 0.1, 0.2\\), and \\(0.1\\), respectively, so the cdf \\(F_{X^\\ast}\\) of a randomly selected replicate is exactly the ecdf \\(F_n(x)\\):\n\\[\nF_{X *}(x)=F_n(x)= \\begin{cases}0, & x&lt;1 ; \\\\ 0.3, & 1 \\leq x&lt;2 ; \\\\ 0.6, & 2 \\leq x&lt;3 ; \\\\ 0.7, & 3 \\leq x&lt;4 ; \\\\ 0.9, & 4 \\leq x&lt;5 ; \\\\ 1, & x \\geq 5 .\\end{cases}\n\\]\nNote that if \\(F_n\\) is not close to \\(F_X\\) then the distribution of the replicates will not be close to \\(F_X\\). The sample \\(x\\) above is actually a sample from a Poisson(2) distribution. Resampling from \\(x\\) a large number of replicates produces a good estimate of \\(F_n\\) but not a good estimate of \\(F_X\\) , because regardless of how many replicates are drawn, the bootstrap samples will never include \\(0\\).\n\n\n6.4.1 Bootstrap Estimation of Standard Error\nRecall that, in normal approximate, the \\((100-\\alpha)\\)% confidence interval (for \\(\\mu\\)) is given as \\[\n  \\bar{x}_n \\pm z_{1-\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}.\n\\] In the above formula, we refer\n\n\\(\\sigma/\\sqrt{n}\\): as the standard error (se).\n\\(z_{1-\\alpha/2}  \\sigma/\\sqrt{n}\\): as the margin of error.\n\n\nThe standard error (SE) of a statistic is the standard deviation of its sampling distribution. The se is often used in calculations of confidence intervals.\n\nTo estimate the se of an estimator \\(\\hat \\theta\\), we can use the bootstrap method. Let \\(\\hat \\theta^{(1)},\\dots,\\hat \\theta^{(B)}\\) be the bootstrap replicates. The bootstrap estimate of the standard error of \\(\\hat \\theta\\) is given by the sample standard deviation of the replicates:\n\\[\n\\widehat{s e}\\left(\\hat{\\theta}^*\\right)=\\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B\\left(\\hat{\\theta}^{(b)}-\\overline{\\hat{\\theta}^*}\\right)^2},\n\\] where \\(\\overline{\\hat{\\theta}^*}=\\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{(b)}\\).\n\n\n\n\n\n\nNoteRemark for estimating se\n\n\n\nThe number of replicates needed for good estimates of standard error is not large; \\(B = 50\\) is usually large enough, and rarely is \\(B &gt; 200\\) necessary.\n\n\n\nExample 8.2 (Bootstrap estimate of standard error). The law school data set law in the bootstrap [286] package is from Efron and Tibshirani [91]. The data frame contains LSAT (average score on law school admission test score) and GPA (average undergraduate grade point average) for 15 law schools.\n\n\n\nObservation\nLSAT\nGPA\n\n\n\n\n1\n576\n339\n\n\n2\n635\n330\n\n\n3\n558\n281\n\n\n4\n578\n303\n\n\n5\n666\n344\n\n\n6\n580\n307\n\n\n7\n555\n300\n\n\n8\n661\n343\n\n\n9\n651\n336\n\n\n10\n605\n313\n\n\n11\n653\n312\n\n\n12\n575\n274\n\n\n13\n545\n276\n\n\n14\n572\n288\n\n\n15\n594\n296\n\n\n\nThis data set is a random sample from the universe of 82 law schools in law82 (bootstrap). Estimate the correlation between LSAT and GPA scores, and compute the bootstrap estimate of the standard error of the sample correlation.\nThe bootstrap procedure is\n\nFor each bootstrap replicate, indexed \\(b = 1,\\dots , B\\)\n\nGenerate sample \\((\\text{LSAT}^{\\ast (b)}_i, \\text{GPA}^{\\ast (b)}_i), i = 1,\\dots , n\\) by sampling with replacement from the observed sample.\nCompute the \\(b\\)th replicate of the correlation \\(\\hat \\theta^{(b)} = \\cor(\\text{LSAT}^{\\ast (b)}, \\text{GPA}^{\\ast (b)})\\) from the \\(b\\)th bootstrap sample.\n\nThe bootstrap estimate of the standard error of \\(\\hat \\theta\\) is the sample standard deviation of the replicates \\(\\hat \\theta^{(1)},\\dots ,\\hat \\theta^{(B)}\\), \\(\\{R^{(i)}\\}_{i=1}^B\\)\n\n\nlibrary(bootstrap) #for the law data\nprint(cor(law$LSAT, law$GPA))\n\n[1] 0.7763745\n\n# [1] 0.7763745\nprint(cor(law82$LSAT, law82$GPA))\n\n[1] 0.7599979\n\n# [1] 0.7599979\n\n\n#set up the bootstrap\nB &lt;- 200 #number of replicates\nn &lt;- nrow(law) #sample size\nR &lt;- numeric(B) #storage for replicates\n#bootstrap estimate of standard error of R\nfor (b in 1:B) {\n  # randomly select the indices\n  i &lt;- sample(1:n, size = n, replace = TRUE)\n  LSAT &lt;- law$LSAT[i] # i is a vector of indices\n  GPA &lt;- law$GPA[i]\n  R[b] &lt;- cor(LSAT, GPA)\n}\n#output\n(se.R &lt;- sd(R))\n\n[1] 0.1455406\n\n# [1] 0.1358393\nhist(R, prob = TRUE)\n\n\n\n\n\n\n\n\nThe bootstrap estimate of se(R) is 0.1455406. The normal theory estimate for standard error of R is 0.115.\n\n\nWe can also use the boot function in the boot package to compute the bootstrap estimate of standard error.\nFirst, write a function that returns \\(\\hat \\theta(b)\\), where the first argument to the function is the sample data, and the second argument is the vector \\(\\{i_1, \\dots , i_n\\}\\) of indices. If the data is \\(x\\) and the vector of indices is i, we need \\(x[i,1]\\) to extract the first resampled variable, and x[i,2] to extract the second resampled variable. The code and output is shown below.\n\nmy_r &lt;- function(x, i) {\n  # want correlation of columns 1 and 2\n  cor(x[i, 1], x[i, 2])\n}\n\nThe printed summary of output from the boot function is obtained by the command boot or the result can be saved in an object for further analysis. Here we save the result in obj and print the summary.\n\nlibrary(boot) #for boot function\nset.seed(777)\n(obj &lt;- boot(data = bootstrap::law, statistic = my_r, R = 2000))\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = bootstrap::law, statistic = my_r, R = 2000)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.7763745 -0.01075929   0.1364617\n\n\nThe observed value ÀÜŒ∏ of the correlation statistic is labeled t1*. The bootstrap estimate of standard error of the estimate i≈ù se(ÀÜŒ∏) . = 0.13, based on 2000 replicates. To compare with formula (8.1), extract the replicates in $t, so that the standard deviation of the replicates can be computed directly\n\nsd(obj$t)\n\n[1] 0.1364617\n\n\n\n\n\n6.4.2 Bootstrap Estimation of Bias\nRecall that a bias of an estimator \\(\\hat \\theta\\) is defined as \\[\n\\mathbb b(\\hat \\theta) = \\mathbb E[\\hat \\theta] - \\theta.\n\\]\nAn example of a biased estimator is the MLE of variance, \\(\\hat{\\sigma}^2 = \\sum (X_i-\\bar{X}_n)^2/n\\) of \\(\\sigma^2 = \\sum (X_i-\\bar{X}_n)^2/(n-1)\\), which has expected value \\((1 ‚àí 1/n)\\sigma^2\\). Here, the bias is \\(‚àí\\sigma^2/n\\).\nThe bootstrap estimation of bias uses the bootstrap replicates of \\(\\hat \\theta\\) to estimate the sampling distribution of \\(\\hat \\theta\\). For the finite population \\(x = (x_1,\\dots , x_n)\\), the parameter is \\(\\hat \\theta(x)\\) and there are \\(B\\) i.i.d. estimators \\(\\hat \\theta^{(b)}\\). The sample mean of the replicates {ÀÜŒ∏(b)} is unbiased for its expected value \\(E[\\hat \\theta^\\ast]\\), so the bootstrap estimate of bias is\n\\[bias(ÀÜŒ∏) = ÀÜŒ∏‚àó ‚àí ÀÜŒ∏\\], where \\(\\hat \\theta^\\ast  = \\sum_{b=1}^B \\hat \\theta^{(b)}\\), and \\(\\hat \\theta= \\hat \\theta(x)\\) is the estimate computed from the original observed sample. (In bootstrap \\(F_n\\) is sampled in place of \\(F_X\\) , so we replace \\(\\theta\\) with \\(\\hat \\theta\\) to estimate the bias.)\n\n\n\n\n\n\nNoteRemark for estimating bias\n\n\n\n\n\\(\\oplus\\) Positive bias indicates that \\(\\hat \\theta\\) on average tends to overestimate \\(\\theta\\),\n\\(\\ominus\\) Negative bias is on average underestimate \\(\\theta\\)\n\n\n\n\nIn the law data of Example 8.2, compute the bootstrap estimate of bias in the sample correlation.\n\n# sample estimate for n=15\ntheta.hat &lt;- cor(law$LSAT, law$GPA)\n# bootstrap estimate of bias\nB &lt;- 2000 # larger for estimating bias\nn &lt;- nrow(law)\ntheta.b &lt;- numeric(B)\nfor (b in 1:B) {\n  i &lt;- sample(1:n, size = n, replace = TRUE)\n  LSAT &lt;- law$LSAT[i]\n  GPA &lt;- law$GPA[i]\n  theta.b[b] &lt;- cor(LSAT, GPA)\n}\nbias &lt;- mean(theta.b - theta.hat)\nbias\n\n[1] -0.001063244\n\n# [1] -0.005797944\n\nThe estimate of bias is -0.005797944. Note that this is close to the estimate of bias returned by the boot function in Example 8.3. See Section 9.1 for the jackknife-after-bootstrap method to estimate the standard error of the bootstrap estimate of bias.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#others",
    "href": "06-resample.html#others",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.5 Others",
    "text": "6.5 Others\nThe distribution of \\(X^\\ast\\) is the empirical distribution \\(F_n\\) of the sample \\(x\\). The empirical distribution places mass \\(1/n\\) at each observed data point. The bootstrap uses the empirical distribution as an estimate of the true but unknown distribution \\(F\\).\nThe bootstrap is a resampling method that allows estimation of the sampling distribution of almost any statistic using random sampling methods.\n\nParametric bootstrap\nNonparametric bootstrap: In nonparametric bootstrap, the distribution is not specified.\n\n\n\nset.seed(333)\nn &lt;- 1E4\nB &lt;- 1E4\nx &lt;- rnorm(n, mean = 5, sd = 2) #original sample\n\n\nboot_means &lt;- pbapply::pbsapply(1:B, function(i){\n  indices &lt;- sample(1:n, size = n, replace = TRUE) #resample\n  boot.sample &lt;- x[indices]\n  mean(boot.sample)\n}) \nmean(boot_means)\n\n[1] 4.999525\n\nhist(boot_means, breaks = 50, main = \"Bootstrap distribution of the mean\", xlab = \"Mean\")\n\n\n\n\n\n\n\n# Compute statistics\ntrue_mean &lt;- 5\nboot_mean &lt;- mean(boot_means)\ndf &lt;- tibble(x = boot_means)\n# Plot\nggplot(df, aes(x = x)) +\n  geom_histogram(aes(y = ..density..), bins = 50, \n                 fill = \"lightgreen\", color = \"black\", alpha = 0.7) +\n  geom_density(color = \"darkgreen\", size = 1) +\n  geom_vline(xintercept = true_mean, color = \"red\", linetype = \"dashed\", size = 1.1) +\n  geom_vline(xintercept = boot_mean, color = \"blue\", linetype = \"solid\", size = 1.1) +\n  labs(\n    title = \"Bootstrap Distribution of the Sample Mean\",\n    x = \"Bootstrap Mean\",\n    y = \"Density\"\n  ) +\n  annotate(\"text\", x = true_mean, y = 0.3, label = \"True mean = 5\", color = \"red\", hjust = -0.1) +\n  annotate(\"text\", x = boot_mean, y = 0.25, label = sprintf(\"Bootstrap mean = %.2f\", boot_mean),\n           color = \"blue\", hjust = -0.1) +\n  theme_minimal(base_size = 14)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\nThe bootstrap is a general tool for assessing statistical accuracy. First we describe the bootstrap in general, and then show how it can be used to estimate extra-sample prediction error. As with cross-validation, the boot- strap seeks to estimate the conditional error ErrT , but typically estimates well only the expected prediction error Err.\nThe distribution of the finite population represented by the sample can be regarded as a pseudo-population with similar characteristics as the true population. By repeatedly generating random samples from this pseudo-population (resampling), the sampling distribution of a statistic can be estimated. Properties of an estimator such as bias or standard error can be estimated by resampling.\nBootstrap estimates of a sampling distribution are analogous to the idea of density estimation. We construct a histogram of a sample to obtain an estimate of the shape of the density function. The histogram is not the density, but in a nonparametric problem, can be viewed as a reasonable estimate of the density. We have methods to generate random samples from completely specified densities; bootstrap generates random samples from the empirical distribution of the sample.\nThe term bootstrap can refer to nonparametric bootstrap or parametric bootstrap. Monte Carlo methods that involve sampling from a fully specified probability distribution, such as methods of Chapter 7 are sometimes called parametric bootstrap. Nonparametric bootstrap is the subject of this chapter. In nonparametric bootstrap, the distribution is not specified.\nTo generate a bootstrap random sample by resampling \\(x\\), generate n random integers \\(\\{i_1,\\dots, i_n\\}\\) uniformly distributed on \\(\\{1,\\dots , n\\}\\) and select the bootstrap sample \\(x^‚àó = (x_{i_1} ,\\dots, x_{i_n} )\\). Suppose \\(\\theta\\) is the parameter of interest (\\(\\theta\\) could be a vector), and \\(\\hat \\theta\\) is an estimator of \\(\\theta\\). Then the bootstrap estimate of the distribution of \\(\\hat \\theta\\) is obtained as follows.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#jackknife",
    "href": "06-resample.html#jackknife",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.6 Jackknife",
    "text": "6.6 Jackknife\nThe jackknife is a resampling technique used to estimate the bias and variance of a statistic.\nJackknife is like a leave-one-out cross-validation. Let \\(\\mathbf{x}= (x_1,\\dots,x_n)\\) be an observed random sample, and denote the \\(i\\)th jackknife sample by \\(\\mathbf{x}_{-i} = (x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)\\), that is, a subset of \\(\\mathbf{x}\\).\nFor the parameter of interest \\(\\theta\\), if the statistics is \\(T(\\mathbf{x})=:\\hat \\theta\\) is computed on the full\n\n6.6.1 When does jackknife not work?\nJackknife does not work when the function \\(T(\\cdot)\\) is not a smooth functional!",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#jackknife-1",
    "href": "06-resample.html#jackknife-1",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.7 Jackknife",
    "text": "6.7 Jackknife\nThe jackknife is another resampling method, proposed by Quenouille [225, 224] for estimating bias, and by Tukey [289] for estimating standard error, a few decades earlier than the bootstrap. Efron [88] is a good introduction to the jackknife.\nJackknife is a special kind of Cross-validation where we leave-one-out (LOO) the observation, and calculate the quantities on the remaining data. To fix the idea, let \\(x=(x_1,\\dots,x_n)\\) be the observed data of samele size \\(n\\). The \\(i\\)th jackknife sample is defined as \\(x_{-i}=(x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)\\), that is, the sample with the \\(i\\)th observation removed. Let \\(\\hat \\theta=T(x)\\) be the estimator of the parameter of interest \\(\\theta\\). The \\(i\\)th jackknife replicate is defined as \\(\\hat \\theta_{-i}=T(x_{-i})\\), that is, the estimate computed from the \\(i\\)th jackknife sample. The jackknife estimate of bias is defined as\n\n6.7.1 Jackknife Estimate of Bias\nIf ÀÜŒ∏ is a smooth (plug-in) statistic, then \\(\\hat \\theta_{(\\cdot)} = t\\{F_{n‚àí1}(x(i))\\}\\), and the jackknife estimate of bias is \\[\n\\hat{b}_{jack} = (n ‚àí 1)(\\hat \\theta_{(\\cdot)} ‚àí \\hat \\theta),\n\\] where \\(\\overline{\\hat \\theta_{(\\cdot)}}=\\frac{1}{n} \\sum_{i=1}^n \\hat \\theta_{(i)}\\) is the average of the estimate from LOO samples, and \\(\\hat \\theta=\\hat \\theta(x)\\) is the estimate from the original observed sample.\n\nCompute the jackknife estimate of bias for the patch data in the bootstrap package.\n\ndata(patch, package = \"bootstrap\")\nn &lt;- nrow(patch)\ny &lt;- patch$y\nz &lt;- patch$z\ntheta.hat &lt;- mean(y) / mean(z)\nprint (theta.hat)\n\n[1] -0.0713061\n\n#compute the jackknife replicates, leave-one-out estimates\ntheta.jack &lt;- numeric(n)\nfor (i in 1:n) {\n  theta.jack[i] &lt;- mean(y[-i]) / mean(z[-i])\n}\n(bias &lt;- (n - 1) * (mean(theta.jack) - theta.hat) )\n\n[1] 0.008002488",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#applications",
    "href": "06-resample.html#applications",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.8 Applications",
    "text": "6.8 Applications\nThese methods are widely used in statistical inference and have applications in various fields.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#side-reading-wont-be-on-the-exam",
    "href": "06-resample.html#side-reading-wont-be-on-the-exam",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.9 Side reading (Won‚Äôt be on the exam)",
    "text": "6.9 Side reading (Won‚Äôt be on the exam)\n\n\n\n\n\n\nNoteNormal theory for SE (will not be on exam)\n\n\n\nNormal Approximation for Correlation SE\nFor a bivariate normal population, the sampling distribution of the sample correlation \\(r\\) is approximately normal with variance\n\\[\n\\operatorname{Var}(r) \\approx \\frac{(1 - \\rho^2)^2}{n - 1}.\n\\]\nThus, the standard error (SE) under the normal approximation is\n\\[\n\\operatorname{se}_{\\text{normal}}(r) = \\sqrt{\\frac{(1 - r^2)^2}{n - 1}}.\n\\]\nIn the Law school example\nGiven: - \\(r = 0.7763745\\) - \\(n = 15\\)\nWe can compute:\n\\[\n1 - r^2 = 1 - (0.776)^2 = 1 - 0.602 = 0.398,\n\\] \\[\n\\operatorname{Var}(r) \\approx \\frac{(0.398)^2}{14} = \\frac{0.158}{14} = 0.0113,\n\\] \\[\n\\operatorname{se}(r) = \\sqrt{0.0113} = 0.106.\n\\]\nThis gives a rough normal-theory estimate of 0.106.\n\nFor Fisher z-Transformation Approximation\n\nA more accurate approximation uses the Fisher z-transform:\n\\[\nz = \\frac{1}{2}\\log\\!\\left(\\frac{1 + r}{1 - r}\\right),\n\\]\nwhich is approximately normal with\n\\[\n\\operatorname{Var}(z) = \\frac{1}{n - 3}.\n\\]\nApplying the delta method:\n\\[\n\\operatorname{se}(r) \\approx (1 - r^2)\\sqrt{\\frac{1}{n - 3}}.\n\\]\nPlug in the values:\n\\[\n\\operatorname{se}(r) = (1 - 0.776^2)\\sqrt{\\frac{1}{12}} = 0.398 \\times 0.289 = 0.115.\n\\]\n‚úÖ Normal approximation (Fisher z) SE = 0.115\nAlternatively, we may use the Bootstrap Estimate to compare with:\n\nlibrary(bootstrap) # for the law data\nset.seed(123)\n\n# Sample data\nlaw &lt;- data.frame(\n  LSAT = c(576, 635, 558, 578, 666, 580, 555, 661, 651, 605, 653, 575, 545, 572, 594),\n  GPA  = c(339, 330, 281, 303, 344, 307, 300, 343, 336, 313, 312, 274, 276, 288, 296)\n)\n\n# Sample correlation\nr &lt;- cor(law$LSAT, law$GPA)\nn &lt;- nrow(law)\n\n# Bootstrap estimate of SE\nB &lt;- 200\nR &lt;- replicate(B, {\n  i &lt;- sample(1:n, size = n, replace = TRUE)\n  cor(law$LSAT[i], law$GPA[i])\n})\nse_boot &lt;- sd(R)\n\n# Normal approximation\nse_normal &lt;- sqrt((1 - r^2)^2 / (n - 1))\n\n# Fisher z-approximation\nse_fisher &lt;- (1 - r^2) / sqrt(n - 3)\n\n# Display results\ndata.frame(\n  Method = c(\"Bootstrap\", \"Normal theory\", \"Fisher z-approximation\"),\n  SE = c(se_boot, se_normal, se_fisher)\n)\n\n                  Method        SE\n1              Bootstrap 0.1200175\n2          Normal theory 0.1061676\n3 Fisher z-approximation 0.1146741\n\n\n\n\n6.10 Reference:\n\nSection 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). The Elements of Statistical Learning. Springer, 2nd edition.\nChapter 8 of Rizzo, M.L. (2007). Statistical Computing with R. CRC Press, Roca Baton.\nWu, C.F.J. (1986).Bootstrap and Other Resampling Methods in Regression Analysis. The Annals of Statistics.\n\n\n\n\n6.11 R Functions\nSome useful R functions for resampling methods:\n\nsample(): Generate random samples from a specified set of data points.\nboot::boot(): Perform bootstrap resampling and compute statistics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#reference",
    "href": "06-resample.html#reference",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.10 Reference:",
    "text": "6.10 Reference:\n\nSection 7.11 in Hastie, T., Tibshirani R. and Friedman, J. (2008). The Elements of Statistical Learning. Springer, 2nd edition.\nChapter 8 of Rizzo, M.L. (2007). Statistical Computing with R. CRC Press, Roca Baton.\nWu, C.F.J. (1986).Bootstrap and Other Resampling Methods in Regression Analysis. The Annals of Statistics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "06-resample.html#r-functions",
    "href": "06-resample.html#r-functions",
    "title": "6¬† Resampling, Jackknife and Bootstrap",
    "section": "6.11 R Functions",
    "text": "6.11 R Functions\nSome useful R functions for resampling methods:\n\nsample(): Generate random samples from a specified set of data points.\nboot::boot(): Perform bootstrap resampling and compute statistics.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "other-topics.html",
    "href": "other-topics.html",
    "title": "7¬† Additional Topics",
    "section": "",
    "text": "7.1 High-dimensional data\nThis chapter covers additional topics that will only be going over if time permits.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Topics</span>"
    ]
  },
  {
    "objectID": "other-topics.html#dimensional-reduction-methods",
    "href": "other-topics.html#dimensional-reduction-methods",
    "title": "7¬† Additional Topics",
    "section": "7.2 Dimensional Reduction Methods",
    "text": "7.2 Dimensional Reduction Methods\n\n7.2.1 Principal Component Analysis",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Additional Topics</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html",
    "href": "App_A-intro-R.html",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "",
    "text": "8.1 R\nFor conducting analyses with data sets of hundreds to thousands of observations, calculating by hand is not feasible and you will need a statistical software. R is one of those. R can also be thought of as a high-level programming language. In fact, R is one of the top languages to be used by data analysts and data scientists. There are a lot of analysis packages in R that are currently developed and maintained by researchers around the world to deal with different data problems. Most importantly, R is free! In this section, we will learn how to use R to conduct basic statistical analyses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#ide",
    "href": "App_A-intro-R.html#ide",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.2 IDE",
    "text": "8.2 IDE\n\n8.2.1 Rstudio\nRStudio is an integrated development environment (IDE) designed specifically for working with the R programming language. It provides a user-friendly interface that includes a source editor, console, environment pane, and tools for plotting, debugging, version control, and package management. RStudio supports both R and Python and is widely used for data analysis, statistical modeling, and reproducible research. It also integrates seamlessly with tools like R Markdown, Shiny, and Quarto, making it popular among data scientists, statisticians, and educators.\n\n\n8.2.2 Visual Studio Code (VS Code)\nVS Code is a versatile code editor that supports multiple programming languages, including R. With the R extension for VS Code, users can write and execute R code, access R‚Äôs console, and utilize features like syntax highlighting, code completion, and debugging. While not as specialized as RStudio for R development, VS Code offers a lightweight alternative with extensive customization options and support for various programming tasks.\n\n\n8.2.3 Positron\nPositron IDE is the next-generation integrated development environment developed by Posit, the company behind RStudio. Designed to be a modern, extensible, and language-agnostic IDE, Positron builds on the strengths of RStudio while supporting a broader range of languages and workflows, including R, Python, and Quarto.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#rstudio-layout",
    "href": "App_A-intro-R.html#rstudio-layout",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.3 RStudio Layout",
    "text": "8.3 RStudio Layout\nRStudio consists of several panes: - Source: Where you write scripts and markdown documents. - Console: Where you type and execute R commands. - Environment/History: Shows your variables and command history. - Files/Plots/Packages/Help/Viewer: For file management, viewing plots, managing packages, accessing help, and viewing web content.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-scripts",
    "href": "App_A-intro-R.html#r-scripts",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.4 R Scripts",
    "text": "8.4 R Scripts\nR scripts are plain text files containing R code. You can create a new script in RStudio by clicking File &gt; New File &gt; R Script.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-help",
    "href": "App_A-intro-R.html#r-help",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.5 R Help",
    "text": "8.5 R Help\nUse ?function_name or help(function_name) to access help for any R function. For example:\n?mean\nhelp(mean)",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-packages",
    "href": "App_A-intro-R.html#r-packages",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.6 R Packages",
    "text": "8.6 R Packages\nPackages extend R‚Äôs functionality. There are thousands of packages available in R ecosystem. You may install them from different sources.\n\n8.6.1 With Comprehensive R Archive Network (CRAN)\nCRAN is the primary repository for R packages. It contains thousands of packages that can be easily installed and updated.\nInstall a package with:\ninstall.packages(\"package_name\")\n\n\n8.6.2 With Bioconductor\nBioconductor is a repository for bioinformatics packages in R. It provides tools for the analysis and comprehension of high-throughput genomic data.\nInstall Bioconductor packages using the BiocManager package:\nBiocManager::install(\"package_name\")\n\n\n8.6.3 From GitHub\nMany of the authors of R packages host their work on GitHub. You can install these packages using the devtools package:\ndevtools::install_github(\"username/package_name\")\n\n\n8.6.4 Load a package\nOnce a package is installed, you need to load it into your R session to use its functions:\nlibrary(package_name)\nAlternatively, you may use a function in the package with package_name::function_name() without loading the entire package.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#r-markdown",
    "href": "App_A-intro-R.html#r-markdown",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.7 R Markdown",
    "text": "8.7 R Markdown\nR Markdown allows you to combine text, code, and output in a single document. Create a new R Markdown file in RStudio via File &gt; New File &gt; R Markdown....\nRecently, the posit team has developed a new version of the R Markdown called quarto document, with the file extension .qmd. It is still under rapid development.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#vectors",
    "href": "App_A-intro-R.html#vectors",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.8 Vectors",
    "text": "8.8 Vectors\nVectors are the most basic data structure in R.\n\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\n\nYou can perform operations on vectors:\n\nx * 2\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_A-intro-R.html#data-sets",
    "href": "App_A-intro-R.html#data-sets",
    "title": "8¬† Appendix: Introduction to R?",
    "section": "8.9 Data Sets",
    "text": "8.9 Data Sets\nData frames are used for storing data tables. Create a data frame:\n\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Score = c(90, 85))\ndf\n\n   Name Score\n1 Alice    90\n2   Bob    85\n\n\nYou can import data from files using read.csv() or read.table().\n\nThis appendix is adapted from Why R?.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "App_B-distribution.html",
    "href": "App_B-distribution.html",
    "title": "9¬† Appendix: Distributions",
    "section": "",
    "text": "9.1 Discrete Distributions",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Appendix: Distributions</span>"
    ]
  },
  {
    "objectID": "App_B-distribution.html#discrete-distributions",
    "href": "App_B-distribution.html#discrete-distributions",
    "title": "9¬† Appendix: Distributions",
    "section": "",
    "text": "9.1.1 Discrete uniform distributions\nif the variable can take on finite, countable values with equal probability, it is said to be discrete uniform. The probability mass function of a discrete uniform. random variable \\(X\\) is given by \\[\nf(x) = \\frac{1}{x_n-x_1+1},\\quad \\text{for}\\quad x=x_1,\\dots,x_n.\n\\]\n\nThe mean and variance of the discrete uniform distribution are \\[\n\\mathbb{E}X = \\frac{x_1+x_n}{2},\\quad \\text{and}\\quad \\mathbb{V}ar(X) = \\frac{(x_n-x_1+1)^2 -1}{12}.\\]\n\n\n9.1.1.1 Implementation in R\nThere is no buildt-in function t osimlate from the discrete uniform distributions. Hence, we need to download additional packages.\n\npacman::p_load(extraDistr)\nlibrary(extraDistr)\n# Generate random sample from Uniform(0, 10)\nA &lt;- rdunif(10000, 0, 10) \n\n# Histogram of the sample\nhist(A)\n\n\n\n\n\n\n\n# Sample mean and variance\nmean(A)\n\n[1] 5.01\n\nvar(A)\n\n[1] 9.875088\n\n# Theoretical mean of Uniform(a, b) is (a + b)/2\n(mean.est &lt;- (min(A) + max(A)) / 2)\n\n[1] 5\n\n# Theoretical variance of Uniform(a, b) is (b - a)^2 / 12\n(var.est &lt;- ((max(A) - min(A) + 1 )^2 -1 ) / 12)\n\n[1] 10\n\n\n\n\n\n9.1.2 Bernoulli Distribution\nThere are only 2 possible outcomes in an experiment: True and False, with probabiltiy \\(\\theta\\) and \\(1-\\theta\\), respectively, the random variable \\(X\\) follows a Bernoulli distribution, denoted by \\(X\\sim \\text{Bern}(\\theta)\\). The probabiltiy distribution has probablity mass function as \\[f(x) = \\theta^x (1-\\theta)^{1-x},\\quad x=0,1.\\]\nNote that Bernoulli distribution is a special case of the Binomial distribution with \\(n=1\\).\n\n9.1.2.1 Implementation in R\nrbinom(10, 1, 0.7) # 0.7 is the probability of success\n\n\n\n9.1.3 Binomial Distribution\nBernoulli trials (experiment where a Bernoulli Distribution applies) are not very realistic in real-life scenario. In statistics, repeated experiments are important. When the experiments is repeated, \\(\\theta\\) is the same for each of the trials, and the trials are independent, and they are only two mutually excludes outcomes (T, and F), we can model this using the Binomial Distribution. If \\(X\\) follows the Binomial Distribution, it has a probablitliy mass function as \\[b(x ; n, \\theta)=\\binom{n}{x} \\theta^x(1-\\theta)^{n-x}, x=0,1,2,3, \\ldots, n.\\]\nThe Binomial Distribution is useful to predict the number of heads in \\(n\\) coin tosses, the number of people infected with a disease in a certain population of known size, etc. The parameters \\(n\\) and \\(\\theta\\) must be given.\n\nThe mean and variance of the Binomial Distribution \\[\n\\mathbb{E}X = n\\theta,\\quad \\text{and}\\quad \\mathbb{V}ar(X) = n\\theta(1-\\theta).\\]\n\n\n9.1.3.1 Implementation in R\n\nX = rbinom(10000, 4, prob = 0.3)\nmean(X == 1)\n\n[1] 0.4096\n\nmean(X &gt; 1)\n\n[1] 0.3435\n\n\n\n\n\n9.1.4 Geometric Distribution\nIf we are interested in the number of trials until the first success, then this is modeled by the geometric distribution. A random variable \\(X\\) follows a geometric distribution if and only if its probability distribution is given by: \\[\ng(x;\\theta) = \\theta (1 - \\theta)^{\\,x-1}, \\quad x = 1,2,3,\\ldots\n\\]\n\n# Function to generate n samples from Geometric(p)\ngeom.gener &lt;- function(n, p) {\n  tmp &lt;- NULL\n  for (i in 1:n) {\n    u &lt;- runif(1)  # Uniform(0,1)\n    x &lt;- 1\n    p.x &lt;- p\n    sum &lt;- p.x\n    \n    while (sum &lt; u) {\n      x &lt;- x + 1\n      p.x &lt;- p.x * (1 - p)\n      sum &lt;- sum + p.x\n    }\n    \n    tmp &lt;- c(tmp, x)\n  }\n  return(tmp)\n}\n\n# Example: generate 100,000 samples with p = 0.75\nx &lt;- geom.gener(100000, 0.75)\n\n# Relative frequencies\ntable(x) / 100000\n\nx\n      1       2       3       4       5       6       7       8       9 \n0.74967 0.18814 0.04681 0.01153 0.00297 0.00066 0.00016 0.00004 0.00002 \n\n# OR \nX = rgeom(100000, prob= 0.75)\ntable(X)/100000\n\nX\n      0       1       2       3       4       5       6       7 \n0.75008 0.18631 0.04797 0.01189 0.00286 0.00068 0.00018 0.00003 \n\n\nDo you notice the difference between the two functions? R starts the geometric distribution at \\(x = 0\\) and not \\(x = 1\\). We can check that both of these simulate the same distribution by checking a qqplot.\n\n# Correlation between points in QQ plot\ncor(qqplot(x, X + 1)$x, qqplot(x, X + 1)$y)\n\n\n\n\n\n\n\n\n[1] 0.9974553\n\n\n\n\n9.1.5 Hypergeometric Distribution\nThe motivating question: what happens in a Binomial setting when our trials are NOT independent? In other words, what happens when we sample without replacement?\nConsider a set of N elements, of which M are successes. We are interested in obtaining X successes in n trials. This situation is modeled by the Hypergeometric Distribution, which has pdf:\n\\[h(x,n,M,N) = \\frac{\\binom{M}{x}\\binom{N-M}{n-x}}{\\binom{N}{n}}, \\quad x = 0,1,2,\\ldots,n\\]\n\nThe mean and the variance are \\[\\mathbb{E}X = \\frac{nM}{N}, \\qquad\n\\mathbb{V}ar(X) = \\frac{nM(N-M)(N-n)}{N^2(N-1)}.\\]\n\n\n9.1.5.1 Implementation in R\n\n# Define population: 12 ones and 13 zeros\nx &lt;- c(1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)\n\n# Take one sample of size 8 without replacement\ns1 &lt;- sample(x, 8, replace = FALSE)\ns1\n\n[1] 1 1 0 0 0 0 0 1\n\n# Count how many ones in that sample\nsum(s1 == 1)\n\n[1] 3\n\n# Repeat the sampling 10,000 times\ny &lt;- replicate(10000, sample(x, 8, replace = FALSE))\n\n# Column sums = number of ones in each sample\nz &lt;- colSums(y)\n\n# Histogram of simulated distribution\nhist(z)\n\n\n\n\n\n\n\n# Probability of getting exactly 5 ones\nmean(z == 5)\n\n[1] 0.1044\n\n# Sample mean and variance of distribution\nmean(z)\n\n[1] 3.1976\n\nvar(z)\n\n[1] 1.365891\n\n\n\n\n\n9.1.6 Poisson Distribution\nCalculating Binomial probabilities when \\(n\\) is large can be highly tedious and time-consuming. As \\(n\\) approaches infinity and the probability of success approaches \\(0\\), where \\(n\\theta\\) remains fixed. We can define \\(n\\theta = \\lambda\\), and obtain the distribution called Poisson.\nA random variable X has the Poisson Distribution if and only if its probability distribution is given by:\n\\[P(x;\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0,1,2,\\ldots\\]\n\n# Generate 10,000 samples from Poisson(Œª = 2)\nX &lt;- rpois(10000, 2)\n\n# Histogram of simulated data\nhist(X)\n\n\n\n\n\n\n\n# Probability that X &gt; 3 (estimated by simulation)\nmean(X &gt; 3)\n\n[1] 0.1351\n\n# Probability that X = 0 (estimated by simulation)\nmean(X == 0)\n\n[1] 0.1354\n\n# Theoretical probability that X = 0\nexp(-2)\n\n[1] 0.1353353\n\n# Sample mean and variance of simulated distribution\nmean(X)\n\n[1] 1.9828\n\nvar(X)\n\n[1] 1.9549\n\n\n\nThe mean and variance of the Poisson distribution are\n\\[\\mathbb{E}X = \\lambda, \\qquad \\mathbb{V}ar(X) = \\lambda\\]\n\nThe Poisson distribution is derived as the limiting case of the Binomial (with the above mentioned restrictions) BUT there are many more applications. It models:\n\nNumber of successes to occur in a given time period\nNumber of telephone calls received in a given time\nNumber of misprints on a page\nNumber of customers entering a bank during various intervals of time\n\n‚∏ª\nWe are now moving onto the continuous distributions that play an important role in Statistical Theory.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Appendix: Distributions</span>"
    ]
  },
  {
    "objectID": "App_B-distribution.html#continuous-distributions",
    "href": "App_B-distribution.html#continuous-distributions",
    "title": "9¬† Appendix: Distributions",
    "section": "9.2 Continuous Distributions",
    "text": "9.2 Continuous Distributions\n\n9.2.1 Uniform Distribution\nSimilar to the discrete uniform distribution except all values within an interval have equal probability. The parameters of the Uniform Density are and (&lt; ). The random variable X has the Uniform Distribution if it is continuous and its probability density function is given by \\[\nf_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\beta - \\alpha}, & \\alpha &lt; x &lt; \\beta, \\\\[1ex]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\n\nThe mean and variance of the Uniform Distribution are \\[\\mathbb{E}X = \\frac{\\alpha + \\beta}{2},\n\\qquad\n\\mathbb{V}ar(X) = \\frac{(\\beta - \\alpha)^2}{12}.\n\\]\n\n\n9.2.1.1 Implementation in R\n\nX &lt;- runif(1000)\n\n# Histogram of simulated data\nhist(X)\n\n\n\n\n\n\n\nmean(X);  var(X)\n\n[1] 0.4993008\n\n\n[1] 0.08124158\n\n\n\n\n\n9.2.2 Gamma Distribution\nThe Gamma function is defined as \\[\\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha - 1} e^{-y} \\, dy.\\]\n\nFor \\(\\alpha &gt; 0\\), \\[\n\\Gamma(\\alpha) = (\\alpha - 1)\\Gamma(\\alpha - 1).\\]\n\n\nFor any positive integer \\(\\alpha &gt; 0\\), \\[\\Gamma(\\alpha) = (\\alpha - 1)!\\]\n\nA continuous random variable follows a Gamma Distribution if and only if its probability density function is of the form \\[f_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-x/\\beta}, & x &gt; 0, \\\\[2ex]\n0, & \\text{otherwise},\n\\end{cases}\\] where \\(\\alpha, \\beta\\) are positive. The parameters \\(\\alpha\\) and \\(\\beta\\) determine the shape of the distribution. \\(\\alpha\\) is called the shape parameter and \\(\\beta\\) is called the scale parameter.\n\nTheorem: The mean and variance of the Gamma Distribution are \\[\\mathbb{E}X = \\alpha \\beta,\n\\qquad\n\\mathbb{V}ar(X) = \\alpha \\beta^2\\]\n\n\n9.2.2.1 Implementation in R\nrgamma(n = 10, shape = 10, scale = 3)\n\n\n\n9.2.3 Exponential Distribution\nA special case of the Gamma Distribution arises when \\(\\alpha = 1\\). To differentiate it from the Gamma distribution, we also let \\(\\beta = \\theta\\). A continuous random variable follows an Exponential Distribution if and only if its probability density function is of the form: \\[f_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\theta} e^{-x/\\theta}, & x &gt; 0, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}\\]\nExponential Distributions have many applications. One of them is a waiting time until the first success of a Poisson process. In this situation, it is often better to model the phenomenon in terms of a rate parameter (i.e.¬†4 calls per week). Thus, the distribution of waiting times becomes: \\[\nf_Y(y) =\n\\begin{cases}\n\\lambda e^{-\\lambda y}, & y &gt; 0, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}\\]\n\n9.2.3.1 Memoryless Property\nThis is also called the Markov Property. The distribution of waiting time does not depend on how long you have already waited. In other words: \\[\nP(X &gt; s + t \\mid X &gt; t) = P(X &gt; s).\n\\]\n\nThe mean and variance of the exponential distribution are \\[\\mathbb{E}X = \\theta,\n\\qquad\n\\mathbb{V}ar(X) = \\theta^2.\\]\n\n\n\n9.2.3.2 Implementation in R\n\n# Generate 1000 samples from Exponential(Œª = 1)\nX &lt;- rexp(1000)\nhist(X)\n\n\n\n\n\n\n\nmean(X); var(X)\n\n[1] 1.023989\n\n\n[1] 1.07127\n\n\n\n\n\n9.2.4 Chi-Square Distribution\nThere is another special form of the gamma distribution when \\(\\alpha =\\nu/2\\) and \\(\\beta = 2.\n\\nu\\) is pronounced ‚Äúnu‚Äù and is called the degrees of freedom.\nA continuous random variable follows a Chi-Square Distribution if and only if its probability density is given by \\[f_X(x) =\n\\begin{cases}\n\\dfrac{1}{2^{\\nu/2}\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right)} x^{\\tfrac{\\nu}{2} - 1} e^{-x/2}, & x &gt; 0, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}\\]\n\nThe mean and variance of the Chi-Square Distribution are\n\\[\\mathbb{E}X = \\nu,\n\\qquad\n\\mathbb{V}ar(X) = 2\\nu.\\]\n\n\nX &lt;- rchisq(1000, df = 5)\n\nhist(X)\n\n\n\n\n\n\n\nmean(X);  var(X)\n\n[1] 4.987528\n\n\n[1] 9.941394\n\n\n\n\n9.2.5 Beta Distribution\nThis is Not a special case of the Gamma distribution\nThe Beta function is defined as \\[\nB(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n= \\int_0^1 x^{\\alpha - 1} (1-x)^{\\beta - 1} \\, dx.\\]\nLike any probability density, the area underneath it must be equal to 1. So, we can rearrange the above definition and write: \\[1 = \\int_0^1 \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\nx^{\\alpha-1} (1-x)^{\\beta-1} \\, dx.\\]\nA continuous random variable follows a Beta Distribution if and only if its probability density function is given by \\[\nf_X(x) =\n\\begin{cases}\n\\dfrac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\nx^{\\alpha-1}(1-x)^{\\beta-1}, & 0 &lt; x &lt; 1, \\\\[2ex]\n0, & \\text{otherwise},\n\\end{cases}\\] where \\(\\alpha &gt; 0\\), \\(\\beta &gt; 0\\).\n\nThe mean and variance of the Beta Distribution are \\[\\mathbb{E}X = \\frac{\\alpha}{\\alpha + \\beta},\n\\qquad\n\\mathbb{V}ar(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}.\n\\]\n\n9.2.5.1 Implementation in R\n\nlibrary(ggplot2)\n\n# Sequence of x values\nt &lt;- seq(0, 1, by = 0.01)\n\n# Create data frame with multiple Beta densities\ndf &lt;- data.frame(\n  x = rep(t, 4),\n  density = c(dbeta(t, 2, 2),\n              dbeta(t, 2, 8),\n              dbeta(t, 8, 2),\n              dbeta(t, 1, 1)),\n  dist = factor(rep(c(\"Beta(2,2)\", \"Beta(2,8)\", \"Beta(8,2)\", \"Beta(1,1)\"),\n                    each = length(t)))\n)\n\n# Plot with ggplot\nggplot(df, aes(x = x, y = density, color = dist)) +\n  geom_line(size = 1) +\n  labs(x = \"X\", y = \"Beta Density\", title = \"Beta Distributions\") +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nQ: What do you noitce about \\(B(1,1)\\)? What distribution is this?\nThe Beta distribution is related to the Binomial distribution when computing maximum likelihood estimators !(We will make use of this property later when we do Bayesian analysis.) \\[\\text{Beta}_{pdf}(p, n, k) = (n+1)\\binom{n}{k} p^k (1-p)^{n-k},\\] where \\(p^\\prime = \\text{Binomial}_{pmf}(k,n,p), \\quad k = \\text{mode}(\\text{Binomial}(n,p)).\\)\nTo relate the binomial distribution to the scale and shape parameters of the beta distribution: \\(\\alpha = k + 1, \\quad \\beta = n - \\alpha + 2\\).\n\n\n9.2.5.2 Implementation in R\nSuppose we flip a coin 20 times and find that we have 8 heads. Thus, our MLE is \\(8/20 = 0.4\\). We can visualize the likelihood function for this scenario:\n\n# Likelihood plot using Binomial likelihood\nlikeli_bino.plot &lt;- function(y, n) {\n  L &lt;- function(p) dbinom(y, n, p)\n  mle &lt;- optimize(L, interval = c(0, 1), maximum = TRUE)$max\n  \n  p &lt;- (1:100) / 100\n  \n  # Likelihood\n  plot(p, L(p), type = \"l\")\n  abline(v = mle)\n  \n  # Log-likelihood\n  plot(p, log(L(p)), type = \"l\", main = \"binomial\")\n  abline(v = mle)\n}\npar(mfrow = c(1,1), mar = c(4,4,2,1))\nlikeli_bino.plot(8, 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Likelihood plot using Beta distribution\nlikeli_beta.plot &lt;- function(y, n) {\n  L &lt;- function(p) dbeta(p, y + 1, n - (y + 1) + 2)\n  mle &lt;- optimize(L, interval = c(0, 1), maximum = TRUE)$max\n  \n  p &lt;- (1:100) / 100\n  \n  # Likelihood\n  plot(p, L(p), type = \"l\")\n  abline(v = mle)\n  \n  # Log-likelihood\n  plot(p, log(L(p)), type = \"l\", main = \"Beta\")\n  abline(v = mle)\n  \n  mle\n}\nlikeli_beta.plot(8, 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.3999996\n\noverlay.likeli &lt;- function(y, n) {\n  # Binomial likelihood\n  L_binom &lt;- function(p) dbinom(y, n, p)\n  \n  # Beta likelihood (with Œ± = y+1, Œ≤ = n-y+1)\n  L_beta &lt;- function(p) dbeta(p, y + 1, n - y + 1)\n  \n  # Sequence of p values\n  p &lt;- seq(0, 1, length.out = 200)\n  \n  # Scale the beta likelihood so it‚Äôs comparable\n  scale_factor &lt;- max(L_binom(p)) / max(L_beta(p))\n  par(mfrow=c(1,1))\n  # Plot Binomial likelihood\n  plot(p, L_binom(p), type = \"l\", col = \"blue\", lwd = 2,\n       ylab = \"Likelihood\", xlab = \"p\",\n       main = \"Binomial vs Beta Likelihood\")\n  \n  # Add Beta likelihood (scaled for comparison)\n  lines(p, L_beta(p) * scale_factor, col = \"red\", lwd = 2, lty = 2)\n  \n  # Add legend\n  legend(\"topright\",\n         legend = c(\"Binomial Likelihood\", \"Beta Likelihood (scaled)\"),\n         col = c(\"blue\", \"red\"),\n         lty = c(1, 2), lwd = 2)\n}\n\n# Example: 8 successes out of 20 trials\n\noverlay.likeli(8, 20)\n\n\n\n\n\n\n\n\n\n\n9.2.6 Gaussian Distribution\nThis is probably the most famous statistical distribution. It is defined by its mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)). It is also known as the Normal Distribution. A continuous random variable follows a Normal Distribution if and only if its probability density function is given by\n\\[\nf_X(x) =\n\\begin{cases}\n\\dfrac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left( -\\dfrac{(x - \\mu)^2}{2\\sigma^2} \\right), & -\\infty &lt; x &lt; \\infty, \\\\[2ex]\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nNote: The Standard Normal Distribution has density with \\(\\mathbb{E}X = 0\\) and \\(\\mathbb{V}ar(X) = 1\\).\n\nX &lt;- rnorm(1000, 0, 1)\nqqnorm(X)\n\n\n\n\n\n\n\n# Approximate probabilities of being within 1, 2, 3 standard deviations\nmean(-1 &lt; X & X &lt; 1)  # ~ 68%\n\n[1] 0.684\n\nmean(-2 &lt; X & X &lt; 2)  # ~ 95%\n\n[1] 0.961\n\nmean(-3 &lt; X & X &lt; 3)  # ~ 99.7%\n\n[1] 1\n\n\n\n\n9.2.7 T-Distribution\nAnother special distribution in statistical inference is the Student‚Äôs t-Distribution. \\[f_X(x) =\n\\begin{cases}\n\\dfrac{\\Gamma\\!\\left(\\tfrac{\\nu+1}{2}\\right)}\n{\\sqrt{\\nu \\pi}\\,\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right)}\n\\left(1 + \\dfrac{x^2}{\\nu}\\right)^{-\\tfrac{\\nu+1}{2}}, & -\\infty &lt; x &lt; \\infty, \\\\[2ex]\n0, & \\text{otherwise},\n\\end{cases}\\] where \\(\\nu\\) is the degrees of freedom. As \\(\\nu \\to \\infty\\), the pdf converges to the normal distribution.\n\n\n9.2.8 Implementation in R\n\nmy_df &lt;- 5\nX &lt;- rt(1000, df = my_df)\n\nmean(X); var(X)\n\n[1] 0.05605437\n\n\n[1] 1.793322\n\nmy_df / (my_df - 2)   # Theoretical variance for df &gt; 2\n\n[1] 1.666667\n\n\n\n\n9.2.9 Distribution Function Technique\nFor continuous random variables, a simple method for finding the probability density of a function of random variables is to find the distribution function and then differentiate to find the pdf.\nTo find an expression for the distribution function, let \\(Y = u(x_1, x_2, \\ldots, x_n)\\), where \\(u\\) is a function. Then, \\[F(Y) = P(Y \\leq y) = P(u(x_1, x_2, \\ldots, x_n) \\leq y).\\] Then, \\[f(y) = \\frac{dF(Y)}{dy}.\\]\nExample:\n\n# Create sequence from 0 to 1\nt &lt;- seq(0, 1, length = 1000)\n\n# Define density function Y = 3 * (1 - t)^2\nY &lt;- 3 * (1 - t)^2\n\n# Plot density\nplot(t, Y, type = \"l\")\n\n\n\n\n\n\n\n# Sample from t with probability weights Y\nZ &lt;- sample(t, 10000, replace = TRUE, prob = Y)\n\n# Histogram of sampled values\nhist(Z, freq = FALSE)\n\n# Overlay the density curve\nlines(t, Y)\n\n\n\n\n\n\n\n# Apply transformation: X = (1 - Z)^3\nX.sample &lt;- (1 - Z)^3\n\n# Histogram of transformed sample\nhist(X.sample)\n\n\n\n\n\n\n\n# Compare with uniform distribution\nU &lt;- runif(10000)\n\n# QQ-plot to check distributional similarity\nqqplot(U, X.sample)\n\n\n\n\n\n\n\n# Correlation from QQ-plot\ncor(qqplot(U, X.sample)$x, qqplot(U, X.sample)$y)\n\n[1] 0.9998769\n\n\n\nSpecial thanks to Dr.¬†Brian Pidgeon who kindly share the notes.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Appendix: Distributions</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html",
    "href": "App_C_Rpackage.html",
    "title": "10¬† Appendix: Project & R Package",
    "section": "",
    "text": "10.1 Group",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#group",
    "href": "App_C_Rpackage.html#group",
    "title": "10¬† Appendix: Project & R Package",
    "section": "",
    "text": "Group\nMember 1\nMember 2\nTopic\n\n\n\n\n1\nAkomanyi-Addo, Allswell\nSarpong, Felix\nTBA\n\n\n2\nZhang, Zilong\n\nTBA\n\n\n3\nStell, Tommy\n\nTBA\n\n\n4\nVu, Ha\n\nTBA\n\n\n5\nZhang, Boshu\nYou, Sifan\nTBA\n\n\n6\nZhang, Yalin\n\nTBA\n\n\n7\nRehman, Muhammad Aimal\nLu, Zhili\nTBA\n\n\n8\nCreary, Maya\nAlvarado, Alexus\nTBA\n\n\n9\nShamshad, Azka\nKhadka, Sushil\nTBA\n\n\n10\nWu, Yunxue\n\nTBA",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#format",
    "href": "App_C_Rpackage.html#format",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.2 Format",
    "text": "10.2 Format\n\n\n\n\n\n\nNoteFinal Project ‚Äî Fall 2025\n\n\n\n\nDue: To be Annouced\nPoints: [20+10] points\nThis version: October 16, 2025",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#overview",
    "href": "App_C_Rpackage.html#overview",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.3 Overview",
    "text": "10.3 Overview\nThe final project is a key component of this course, designed to integrate computational thinking, statistical theory, and reproducible research practices. Your goal is to develop an R package that implements a computational or statistical method, document it, and present your work both in a written report and an oral presentation.\nThis project will give you hands-on experience with:\n\nImplementing statistical algorithms efficiently in R,\nPackaging reproducible code and documentation,\nWriting professional scientific reports,\nCommunicating technical results effectively.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#project-components",
    "href": "App_C_Rpackage.html#project-components",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.4 Project Components",
    "text": "10.4 Project Components\nYour submission will consist of three main parts.\n\n10.4.1 1. R Package\nYou will create an R package that provides a computational or statistical tool. The package should:\n\nContain well-documented functions using roxygen2,\nInclude example data (if appropriate),\nProvide at least one working vignette demonstrating use cases,\nInclude meaningful unit tests (e.g., using testthat),\nPass R CMD check without errors.\n\nPossible directions include:\n\nImplementation of a Monte Carlo or MCMC algorithm,\nOther numerical optimization algorithms or integration routines,\nResampling methods (bootstrap, permutation tests),\nDimensionality reduction or regression algorithms,\nSimulation frameworks for a statistical model.\nOther things you find interesting. Come and talk to me before you start.\n\n\n\n10.4.2 2. Written Report\nThe report 8-10 pages should be written in LaTeX and may include but not limited to:\n\nMotivation and background of your method or problem,\nDescription of algorithmic implementation,\nSimulation or data analysis examples,\nDiscussion of computational efficiency and accuracy,\nFuture extensions or open problems.\n\nTry to look at the R Journal articles for inspiration on structure and style.\n\n\n10.4.3 3. Presentation\nYou will give a presentation [Length TBA after everyone finds a team] during the class sessions. Your presentation should:\n\nSummarize the motivation, method, and results,\nInclude a live demo or code walkthrough (optional but encouraged),\nBe clear, concise, and visually well-prepared.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#collaboration-policy",
    "href": "App_C_Rpackage.html#collaboration-policy",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.5 Collaboration Policy",
    "text": "10.5 Collaboration Policy\nYou may discuss ideas and coding techniques with classmates, but the final package and report must be your own work. All code and text should be written individually or with your groupmate if you decide to team up.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#submission-instructions",
    "href": "App_C_Rpackage.html#submission-instructions",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.6 Submission Instructions",
    "text": "10.6 Submission Instructions\n\nSubmit your package via a public GitHub repository link.\nUpload your report (PDF) and slides to the course submission portal.\nInclude installation instructions in the README.md.\n\n\n\n\n\n\n\nTip\n\n\n\nTip: Start early ‚Äî the package structure, documentation, and testing usually take longer than expected!",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#some-useful-functions",
    "href": "App_C_Rpackage.html#some-useful-functions",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.7 Some useful functions",
    "text": "10.7 Some useful functions\n\n10.7.1 Create\n\ndevtools::create() function is used to create the basic structure of an R package. It sets up the necessary directories and files for package development.\n\n\n\n10.7.2 Build\n\ndevtools::build() function is used to build the R package into a distributable format (e.g., a tar.gz file). This function compiles the package and prepares it for installation.\n\n\n\n10.7.3 Documentation\n\nroxygen2::roxygenise() function is used to generate documentation for R packages. It processes specially formatted comments in your R code and creates the necessary documentation files.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "App_C_Rpackage.html#good-readings",
    "href": "App_C_Rpackage.html#good-readings",
    "title": "10¬† Appendix: Project & R Package",
    "section": "10.8 Good readings",
    "text": "10.8 Good readings\nAnother good reference in writing a Rpackage written by Fong Chun Chan.\n\nAn article by Posit (formerly Rstudio) LINK\nAnother one by the MIT folks LINK\n\nResources\n\nCRAN page\nWickham, H. and Bryan, J. R Package.\nWriting R Extensions (CRAN Manual)\nR Markdown Cookbook\nAdvanced R",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Appendix: Project & R Package</span>"
    ]
  },
  {
    "objectID": "APP_D_integration.html",
    "href": "APP_D_integration.html",
    "title": "11¬† Appendix: Numerical integration",
    "section": "",
    "text": "11.1 Motivation and usage\nIntegration is a fundamental concept in statistics, for instance, when calculating the variance, we have \\[\n  \\mathbb EX = \\int_{x\\in\\mathcal X} x dF_X(x),\\quad \\mathbb E_{x\\in\\mathcal X} g(X) = \\int g(x) dF_X(x).\n\\] If density exists, we have \\[\n\\mathbb EX = \\int_{x\\in\\mathcal X} x f_X(x) dx,\\quad \\mathbb Eg(X) = \\int_{x\\in\\mathcal X} g(x) f_X(x) dx.\n\\] Other than calculating for the moments, integration also plays a big parts in other parts in the field of statistics and data science.\n\\[\n  p(x) = \\int_{\\theta\\in\\Theta}     p(x|\\theta)p(\\theta)\\,d\\theta,\n\\] where the integration is over the nuisance/latent parameter \\(\\theta\\in\\Theta\\).\nThe Bayes‚Äô theorem involves integration in both the denominator and posterior expectations:\n\\[\np(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{\\int p(x|\\theta)p(\\theta)\\,d\\theta}\n\\]\nWhen you want to calculate the inner product of the two observations \\(x,y\\in L^2(\\mathcal X)\\), you need to calculate the integral\n\\[\n\\langle f,g \\rangle = \\int f(t)g(t)\\,dt.\n\\]\nIn machine learning, you often need to compute integrals in many tasks, including calculating the risks and loss.\nFor example, in supervised learning, we have\n\\[R(\\theta) := \\mathbb E{(X,Y) \\sim P}[L(f_\\theta(X), Y)],\n\\] where \\(L(\\cdot,\\cdot)\\) is a loss function (e.g., squared error, cross-entropy).\nIn variational inference (VI) solves this by approximating the true posterior \\(p(\\theta|x)\\) with a simpler distribution \\(q_\\phi(\\theta)\\). We then maximize the Evidence Lower Bound (ELBO):\n\\[\n\\log p(x) \\geq \\mathbb{E}{q\\phi(\\theta)}\\big[\\log p(x|\\theta)\\big] - \\text{KL}(q_\\phi(\\theta)\\,\\|\\,p(\\theta)).\n\\] In this formula, we have two integrals to solve:\n\\[\\int q_\\phi(\\theta)\\,\\log p(x|\\theta)\\, d\\theta,\\] which is often approximated by Monte Carlo sampling.\n\\[\\text{KL}(q \\| p) = \\int q_\\phi(\\theta) \\log \\frac{q_\\phi(\\theta)}{p(\\theta)} \\, d\\theta.\\]",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Appendix: Numerical integration</span>"
    ]
  },
  {
    "objectID": "APP_D_integration.html#motivation-and-usage",
    "href": "APP_D_integration.html#motivation-and-usage",
    "title": "11¬† Appendix: Numerical integration",
    "section": "",
    "text": "Marginal likelihood and Bayesian statistics:\n\n\n\nBayesian Statistics\n\n\n\n\nFunctional observations\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\nThe 1st term is an integral:\n\n\n\nThe 2nd term, the KL divergence, is another integral:",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Appendix: Numerical integration</span>"
    ]
  },
  {
    "objectID": "APP_D_integration.html#section",
    "href": "APP_D_integration.html#section",
    "title": "11¬† Appendix: Numerical integration",
    "section": "11.2 ",
    "text": "11.2",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Appendix: Numerical integration</span>"
    ]
  },
  {
    "objectID": "APP_D_integration.html#multivariate-case",
    "href": "APP_D_integration.html#multivariate-case",
    "title": "11¬† Appendix: Numerical integration",
    "section": "11.3 Multivariate Case",
    "text": "11.3 Multivariate Case\nWe need to calculate the gradient/Jacobian matrix and Hessian matrix.\n\n11.3.1 EM Algorithm\nThe EM (Expectation‚ÄìMaximization) algorithm is an optimization method that is often applied to find maximum likelihood estimates when data is incomplete or has missing values. It iteratively refines estimates of parameters by alternating between (1) expectation step (E-step) and (2) maximization step (M-step).",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Appendix: Numerical integration</span>"
    ]
  }
]