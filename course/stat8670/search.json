[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Description",
    "text": "Description\nTopics ins included are optimization, numerical integration, bootstrapping, cross-validation and Jackknife, density estimation, smoothing, and use of the statistical computer package of S-plus/R.\n\nPrerequisites\nMATH 4752/6752 – Mathematical Statistics II, and the ability to program in a high-level language.\n\n\nInstructor\nChi-Kuang Yeh, I am an Assistant Professor in the Department of Mathematics and Statistics, Georgia State University.\n\nOffice: Suite 1407, 25 Park Place.\nEmail: cyeh@gsu.edu.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#office-hour",
    "href": "index.html#office-hour",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Office Hour",
    "text": "Office Hour\n14:00–15:00 on Tuesday and Wednesday.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#grade-distribution",
    "href": "index.html#grade-distribution",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Grade Distribution",
    "text": "Grade Distribution\n\nAssignments: 40%\nTerm Exam 1: 15%\nTerm Exam 2: 15%\nFinal Project: 30%",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#assignment",
    "href": "index.html#assignment",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Assignment",
    "text": "Assignment\n\nAssignment 1: Due on September 12th, 2025\nAssignment 2: TBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#midterm",
    "href": "index.html#midterm",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Midterm",
    "text": "Midterm\n\nMidterm 1 on October 8, 2025\nMidterm 2 on November 12, 2025",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Final Project",
    "text": "Final Project\n\nReport: Due Date TBA\nPresentation: Due Date TBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#topics-and-corresponding-lectures",
    "href": "index.html#topics-and-corresponding-lectures",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Topics and Corresponding Lectures",
    "text": "Topics and Corresponding Lectures\nThose chapters are based on the lecture notes. This part will be updated frequently.\n\n\n\nTopic\nLecture Covered\n\n\n\n\nIntroduction to R Programming\n1–2\n\n\nNumerical Approaches and Optimization\n3–\n\n\nNumerical integration\nTBA\n\n\nJackknife\nTBA\n\n\nBootstrap\nTBA\n\n\nCross-validation\nTBA\n\n\nSmoothing\nTBA\n\n\nDensity estimation\nTBA\n\n\nMonte Carlo Methods\nTBA",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#recommended-textbooks",
    "href": "index.html#recommended-textbooks",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Recommended Textbooks",
    "text": "Recommended Textbooks\n\nGivens, G.H. and Hoeting, J.A. (2012). Computational Statistics. Wiley, New York.\nRizzo, M.L. (2007) Statistical Computing with R. CRC Press, Roca Baton.\nHothorn, T. and Everitt, B.S. (2006). A Handbook of Statistical Analyses Using R. CRC Press, Boca Raton.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#side-readings",
    "href": "index.html#side-readings",
    "title": "STAT 8670 - Computational Methods in Statistics",
    "section": "Side Readings",
    "text": "Side Readings\n\nWickham, H., Çetinkaya-Rundel, M. and Grolemund, G. (2023). R for Data Science. O’Reilly.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Data Structure and R Programming",
    "section": "",
    "text": "1.1 Data type\nData types, operators, variables\nTwo basic types of objects: (1) data & (2) functions\nday &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\nweather &lt;- c(\"Raining\", \"Sunny\", NA, \"Windy\", \"Snowing\")\ndata.frame(rbind(day, weather))\n\n             X1      X2        X3       X4      X5\nday      Monday Tuesday Wednesday Thursday  Friday\nweather Raining   Sunny      &lt;NA&gt;    Windy Snowing",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-type",
    "href": "01-intro.html#data-type",
    "title": "1  Data Structure and R Programming",
    "section": "",
    "text": "Boolean/Logical: Yes or No, Head or Tail, True or False\nIntegers: Whole numbers \\(\\mathbb{Z}\\), e.g., 1, 2, 3, -1, -2, -3\nCharacters: Text strings, e.g., “Hello”, “World.”\nFloats: Noninteger fractional numbers, e.g., \\(\\pi\\), \\(e\\).\nMissing data: NA in R, which stands for “Not Available.” It is used to represent missing or undefined values in a dataset.\n\n\n\nOther more complex types\n\n\n1.1.1 To change data type\nYou may change the data type using the following functions, but the chance is that some of the information will be missing. Do this with caution!\n\nx &lt;- pi\nprint(x)\n\n[1] 3.141593\n\nx_int &lt;- as.integer(x)\nprint(x_int)\n\n[1] 3\n\n\nSome of the conversion functions:\n\nas.integer(): Convert to integer.\nas.numeric(): Convert to numeric (float).\nas.character(): Convert to character.\nas.logical(): Convert to logical (boolean).\nas.Date(): Convert to date.\nas.factor(): Convert to factor (categorical variable).\nas.list(): Convert to list.\nas.matrix(): Convert to matrix.\nas.data.frame(): Convert to data frame.\nas.vector(): Convert to vector.\nas.complex(): Convert to complex number.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#operators",
    "href": "01-intro.html#operators",
    "title": "1  Data Structure and R Programming",
    "section": "1.2 Operators",
    "text": "1.2 Operators\n\nUnary: With only one argument. E.g., -x (negation), !x (logical negation).\nBinary: With two arguments. E.g., x + y (addition), x - y (subtraction), x * y (multiplication), x / y (division).\n\n\n1.2.1 Comparison Operator\nComparing two objects. E.g., x == y (equal), x != y (not equal), x &lt; y (less than), x &gt; y (greater than), x &lt;= y (less than or equal to), x &gt;= y (greater than or equal to).\n\n\n1.2.2 Logical Operator\nLogical operators are used to combine or manipulate logical values (TRUE or FALSE). E.g., x & y (logical AND), x | y (logical OR), !x (logical NOT).\nWe shall note that the logical operators in R are vectorized, x | y and x || y are different. The former is vectorized, while the latter is not.\nx &lt;- c(TRUE, FALSE, FALSE)\ny &lt;- c(TRUE, FALSE, FALSE)\nx | y  # [1]  TRUE FALSE FALSE\nx || y # This will return an error",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#indexing",
    "href": "01-intro.html#indexing",
    "title": "1  Data Structure and R Programming",
    "section": "1.3 Indexing",
    "text": "1.3 Indexing\nIndexing is a way to access or modify specific elements in a data structure. In R, indexing can be done using square brackets [] for vectors and matrices, or the $ operator for data frames. Note that the index starts from 0 in R, which is different from some other programming languages like Python.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#naming",
    "href": "01-intro.html#naming",
    "title": "1  Data Structure and R Programming",
    "section": "1.4 Naming",
    "text": "1.4 Naming\nIn R, you can assign names to objects using the names() function. This is useful for making your code more readable and for accessing specific elements in a data structure.\nA good practice is to use _ (underscore) to separate words in variable names, e.g., my_variable. This makes the code more readable and easier to understand.\n\n# Assign names to a vector\ntemp &lt;- c(20, 30, 27, 31, 45)\nnames(temp) &lt;- c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\")\nprint(temp)\n\n  Mon  Tues   Wed Thurs   Fri \n   20    30    27    31    45 \n\n\nrownames(temp) &lt;- \"Day1\" # error\n\ntemp_mat &lt;- matrix(c(20, 30, 27, 31, 45), nrow = 1, ncol = 5)\ncolnames(temp_mat) &lt;- c(\"Mon\", \"Tues\", \"Wed\", \"Thurs\", \"Fri\")\nrownames(temp_mat) &lt;- \"Day1\" # error\nprint(temp_mat)\n\n     Mon Tues Wed Thurs Fri\nDay1  20   30  27    31  45",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#array-and-matrix",
    "href": "01-intro.html#array-and-matrix",
    "title": "1  Data Structure and R Programming",
    "section": "1.5 Array and Matrix",
    "text": "1.5 Array and Matrix\nOne may define an array or a matrix in R using the array() or matrix() functions, respectively. An array is a multi-dimensional data structure, while a matrix is a two-dimensional array.\n\n# Create a 1-dimensional array\narray_1d &lt;- array(1:10, dim = 10)\narray_1d\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# Create a 2-dimensional array\narray_2d &lt;- array(1:12, dim = c(4, 3))\narray_2d\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n# Create a 3-dimensional array\narray_3d &lt;- array(1:24, dim = c(4, 3, 2))\narray_3d\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]   13   17   21\n[2,]   14   18   22\n[3,]   15   19   23\n[4,]   16   20   24\n\n# Create a matrix\nmy_matrix &lt;- matrix(1:12, nrow = 4, ncol = 3)\nmy_matrix\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\n\nNote here, the matrix is a special case of an array, where the number of dimensions is exactly 2.\nis.matrix(array_2d)   # TRUE\nis.matrix(my_matrix)  # TRUE\n\nis.array(array_2d)    # TRUE\nis.array(my_matrix)   # TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#key-and-value-pair",
    "href": "01-intro.html#key-and-value-pair",
    "title": "1  Data Structure and R Programming",
    "section": "1.6 Key and Value Pair",
    "text": "1.6 Key and Value Pair\nKey-Value Pair is a data structure that consists of a key and its corresponding value. In R, this can be implemented using named vectors, lists, or data frames. Usually, the most commonly used case is in the lists and data frames. The values can be extra by providing the corresonding key\n\nkey1 &lt;- \"Tues\"\nvalue1 &lt;- 32\nkey2 &lt;- \"Wed\"\nvalue2 &lt;- 28\n\nlist_temp &lt;- list()\nlist_temp[[ key1 ]] &lt;- value1\nlist_temp[[ key2 ]] &lt;- value2\n\nprint(list_temp)\n\n$Tues\n[1] 32\n\n$Wed\n[1] 28\n\n## Now providing a key - Tues\n### First way\nlist_temp[[\"Tues\"]]\n\n[1] 32\n\n### Second way\nlist_temp$Tues\n\n[1] 32",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-frame",
    "href": "01-intro.html#data-frame",
    "title": "1  Data Structure and R Programming",
    "section": "1.7 Data Frame",
    "text": "1.7 Data Frame\nDataframe is a two-dimensional, tabular data structure in R that can hold different types of variables (numeric, character, factor, etc.) in each column. It is similar to a spreadsheet or SQL table.\n\niris &lt;- datasets::iris\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#apply-function",
    "href": "01-intro.html#apply-function",
    "title": "1  Data Structure and R Programming",
    "section": "1.8 Apply function",
    "text": "1.8 Apply function\nThe apply() function is the basic model of the family of apply functions in R, which includes specific functions like lapply(), sapply(), tapply(), mapply(), vapply(), rapply(), bapply(), eapply(), and others. These functions are used to apply a function to elements of a data structure (like a vector, list, or data frame) in a (sometimes) more efficient and concise way than using loops.\n\nx &lt;- cbind(x1 = 3, x2 = c(4:1, 2:5))\ndimnames(x)[[1]] &lt;- letters[1:8]\nprint(x)\n\n  x1 x2\na  3  4\nb  3  3\nc  3  2\nd  3  1\ne  3  2\nf  3  3\ng  3  4\nh  3  5\n\napply(x, MARGIN = 2, mean) #apply the mean function to their \"columns\"\n\nx1 x2 \n 3  3 \n\ncol.sums &lt;- apply(x, MARGIN = 2, sum) #apply the sum function to their \"columns\"\nrow.sums &lt;- apply(x, MARGIN = 1, sum) #apply the sum function to their \"rows\"\nrbind(cbind(x, Rtot = row.sums), Ctot = c(col.sums, sum(col.sums)))\n\n     x1 x2 Rtot\na     3  4    7\nb     3  3    6\nc     3  2    5\nd     3  1    4\ne     3  2    5\nf     3  3    6\ng     3  4    7\nh     3  5    8\nCtot 24 24   48\n\n\nSome of the commonly used apply functions:\n\nlapply: Apply a Function over a List or Vector\nsapply: a user-friendly version and wrapper of lapply by default returning a vector, matrix\nvapply: similar to sapply, but has a pre-specified type of return value, so it can be safer (and sometimes faster) to use.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#tidyverse",
    "href": "01-intro.html#tidyverse",
    "title": "1  Data Structure and R Programming",
    "section": "1.9 Tidyverse",
    "text": "1.9 Tidyverse\nThe tidyverse is a collection of open source packages for the R programming language introduced by Hadley Wickham and his team that “share an underlying design philosophy, grammar, and data structures” of tidy data. Characteristic features of tidyverse packages include extensive use of non-standard evaluation and encouraging piping.\n\n## Load all tidyverse packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n## Or load specific packages in the tidy family\nlibrary(dplyr) # Data manipulation\nlibrary(ggplot2) # Data visualization\nlibrary(readr) # Data import\nlibrary(tibble) # Tidy data frames\nlibrary(tidyr) # Data tidying\n# ...",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#pipe",
    "href": "01-intro.html#pipe",
    "title": "1  Data Structure and R Programming",
    "section": "1.10 Pipe",
    "text": "1.10 Pipe\nPipe operator |&gt; (native after R version 4.0) or %&gt;$ (from magrittr package) is a powerful tool in R that allows you to chain together multiple operations in a clear and concise way. It takes the output of one function and passes it as the first argument to the next function.\nFor example, we can write\n\nset.seed(777)\nx &lt;- rnorm(5)\n\n## Without using pipe\nprint(round(mean(x), 2))\n\n[1] 0.37\n\n## Using pipe\nx |&gt; \n  mean() |&gt; # applying the mean function\n  round(2) |&gt; #round to 2nd decimal place\n  print()\n\n[1] 0.37\n\n\nWe can see that, without using the pipe, if we are applying multiple functions to the same object, we may have hard time to track. This can make the code less readable and harder to maintain. On the other hand, using pipe, we can clearly see the sequence of operations being applied to the data, making it easier to understand and modify.\n\n1.10.1 Some rules\n|&gt; should always have a space before it and should typically be the last thing on a line. This simplifies adding new steps, reorganizing existing ones, and modifying elements within each step.\nNote that all of the packages in the tidyverse family support the pipe operator (except ggplot2!), so you can use it with any of them.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "01-intro.html#questions-in-class",
    "href": "01-intro.html#questions-in-class",
    "title": "1  Data Structure and R Programming",
    "section": "1.11 Questions in class",
    "text": "1.11 Questions in class\n\n1.11.1 Lecture 1, August 25, 2025\nQ1. If I know Python already, why learn R?\nReply: My general take are 1). R is more specialized for statistical analysis and data visualization, while Python is a more general-purpose programming language. 2). R has a rich ecosystem of packages and libraries specifically designed for statistical computing, making it a popular choice among statisticians and data scientists. 3). R’s syntax and data structures are often more intuitive for statistical tasks, which can lead to faster development and easier collaboration with other statisticians. 4). Also, the tidyverse ecosystem including ggplot and others are a big plus when dealing with big dataframes. 5). They are not meant to replace each other, but work as a complement.\nQ2. Why my installation of R sometimes failed on a Windows machine?\nReply: There are many reasons. One of the most common reasons is that you may need to manually add the path to the environment variable.\n\n\n1.11.2 Lecture 2, August 27, 2025\nQ1. What’s the difference of using apply v.s. looping in R?\nReply: The apply functions are often faster and more efficient than looping, especially for large datasets, because they have done some vectorization under the hood. Also, it has much higher readibility and better conciseness. However, depends on the task, you may want to do the benchmarking to see the performance difference.\nQ2. How to use pipe with two or more variables?\nReply: There are several ways to do this.\n\nWithin the tidyverse family: One way is to use the dplyr package, which provides a set of functions that work well with the pipe operator. For example, you can use the mutate() function to create a new variable based on two existing variables. For example, you can do\n\n\nlibrary(dplyr)\nlibrary(magrittr)  # for %$%\nlibrary(purrr)     # for pmap / exec if needed\n\nmy_df &lt;- tibble(x = 1:5, y = 6:10)\nf  &lt;- function(a, b) a + 2*b\n\nmy_df %&gt;%\n  mutate(z = f(x, y))\n\n# A tibble: 5 × 3\n      x     y     z\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     1     6    13\n2     2     7    16\n3     3     8    19\n4     4     9    22\n5     5    10    25\n\n\n\nUsing base R, you may do something like the following through the magrittr package’s exposition pipe %$%:\n\n\nlibrary(magrittr)\n# method 1\nmy_df %$% f(x, y) \n\n[1] 13 16 19 22 25\n\n# or use . as a placeholder\n# method 2\nmy_df %&gt;% { f(.$x, .$y) }\n\n[1] 13 16 19 22 25\n\n\n\nSome of the materials are adapted from CMU Stat36-350.\nA comprehensive reference for all the tidyverse tools is R for Data Science.\nA comprehensive reference for ggplot2 is ggplot2: Elegant Graphics for Data Analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Structure and R Programming</span>"
    ]
  },
  {
    "objectID": "02-optimization.html",
    "href": "02-optimization.html",
    "title": "2  Numerical Approaches and Optimization",
    "section": "",
    "text": "2.1 Theory versus Computation\nThe optimization plays an important role in statistical computing, especially in the context of maximum likelihood estimation (MLE) and other statistical inference methods. This chapter will cover various optimization techniques used in statistical computing.\nThere is a general principle that will be repeated in this chapter that Kenneth Lange calls optimization transfer in his 1999 paper. The basic idea applies to the problem of maximizing a function \\(f\\).\nNote 1: steps 2&3 are repeated until convergence.\nNote 2: maximizing \\(f\\) is equivalent to minimizing \\(-f\\).\nNote 3: the surrogate function \\(g\\) should be chosen such that it is easier to optimize than \\(f\\).\nFor instance, for a linear regression \\[\\begin{equation}\n  y = X\\boldsymbol{\\beta} + \\varepsilon. \\label{eq:linmod}\n\\end{equation}\\]\nFrom regression class, we know that the (ordinary) least-squares estimation (OLE) for \\(\\boldsymbol{\\beta}\\) is given by \\(\\hat{\\boldsymbol{\\beta}}=(X^\\top X)^{-1} X^\\top y\\). It is convenient as the solution is in the closed-form! However, in the most case, the closed-form solutions will not be available.\nFor GLMs or non-linear regression, we need to do this iterativelly!\nOne confusing aspect of statistical computing is that often there is a disconnect between what is printed in a statistical computing textbook and what should be implemented on the computer.\nSome potential issues includ:\nA &lt;- matrix(\n  c(1, 2, 3,\n    4, 5, 6,\n    7, 8, 9),\n  nrow = 3, ncol = 3, byrow = TRUE)\nB &lt;- A\nB[3, 3] &lt;- B[3, 3] + 1E-5\n\nqr(A)$rank\n\n[1] 2\n\nqr(B)$rank\n\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#theory-versus-computation",
    "href": "02-optimization.html#theory-versus-computation",
    "title": "2  Numerical Approaches and Optimization",
    "section": "",
    "text": "In textbooks, simpler to present solutions as convenient mathematical formulas whenever possible, in order to communicate basic ideas and to provide some insight.\n\nHowever, directly translating these formulas into computer code is usually not advisable because there are many problematic aspects of computers that are simply not relevant when writing things down on paper.\n\n\n\n\nMemory overflow: The computer has a limited amount of memory, and it is possible to run out of memory when working with large datasets or complex models.\nNumerical Precision: Sometimes, due to the cut precision of floating-point arithmetic, calculations that are mathematically equivalent can yield different results on a computer.\n\nExample 1: round \\(1/3\\) to two decimal places, we get \\(0.33\\). Then, \\(3 \\cdot (1/3)\\) is exactly \\(1\\), but \\(3 \\cdot 0.33\\) is \\(0.99\\).\nExample 2: \\(1 - 0.99999999\\) is \\(0.00000001\\) (=1E-8), but if we round \\(0.99999999\\) to two decimal places, we get \\(1.00\\), and then \\(1 - 1.00\\) is \\(0\\). If we round \\(0.00000001\\) to two decimal places, we get \\(0.00\\).\nExample 3: \\(\\pi\\)\n\n(Lienar) Dependence: The detection of linear dependence in matrix computations is influenced by machine precision. Since computers operate with finite precision, situations often arise where true linear dependence exists, but the computer cannot distinguish it from independence.\n\nExample: Consider the matrix \\[\nA = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\] The 3rd column is a linear combination of the first two columns (i.e., col3 = col1 + col2). However, due to machine precision limitations, the computer might not recognize this exact linear dependence, leading to numerical instability in computations involving this matrix. With a small distortion, we have \\[\nB = \\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 + 10^{-5} \\\\\n\\end{pmatrix}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#matrix-inversion",
    "href": "02-optimization.html#matrix-inversion",
    "title": "2  Numerical Approaches and Optimization",
    "section": "2.2 Matrix Inversion",
    "text": "2.2 Matrix Inversion\nIn many statistical analyses, such as linear regression and specify the distribution (such as normal distribution), matrix inversion plays a central role.\n\n2.2.1 Example 1: Normal distribution\nWe know that, a normal density with the parameters mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is \\[\nf\\left(x \\mid \\mu, \\sigma^2\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left\\{-\\frac{1}{2 \\sigma^2}(x-\\mu)^2\\right\\}\n\\] or we may work on the multivariate normal distribution case which is a bit more involved.\n\n\\(\\boldsymbol{X} = (X1,\\dots, X_d)\\) is said to be a multivariate normal distribution if and only if it is a linear comibnation of independent and identically distributed standard normals: \\[\n\\boldsymbol{X} = \\boldsymbol{CZ} + \\mu,\\quad \\boldsymbol{Z}=(Z_1,\\dots,Z_d),\\quad Z_i \\stackrel{iid}{\\sim} N(0,1).\n\\]\n\nThe property of the multivariate normal are:\n\nmean vector: \\(E(\\boldsymbol{X}) = \\mu\\)\nvariance: \\(Var(\\boldsymbol{X}) = \\boldsymbol{CZC}^\\top = \\boldsymbol{C} var(\\boldsymbol{Z})\\boldsymbol{C}^\\top:=  \\boldsymbol{\\Sigma}\\)\n\nNotation: \\(\\boldsymbol{X} \\sim N(\\mu, \\boldsymbol{\\Sigma})\\).\nPDF: \\[\nf(\\boldsymbol{x} \\mid \\mu, \\Sigma)=(2 \\pi)^{-d / 2} \\cdot \\exp \\left\\{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\prime} \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})-\\frac{1}{2} \\log |\\boldsymbol{\\Sigma}|\\right\\}.\n\\] Some of the potential ways to do this is to take logarithm of the PDF (Think about why).\n\n\n2.2.2 Example 2: Linear regression\nRecall the linear regression model . The OLE for \\(\\boldsymbol{\\beta}\\) is given by \\(\\hat{\\boldsymbol{\\beta}}=(X^\\top X)^{-1} X^\\top y\\).\nWe can solve this using the R command\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nwhere solve() is the R function for matrix inversion. However, it is not a desired way (think about why).\nA better way is to go back to the formula, and look at \\[\nX^\\top X\\boldsymbol{\\beta}= X^\\top y,\n\\] and solve this using the R command\nsolve( crossprod(X), crossprod(X, y) ) \n# this is the same as \n# solve(t(X) %*% X, t(X) %*% y)\nHere, we avoid explicitly calculating the inverse of \\(X^\\top X\\). Instead, we use gaussian elimination to solve the system of equations, which is generally more numerically stable and efficient.\n\n2.2.2.1 Speed comparison\nset.seed(2025-09-03)\nX &lt;- matrix(rnorm(5000 * 100), 5000, 100)\ny &lt;- rnorm(5000)\nlibrary(microbenchmark)\nmicrobenchmark(solve(t(X) %*% X) %*% t(X) %*% y)\nUnit: milliseconds\n                             expr      min       lq\n solve(t(X) %*% X) %*% t(X) %*% y 28.83505 30.16593\n     mean   median       uq      max neval\n 31.96782 30.79489 32.63315 111.0151   100\nWarning message:\nIn microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y) :\n  less accurate nanosecond times to avoid potential integer overflows\nmicrobenchmark(solve(t(X) %*% X) %*% t(X) %*% y,\n               solve(crossprod(X), crossprod(X, y)))\nUnit: milliseconds\n                                 expr      min       lq\n     solve(t(X) %*% X) %*% t(X) %*% y 28.90135 30.11608\n solve(crossprod(X), crossprod(X, y)) 25.05859 25.27480\n     mean   median       uq      max neval\n 31.78686 31.38513 32.66482 53.03354   100\n 26.15771 25.81678 26.89188 29.12045   100\n\n\n\n2.2.3 Take home message:\nThe take home here is that the issues arise from the finite precision of computer arithmetic and the limited memory available on computers. When implementing statistical methods on a computer, it is crucial to consider these limitations and choose algorithms and implementations that are robust to numerical issues.\n\n\n2.2.4 Multi-collinearity\nThe above approach may break down when there is any multi-colinearity in the \\(\\boldsymbol{X}\\) matrix. For example, we can tack on a column to \\(\\boldsymbol{X}\\) that is very similar (but not identical) to the first column of \\(\\boldsymbol{X}\\).\n\nset.seed(7777)\nN &lt;- 3000\nK &lt;- 100\ny &lt;- rnorm(N)\nX &lt;- matrix(rnorm(N * K), N, K)\nW &lt;- cbind(X, X[, 1] + rnorm(N, sd = 1E-15))\n\nsolve(crossprod(W), crossprod(W, y))\n\nError in `solve.default()`:\n! system is computationally singular: reciprocal condition number = 1.36748e-32\nThe algorithm does not work because the cross product matrix \\(W^\\top W\\) is singular. In practice, matrices like these can come up a lot in data analysis and it would be useful to have a way to deal with it automatically.\nR takes a different approach to solving for the unknown coefficients in a linear model. R uses the QR decomposition, which is not as fast, but has the added benefit of being able to automatically detect and handle colinear columns in the matrix.\nHere, we use the fact that X can be decomposed as \\(\\boldsymbol{X}=QR\\), where \\(Q\\) is an orthonormal matrix and \\(R\\) is an upper triangular matrix. Given that, we can rewrite \\(X^\\top X \\boldsymbol{\\beta}= X^\\top y\\) as \\[\\begin{align*}\nR^\\top Q^\\top Q R \\boldsymbol{\\beta}&= R^\\top Q^\\top y\\\\\nR^\\top I R \\boldsymbol{\\beta}&= R^\\top Q^\\top y\\\\\nR^\\top R \\boldsymbol{\\beta}&= R^\\top Q^\\top y,\n\\end{align*}\\] this leads to \\(R\\boldsymbol{\\beta}= Q^\\top y\\). Now we can perform the Gaussian elimination to do it. Because \\(R\\) is an upper triangular matrix, the computational speed is much faster. Here, we avoid to compute the cross product \\(X^\\top X\\), which is numerical unstable if it is not standardized properly\nWe can see in R code that even with our singular matrix \\(W\\) above, the QR decomposition continues without error.\n\nQw &lt;- qr(W)\nstr(Qw)\n\nList of 4\n $ qr   : num [1:3000, 1:101] 54.43933 0.00123 -0.02004 -0.00671 -0.00178 ...\n $ rank : int 100\n $ qraux: num [1:101] 1.01 1.01 1.01 1 1 ...\n $ pivot: int [1:101] 1 2 3 4 5 6 7 8 9 10 ...\n - attr(*, \"class\")= chr \"qr\"\n\n\nNote that the output of qr() computes the rank of \\(W\\) to be 100, not 101 as the last column is collinear to the 1st column. From there, we can get \\(\\hat{\\boldsymbol{\\beta}}\\) if we want using qr.coef(),\n\nbetahat &lt;- qr.coef(Qw, y)\nhead(betahat, 3)\n\n[1]  0.024314718  0.000916951 -0.005980588\n\ntail(betahat, 3)\n\n[1]  0.01545039 -0.01010440          NA\n\n\nQ: Why there is an NA?\n\n\n2.2.5 Trade-off\nThere isn’t always elegance and flourish. When we take the robust approach, we accept that it comes at a cost.\n\nlibrary(ggplot2)\nlibrary(microbenchmark)\nm &lt;- microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y,\n                    solve(crossprod(X), crossprod(X, y)),\n                    qr.coef(qr(X), y))\n\nWarning in microbenchmark(solve(t(X) %*% X) %*% t(X) %*% y, solve(crossprod(X),\n: less accurate nanosecond times to avoid potential integer overflows\n\nautoplot(m)\n\n\n\n\n\n\n\n\nCompared with the approaches discussed above, this method performs similarly to the naive approach but is much more stable and reliable.\nIn practice, we rarely call functions such as qr() or qr.coef() directly, since higher-level functions like lm() handle these computations automatically. However, in certain specialized and performance-critical settings, it can be advantageous to use alternative matrix decompositions to compute regression coefficients, especially when the computation must be repeated many times in a loop (i.e., Vectorization)\n\n\n2.2.6 Multivariate Normal revisit\nComputing the multivariate normal (MVN) density is a common task, for example, when fitting spatial models or Gaussian process models. Because maximum likelihood estimation(MLE) and likelihood ratio tests (LRT) often require evaluating the likelihood many times, efficiency is crucial.\nAfter taking the log of the MVN density, we have\n\\[\n\\ell(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\Sigma) := \\log \\left\\{ f(\\boldsymbol{x}\\mid \\boldsymbol{\\mu},\\Sigma) \\right\\}\n= -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^\\top \\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}).\n\\] On the right hand side, the first term is a constant, the second term is linear, and the last term is quadratic, which requires much more computational power.\n\n2.2.6.1 A Naive Implementation\nWe first center the data \\(\\boldsymbol{z}:=\\boldsymbol{x}- \\mu\\). Then we have \\(\\boldsymbol{z}^\\top \\Sigma^{-1} \\boldsymbol{z}\\). This simiplified the question for a bit.\nHere, much like the linear regression example above, the key bottleneck is the inversion of the \\(p\\)-dimensional covariance matrix \\(\\Sigma\\). If we take \\(\\boldsymbol{z}\\) to be a \\(p\\times 1\\) column vector, then a literal translation of the mathematics into R code might look something like this,\nt(z) %*% solve(Sigma) %*% z\nTo illustrate, let’s simulate some data and compute the quadratic form the naive way:\n\nset.seed(2025-09-03)\n\n# Generate data\nz &lt;- matrix(rnorm(200 * 100), 200, 100)\nS &lt;- cov(z)\n\n# Naive quadratic form\nquad.naive &lt;- function(z, S) {\n  Sinv &lt;- solve(S)\n  rowSums((z %*% Sinv) * z)\n}\n\nlibrary(dplyr)\nquad.naive(z, S) %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  70.67   93.61   99.94  100.54  107.31  126.73 \n\n\n\n\n2.2.6.2 A Better Way: Cholesky Decomposition\nBecause the covariance matrix is symmetric and positive definite, we can exploit its Cholesky decomposition. That is, we write \\(\\Sigma = R^\\top R\\), where \\(R\\) is a upper triangular matrix. Then, \\[\n\\boldsymbol{z}^\\top \\Sigma^{-1} \\boldsymbol{z}= \\boldsymbol{z}^\\top (R^\\top R)^{-1} \\boldsymbol{z}= \\boldsymbol{z}^\\top R^{-1}R^{-\\top} \\boldsymbol{z}= (R^{-\\top}\\boldsymbol{z})^\\top (R^{-\\top} \\boldsymbol{z}) := \\boldsymbol{v}^\\top \\boldsymbol{v}.\n\\] Note that \\(\\boldsymbol{v}\\in \\mathbb R^p\\) is the solution to the linear system \\(R^\\top \\boldsymbol{v}= \\boldsymbol{z}\\). Because \\(R\\) is upper triangular, we can solve this system efficiently using back substitution. Also, we can solve this without doing the inversion.\nOnce we have \\(\\boldsymbol{v}\\) we can compute its quadratic form \\(\\boldsymbol{v}^\\top \\boldsymbol{v}\\) by the crossprod() function.\n\nquad.chol &lt;- function(z, S) {\n  R &lt;- chol(S)\n  v &lt;- backsolve(R, t(z), transpose = TRUE)\n  colSums(v * v)\n}\n\nquad.chol(z, S) %&gt;% summary()\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  70.67   93.61   99.94  100.54  107.31  126.73 \n\n\n\n\n2.2.6.3 By product\nAnother benefit of the Cholesky decomposition is that it gives us a simple way to compute the log-determinant of \\(\\Sigma\\). The log-determinant of \\(\\Sigma\\) is simply two times the sum of the log of the diagonal elements of R. (Why?)\n\n\n2.2.6.4 Performance comparison\n\nlibrary(microbenchmark)\nlibrary(ggplot2)\nm2 &lt;- microbenchmark(\n  naive = quad.naive(z, S),\n  chol  = quad.chol(z, S)\n)\nautoplot(m2)\n\n\n\n\n\n\n\n\nQ: Why one is faster than the other?\n\n\n2.2.6.5 Take home message 2\nThe naive algorithm simply inverts the covariance matrix. The Cholesky-based approach, on the other hand, exploits the fact that covariance matrices are symmetric and positive definite. This results in an implementation that is both faster and numerically more stable—exactly the kind of optimization that makes a difference in real-world statistical computing.\nThus, a knowledge of statistics and numerical analysis can often lead to better algorithms, often invaluable!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#nonlinear-functions",
    "href": "02-optimization.html#nonlinear-functions",
    "title": "2  Numerical Approaches and Optimization",
    "section": "2.3 Nonlinear functions",
    "text": "2.3 Nonlinear functions\nOn the top, we have linear functions, such as \\(y=f(x) = ax + b\\) or in the linear regression \\(y=X\\beta +\\epsilon\\). It is a small class of the functions, and may be relatively limited.\nE.g., what if we have a quadratic relationship? Then \\(y=f(x) = ax^2 + bx + c\\).\nSuch nonlinear relationship is very common, , such as \\(f(x) = a\\sin(bx + c)\\) or \\(f(x) = a\\exp(bx) + c\\), and they may not have a closed-form solution like in the linear regression case.\nFrom now on, we will be talking about the numerical approaches to solve these problems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#type-of-optimization-algorithms",
    "href": "02-optimization.html#type-of-optimization-algorithms",
    "title": "2  Numerical Approaches and Optimization",
    "section": "2.4 Type of Optimization Algorithms",
    "text": "2.4 Type of Optimization Algorithms\nThere are in general two types of the optimization algorithms: (1). deterministic and (2). metaheuristic. Deterministic and metaheuristic algorithms represent two distinct paradigms in optimization.\n*. Deterministic methods: such as gradient descent, produce the same solution for a given input and follow a predictable path toward an optimum.\n*. In contrast, metaheuristic approaches: incorporate randomness and do not guarantee the best possible solution. However, they are often more effective at avoiding local optima and exploring complex search spaces.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#deterministic-algorithms",
    "href": "02-optimization.html#deterministic-algorithms",
    "title": "2  Numerical Approaches and Optimization",
    "section": "2.5 Deterministic Algorithms",
    "text": "2.5 Deterministic Algorithms\nNumerical approximation, what you learned in the mathematical optimization course. Some of the algorithms include:\n\nGradient Descent\nNewton’s Method\nConjugate Gradient Method\nQuasi-Newton Methods (e.g., BFGS)\nInterior Point Methods\n\nThey often rely on the Karush–Kuhn–Tucker (KKT) conditions.\n\n2.5.1 Root finding\nThe root finding is probably the first numerical approach you learned in the numerical analysis course. Consider a function \\(f: \\mathbb R\\to \\mathbb R\\). The point \\(x\\in \\mathbb R\\) is called a root of \\(f\\) if \\(f(x) = 0\\).\nQ: Why do we care about the root finding?\nThis idea has broad applications. While finding the values of x such that f(x) = 0 is useful in many settings, a more general task is to determine the values of x for which f(x) = y. The same techniques used to find the roots of a function can be applied here by rewriting the problem as \\[\n\\tilde{f}(x) := f(x) - y = 0.\n\\] In this way, new function \\(\\tilde{f}(x)\\) has a root at the solution to, \\(f(x)=y\\), original equation.\nFor linear function, it is trivial. For quadratic function, we can use the quadratic formula, i.e., \\[\nx = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}.\n\\] However, for more complex functions, we need to use numerical methods to solve it iteratively. Below, we are going to go over some numerical algorithms.\n\n\n2.5.2 One-dimensional case\nWe first look at the one-dimensional case. The function we want to optimize is\n\\[f(x) = x^3 - x + 1\\]\n\n\n2.5.3 Bisection method\nBisection method is just like a binary search.\n\nStep 1. Selection two points \\(a,b\\in \\chi \\subseteq \\mathbb R\\), where \\(\\chi\\) is the domain of \\(f\\). Make sure that \\(a\\) and \\(b\\) have opposite signs, i.e., \\(f(a)f(b) &lt; 0\\).\nStep 2. Compute the midpoint \\(c = (a+b)/2\\).\nStep 3. Evaluate and check the sign of \\(f(c)\\). If \\(f(c)\\) has the same sign as \\(f(a)\\), then set \\(a=c\\). Otherwise, set \\(b=c\\).\nStep 4. Iterate Steps 2 and 3 until the interval \\([a,b]\\) is sufficiently small.\n\nThe intuition here is that we are shirking the search space \\(\\chi\\) by half in each iteration.\nQ: Why this algorithm work and what are the assumptions? 1. We require the function to be continuous 2. We require the function to have opposite signs at the two endpoints \\(a,b\\in\\chi\\subseteq \\mathbb R\\). 3. We do not require the differentiability!\nQ: But what’s the cost?\nQ: Can this work for every function?\n\n2.5.3.1 Example\nSuppose the design region is\n\na &lt;- 1\nb &lt;- 4\ncurve(0.5*x^3 - 0.5*x - 18, from = a, to = b, xlab = \"x\", ylab = \"f(x)\")\nfun_obj &lt;- function(x) 0.5*x^3 - 0.5*x - 18\n\nmy_bisec &lt;- function(fun_obj, a, b, tol = 1E-2, ind_draw = FALSE) {\n  if (fun_obj(a) * fun_obj(b) &gt; 0) {\n    stop(\"f(a) and f(b) must have opposite signs!\")\n  }\n  iter &lt;- 0\n  while ((b - a) / 2 &gt; tol) {\n    c &lt;- (a + b) / 2\n    \n    if (ind_draw == TRUE) {\n    # Draw vertical line\n    abline(v = c, col = \"red\", lty = 2)\n    # Label the iteration above the x-axis\n    text(c, par(\"usr\")[3] + 2, labels = iter + 1, col = \"blue\", pos = 3, cex = 0.8)\n    }\n\n    \n    if (fun_obj(c) == 0) {\n      return(c)\n    } else if (fun_obj(a) * fun_obj(c) &lt; 0) {\n      b &lt;- c\n    } else {\n      a &lt;- c\n    }\n    iter &lt;- iter + 1\n  }\n  val_x &lt;- (a + b) / 2\n  val_fx &lt;- fun_obj(val_x)\n  return(list(root = val_x, f_root = val_fx, iter = iter))\n}\n\n# Run it\nres_plot &lt;- my_bisec(fun_obj, a, b, ind_draw = TRUE)\n\n\n\n\n\n\n\nres_plot\n\n$root\n[1] 3.408203\n\n$f_root\n[1] 0.09048409\n\n$iter\n[1] 8\n\nres &lt;- my_bisec(fun_obj, a, b)\nplot(function(x) fun_obj(x), from = a, to = b)\nabline(h = 0, col = \"blue\", lty = 2)\ntitle(main = paste0(\"Bisection Method with \", res$iter, \" iterations\"))\nabline(v = res$root, col = \"red\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5, \n     labels = paste0(\"Root ≈ \", round(res$root, 3)), \n     col = \"red\", pos = 3, cex = 0.9, font = 2)\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Newton-Raphson method\nThe Newton-Raphson method (or simply Newton’s method) is an iterative numerical method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\nHere, we assume that the function \\(f\\) is differentiable. The idea here is to use the Taylor expansion of the function. Suppose we are search a small neighbour of the solution \\(x \\in \\mathbb R\\), say \\(x_j \\in \\mathbb R\\) is a small number. Then Then we first order Taylor series to approximate \\(f(x_j+h)\\) around \\(x_j\\) is \\[\nf(x)\\approx f(x_j) +  f^\\prime(x_j) (x-x_j),\n\\] where \\(f^\\prime(x) := \\partial_x f(x)\\) is the first derivative of \\(f(x)\\). So the root of this approximation can be improved by updating its place to where \\(f(x_{j+1}) = 0\\).\nSo if \\(f(x_j+h)\\) is the root, then we have \\[ 0 = f(x_j) + f^\\prime(x_j) h \\implies h = -\\frac{f(x_j)}{f^\\prime(x_j)}.\\]\nThen, we can come back to \\(x_{j+1}= x_j+h\\), and plug the value of \\(h\\) in from above, we have \\[\nx_{j+1} = x_j - \\frac{f(x_j)}{f^\\prime(x_j)}.\n\\]\nThe algorithm is given as below:\n\nLet \\(f:\\mathbb R\\to\\mathbb R\\) be differentiable. Given a current iterate \\(x_j\\) near a root \\(r\\) with \\(f(r)=0\\), use the first-order Taylor expansion of \\(f\\) at \\(x_j\\): \\[f(x)\\;\\approx\\; f(x_j) + f’(x_j)\\,(x - x_j).\\] To obtain the next iterate, set this linear approximation to zero and solve for x: \\[0 \\approx f(x_j) + f’(x_j)\\,(x - x_j)\n\\quad\\Longrightarrow\\quad\nx \\approx x_j - \\frac{f(x_j)}{f’(x_j)}.\\] Define \\[\\boxed{\\,x_{j+1} \\;=\\; x_j - \\dfrac{f(x_j)}{f’(x_j)}\\,}.\\]\nStep 0: Choose a function \\(f(x)\\): This is the function for which you want to find a root (i.e., solve \\(f(x) = 0)\\).\nStep 1: Calculate the derivative \\(f^\\prime(x)\\): You will need it to apply the formula.\nStep 2: Make an initial guess \\(x_0\\): Select a starting point \\(x_0\\) near the expected root.\nStep 3: Update the estimate: Use the Newton’s method formula to compute the next estimate \\(x_1\\) using \\(x_0\\) by \\[x_{j+1} = x_j - \\frac{f(x_j)}{f^\\prime(x_j)}.\\]\nStep 4: Repeat Steps 2 and 3 until the values converge to a root or reach a desired tolerance.\n\n\n## Function and derivative\nf  &lt;- function(x) 0.5*x^3 - 0.5*x - 18\ndf &lt;- function(x) 1.5*x^2 - 0.5\n\n## Newton–Raphson with iterate tracking\nnewton_raphson &lt;- function(f, df, x0, tol = 1e-5, \n                           maxit = 100, eps = 1e-5) {\n  x &lt;- x0\n  xs &lt;- x0      # store iterates (x0, x1, x2, ...)\n  for (k in 1:maxit) {\n    fx  &lt;- f(x)\n    dfx &lt;- df(x)\n    x_new &lt;- x - fx/dfx\n    xs &lt;- c(xs, x_new)\n    if (abs(x_new - x) &lt; tol || abs(fx) &lt; tol) {\n      return(list(root = x_new, iter = k, path = xs))\n    }\n    x &lt;- x_new\n  }\n  list(root = x, iter = maxit, path = xs)\n}\n\n## Starting point\n\nIf we start at -1 which is far away from\n\nx0 &lt;- 3\nres &lt;- newton_raphson(f, df, x0)\n\nIf we start at 3 which is near to the point\n\nx0 &lt;- 3\nres &lt;- newton_raphson(f, df, x0)\n## Plot range that shows the iterates and root\na &lt;- -2; b &lt;- 5\nplot(function(x) f(x), from = a, to = b, \n     xlab = \"x\", ylab = \"f(x)\",\n     main = paste(\"Newton-Raphson (Iterations:\", res$iter, \")\"))\nabline(h = 0, col = \"blue\", lty = 2)\n\n## Draw vertical lines for each iterate with labels 0,1,2,...\nfor (i in seq_along(res$path)) {\n  xi &lt;- res$path[i]\n  abline(v = xi, col = \"red\", lty = 2)\n  text(xi, par(\"usr\")[3] + 2, labels = i - 1, col = \"blue\", pos = 3, cex = 0.9)\n}\n\n## Final root marker + label\nabline(v = res$root, col = \"darkgreen\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5,\n     labels = paste0(\"Root ≈ \", round(res$root, 5),\n                     \" ; f(root) ≈ \", signif(f(res$root), 3)),\n     col = \"darkgreen\", pos = 3, cex = 0.95, font = 2)\n\n\n\n\n\n\n\nres\n\n$root\n[1] 3.402848\n\n$iter\n[1] 4\n\n$path\n[1] 3.000000 3.461538 3.403866 3.402848 3.402848\n\n\n\na &lt;- -2; b &lt;- 5\nplot(function(x) f(x), from = a, to = b, \n     xlab = \"x\", ylab = \"f(x)\",\n     main = paste(\"Newton-Raphson (Iterations:\", res$iter, \")\"))\nabline(h = 0, col = \"blue\", lty = 2)\n\n## Draw vertical lines for each iterate with labels 0,1,2,...\nfor (i in seq_along(res$path)) {\n  xi &lt;- res$path[i]\n  abline(v = xi, col = \"red\", lty = 2)\n  text(xi, par(\"usr\")[3] + 2, labels = i - 1, col = \"blue\", pos = 3, cex = 0.9)\n}\n\n## Final root marker + label\nabline(v = res$root, col = \"darkgreen\", lwd = 2)\ntext(res$root, par(\"usr\")[3] + 5,\n     labels = paste0(\"Root ≈ \", round(res$root, 5),\n                     \" ; f(root) ≈ \", signif(f(res$root), 3)),\n     col = \"darkgreen\", pos = 3, cex = 0.95, font = 2)\n\n\n\n\n\n\n\nres\n\n$root\n[1] 3.402848\n\n$iter\n[1] 4\n\n$path\n[1] 3.000000 3.461538 3.403866 3.402848 3.402848\n\n\nNotes & common pitfalls\n\nAssumptions: \\(f\\) is differentiable in a neighborhood of the root \\(r\\) and \\(f’(r)\\neq 0\\). Under these and a sufficiently good initial guess, convergence is quadratic.\nFailure cases: if \\(f^\\prime(x_j)=0\\) (or is very small), the update is ill-defined/unstable; if the initial guess is far, the method can diverge or jump to a different root.\nPractical checks: stop when \\(|f(x_j)|\\le \\delta\\) or \\(|x_{j+1}-x_j| \\le \\delta\\) is below tolerance; cap iterations; guard against tiny \\(f’(x_j)\\).\n\n\n\n2.5.5 Second Method\nThe secant method can be thought of as a finite-difference approximation of Newton’s method, so it is considered a quasi-Newton method. It is simialr to Newton’s method, but it does not require the computation of the derivative of the function. Instead, it approximates the derivative using two previous points.\nIn the second method, we require the first two points, say \\(x_0, x_1 \\in \\mathbb R\\). Then, we can approximate the derivative of \\(f\\) at \\(x_1\\) using the finite difference formula. Instead of calculate the derivative \\(f^\\prime(x_1)\\), we approximate it as using the secant line. In calculate, we know that, \\(f^\\prime(x_1) \\approx \\frac{f(x_1)-f(x_0)}{x_1-x_0}\\), if \\(x_1\\) and \\(x_0\\) are close. Then, we can plug this approximation into the Newton’s update formula to get \\[x_j  = x_{j-1}  - f(x_{j-1}) \\frac{x_{j-1}-x_{j-2}}{f(x_{j-1}) - f(x_{j-2})} = \\frac{x_{j-2} f\\left(x_{j-1}\\right)-x_{j-1} f\\left(x_{j-2}\\right)}{f\\left(x_{j-1}\\right)-f\\left(x_{j-2}\\right)} .\\]\n\n\n2.5.6 Multivariate Case\nWe need to calculate the gradient/Jacobian matrix and Hessian matrix.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "02-optimization.html#heuristic-algorithms",
    "href": "02-optimization.html#heuristic-algorithms",
    "title": "2  Numerical Approaches and Optimization",
    "section": "2.6 Heuristic Algorithms",
    "text": "2.6 Heuristic Algorithms\nMany of the heuristic algorithms are inspired by the nature, such as the genetic algorithm (GA) and particle swarm optimization (PSO). These algorithms are often used for complex optimization problems where traditional methods may struggle to find a solution. Some of the popular heuristic algorithms include:\n\nGenetic Algorithm (GA)\nParticle Swarm Optimization (PSO)\nSimulated Annealing (SA)\nAnt Colony Optimization (ACO)\n\n\n2.6.1 In R\noptim() function, nlm() function or mle() function.\n\n\n2.6.2 EM Algorithm\nThe EM (Expectation–Maximization) algorithm is an optimization method that is often applied to find maximum likelihood estimates when data is incomplete or has missing values. It iteratively refines estimates of parameters by alternating between (1) expectation step (E-step) and (2) maximization step (M-step).\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples are borrowed from the following sources:\n\nPeng, R.D. Advanced Statistical Computing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Numerical Approaches and Optimization</span>"
    ]
  },
  {
    "objectID": "03-resampling.html",
    "href": "03-resampling.html",
    "title": "3  Resampling, Jackknife and Bootstrap",
    "section": "",
    "text": "3.1 Introduction\nThis chapter covers resampling methods including the jackknife and bootstrap techniques.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "03-resampling.html#jackknife",
    "href": "03-resampling.html#jackknife",
    "title": "3  Resampling, Jackknife and Bootstrap",
    "section": "3.2 Jackknife",
    "text": "3.2 Jackknife\nThe jackknife is a resampling technique used to estimate the bias and variance of a statistic.\nJackknife is like a leave-one-out cross-validation. Let \\(\\mathbf{x}= (x_1,\\dots,x_n)\\) be an observed random sample, and denote the \\(i\\)th jackknife sample by \\(\\mathbf{x}_{-i} = (x_1,\\dots,x_{i-1},x_{i+1},\\dots,x_n)\\), that is, a subset of \\(\\mathbf{x}\\).\nFor the parameter of interest \\(\\theta\\), if the statistics is \\(T(\\mathbf{x})=:\\hat{\\theta}\\) is computed on the full\n\n3.2.1 When does jackknife not work?\nJackknife does not work when the function \\(T(\\cdot)\\) is not a smooth functional!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "03-resampling.html#bootstrap",
    "href": "03-resampling.html#bootstrap",
    "title": "3  Resampling, Jackknife and Bootstrap",
    "section": "3.3 Bootstrap",
    "text": "3.3 Bootstrap\nThe bootstrap is a resampling method that allows estimation of the sampling distribution of almost any statistic using random sampling methods.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "03-resampling.html#applications",
    "href": "03-resampling.html#applications",
    "title": "3  Resampling, Jackknife and Bootstrap",
    "section": "3.4 Applications",
    "text": "3.4 Applications\nThese methods are widely used in statistical inference and have applications in various fields.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Resampling, Jackknife and Bootstrap</span>"
    ]
  },
  {
    "objectID": "other-topics.html",
    "href": "other-topics.html",
    "title": "4  Additional Topics",
    "section": "",
    "text": "4.1 High-dimensional data\nThis chapter covers additional topics that will only be going over if time permits.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Additional Topics</span>"
    ]
  },
  {
    "objectID": "other-topics.html#dimensional-reduction-methods",
    "href": "other-topics.html#dimensional-reduction-methods",
    "title": "4  Additional Topics",
    "section": "4.2 Dimensional Reduction Methods",
    "text": "4.2 Dimensional Reduction Methods\n\n4.2.1 Principal Component Analysis",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Additional Topics</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html",
    "href": "app-A-intro-R.html",
    "title": "5  Appendix: Introduction to R?",
    "section": "",
    "text": "5.1 R\nFor conducting analyses with data sets of hundreds to thousands of observations, calculating by hand is not feasible and you will need a statistical software. R is one of those. R can also be thought of as a high-level programming language. In fact, R is one of the top languages to be used by data analysts and data scientists. There are a lot of analysis packages in R that are currently developed and maintained by researchers around the world to deal with different data problems. Most importantly, R is free! In this section, we will learn how to use R to conduct basic statistical analyses.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#ide",
    "href": "app-A-intro-R.html#ide",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.2 IDE",
    "text": "5.2 IDE\n\n5.2.1 Rstudio\nRStudio is an integrated development environment (IDE) designed specifically for working with the R programming language. It provides a user-friendly interface that includes a source editor, console, environment pane, and tools for plotting, debugging, version control, and package management. RStudio supports both R and Python and is widely used for data analysis, statistical modeling, and reproducible research. It also integrates seamlessly with tools like R Markdown, Shiny, and Quarto, making it popular among data scientists, statisticians, and educators.\n\n\n5.2.2 Visual Studio Code (VS Code)\nVS Code is a versatile code editor that supports multiple programming languages, including R. With the R extension for VS Code, users can write and execute R code, access R’s console, and utilize features like syntax highlighting, code completion, and debugging. While not as specialized as RStudio for R development, VS Code offers a lightweight alternative with extensive customization options and support for various programming tasks.\n\n\n5.2.3 Positron\nPositron IDE is the next-generation integrated development environment developed by Posit, the company behind RStudio. Designed to be a modern, extensible, and language-agnostic IDE, Positron builds on the strengths of RStudio while supporting a broader range of languages and workflows, including R, Python, and Quarto.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#rstudio-layout",
    "href": "app-A-intro-R.html#rstudio-layout",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.3 RStudio Layout",
    "text": "5.3 RStudio Layout\nRStudio consists of several panes: - Source: Where you write scripts and markdown documents. - Console: Where you type and execute R commands. - Environment/History: Shows your variables and command history. - Files/Plots/Packages/Help/Viewer: For file management, viewing plots, managing packages, accessing help, and viewing web content.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#r-scripts",
    "href": "app-A-intro-R.html#r-scripts",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.4 R Scripts",
    "text": "5.4 R Scripts\nR scripts are plain text files containing R code. You can create a new script in RStudio by clicking File &gt; New File &gt; R Script.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#r-help",
    "href": "app-A-intro-R.html#r-help",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.5 R Help",
    "text": "5.5 R Help\nUse ?function_name or help(function_name) to access help for any R function. For example:\n?mean\nhelp(mean)",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#r-packages",
    "href": "app-A-intro-R.html#r-packages",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.6 R Packages",
    "text": "5.6 R Packages\nPackages extend R’s functionality. There are thousands of packages available in R ecosystem. You may install them from different sources.\n\n5.6.1 With Comprehensive R Archive Network (CRAN)\nCRAN is the primary repository for R packages. It contains thousands of packages that can be easily installed and updated.\nInstall a package with:\ninstall.packages(\"package_name\")\n\n\n5.6.2 With Bioconductor\nBioconductor is a repository for bioinformatics packages in R. It provides tools for the analysis and comprehension of high-throughput genomic data.\nInstall Bioconductor packages using the BiocManager package:\nBiocManager::install(\"package_name\")\n\n\n5.6.3 From GitHub\nMany of the authors of R packages host their work on GitHub. You can install these packages using the devtools package:\ndevtools::install_github(\"username/package_name\")\n\n\n5.6.4 Load a package\nOnce a package is installed, you need to load it into your R session to use its functions:\nlibrary(package_name)\nAlternatively, you may use a function in the package with package_name::function_name() without loading the entire package.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#r-markdown",
    "href": "app-A-intro-R.html#r-markdown",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.7 R Markdown",
    "text": "5.7 R Markdown\nR Markdown allows you to combine text, code, and output in a single document. Create a new R Markdown file in RStudio via File &gt; New File &gt; R Markdown....\nRecently, the posit team has developed a new version of the R Markdown called quarto document, with the file extension .qmd. It is still under rapid development.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#vectors",
    "href": "app-A-intro-R.html#vectors",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.8 Vectors",
    "text": "5.8 Vectors\nVectors are the most basic data structure in R.\n\nx &lt;- c(1, 2, 3, 4, 5)\nx\n\n[1] 1 2 3 4 5\n\n\nYou can perform operations on vectors:\n\nx * 2\n\n[1]  2  4  6  8 10",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  },
  {
    "objectID": "app-A-intro-R.html#data-sets",
    "href": "app-A-intro-R.html#data-sets",
    "title": "5  Appendix: Introduction to R?",
    "section": "5.9 Data Sets",
    "text": "5.9 Data Sets\nData frames are used for storing data tables. Create a data frame:\n\ndf &lt;- data.frame(Name = c(\"Alice\", \"Bob\"), Score = c(90, 85))\ndf\n\n   Name Score\n1 Alice    90\n2   Bob    85\n\n\nYou can import data from files using read.csv() or read.table().\n\nThis appendix is adapted from Why R?.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix: Introduction to R?</span>"
    ]
  }
]